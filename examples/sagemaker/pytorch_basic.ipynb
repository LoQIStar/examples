{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk - Getting Started Demo\n",
    "### Binary Classification with `Trainer` and `imdb` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "    3. [Permissions](#Permissions)\n",
    "3. [Processing](#Preprocessing)   \n",
    "    1. [Tokenization](#Tokenization)  \n",
    "    2. [Uploading data to sagemaker_session_bucket](#Uploading-data-to-sagemaker_session_bucket)  \n",
    "4. [Fine-tuning & starting Sagemaker Training Job](#Fine-tuning-\\&-starting-Sagemaker-Training-Job)  \n",
    "    1. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  \n",
    "    2. [Estimator Parameters](#Estimator-Parameters)   \n",
    "    3. [Download fine-tuned model from s3](#Download-fine-tuned-model-from-s3)\n",
    "    3. [Attach to old training job to an estimator ](#Attach-to-old-training-job-to-an-estimator)  \n",
    "5. [_Coming soon_:Push model to the Hugging Face hub](#Push-model-to-the-Hugging-Face-hub)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to our end-to-end binary Text-Classification example. In this demo, we will use the Hugging Faces `transformers` and `datasets` library together with a custom Amazon sagemaker-sdk extension to fine-tune a pre-trained transformer on binary text classification. In particular, the pre-trained model will be fine-tuned using the `imdb` dataset. To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you haven´t it installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.48.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.52.1)\n",
      "Requirement already satisfied: transformers==4.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (4.6.1)\n",
      "Requirement already satisfied: datasets[s3]==1.6.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.6.2)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (0.0.45)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (3.7.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (3.0.12)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (2020.11.13)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (0.0.8)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (4.49.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (1.19.5)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (0.8)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (2.25.1)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (0.3.3)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (2021.7.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (1.1.5)\n",
      "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (4.0.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (0.70.11.1)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (2.0.2)\n",
      "Collecting botocore==1.19.52\n",
      "  Using cached botocore-1.19.52-py2.py3-none-any.whl (7.2 MB)\n",
      "Collecting boto3==1.16.43\n",
      "  Using cached boto3-1.16.43-py2.py3-none-any.whl (130 kB)\n",
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (2021.4.0)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3==1.16.43->datasets[s3]==1.6.2) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (1.26.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (2.8.1)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (20.3.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.1.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (3.15.2)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.2.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.2.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers==4.6.1) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker>=2.48.0) (1.15.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (2021.5.30)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas->datasets[s3]==1.6.2) (2021.1)\n",
      "Requirement already satisfied: ppft>=1.6.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pathos->sagemaker>=2.48.0) (1.6.6.3)\n",
      "Requirement already satisfied: pox>=0.2.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pathos->sagemaker>=2.48.0) (0.2.9)\n",
      "Requirement already satisfied: aiobotocore>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from s3fs->datasets[s3]==1.6.2) (1.2.2)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2021.4.0-py3-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.10.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.12.1)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (0.7.1)\n",
      "Requirement already satisfied: aiohttp>=3.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (3.7.4.post0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.6.3)\n",
      "Requirement already satisfied: idna-ssl>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (5.1.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (3.0.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1) (7.1.2)\n",
      "Installing collected packages: botocore, s3transfer, fsspec, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.21.16\n",
      "    Uninstalling botocore-1.21.16:\n",
      "      Successfully uninstalled botocore-1.21.16\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.5.0\n",
      "    Uninstalling s3transfer-0.5.0:\n",
      "      Successfully uninstalled s3transfer-0.5.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2021.7.0\n",
      "    Uninstalling fsspec-2021.7.0:\n",
      "      Successfully uninstalled fsspec-2021.7.0\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.18.16\n",
      "    Uninstalling boto3-1.18.16:\n",
      "      Successfully uninstalled boto3-1.18.16\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.19.100 requires botocore==1.20.100, but you have botocore 1.19.52 which is incompatible.\n",
      "awscli 1.19.100 requires colorama<0.4.4,>=0.2.5, but you have colorama 0.4.4 which is incompatible.\n",
      "awscli 1.19.100 requires docutils<0.16,>=0.10, but you have docutils 0.17.1 which is incompatible.\n",
      "awscli 1.19.100 requires s3transfer<0.5.0,>=0.4.0, but you have s3transfer 0.3.7 which is incompatible.\u001b[0m\n",
      "Successfully installed boto3-1.16.43 botocore-1.19.52 fsspec-2021.4.0 s3transfer-0.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**upgrade ipywidgets for `datasets` library and restart kernel, only needed when prerpocessing is done in the notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "!conda install -c conda-forge ipywidgets -y\n",
    "IPython.Application.instance().kernel.do_shutdown(True) # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::618469898284:role/jeff-sagemaker\n",
      "sagemaker bucket: sagemaker-us-east-2-618469898284\n",
      "sagemaker session region: us-east-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We are using the `datasets` library to download and preprocess the `imdb` dataset. After preprocessing, the dataset will be uploaded to our `sagemaker_session_bucket` to be used within our training job. The [imdb](http://ai.stanford.edu/~amaas/data/sentiment/) dataset consists of 25000 training and 25000 testing highly polar movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fsspec\n",
    "fsspec.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fsspec-2021.4.0.dist-info/RECORD'\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq fsspec==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'imdb'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/datasets/imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/ec2-user/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n",
      "Reusing dataset imdb (/home/ec2-user/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e492c5380b942eb9fe4a2a973572b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15a127b06b94a0d908aeb3d2987eadb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'])\n",
    "test_dataset = test_dataset.shuffle().select(range(10000)) # smaller the size for test dataset to 10k \n",
    "\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset =  train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to `sagemaker_session_bucket`\n",
    "\n",
    "After we processed the `datasets` we are going to use the new `FileSystem` [integration](https://huggingface.co/docs/datasets/filesystems.html) to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path,fs=s3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.4',\n",
    "                            pytorch_version='1.6',\n",
    "                            py_version='py36',\n",
    "                            role=role,\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "```\n",
    "\n",
    "The `hyperparameters` you define in the `HuggingFace` estimator are passed in as named arguments. \n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local_gpu'` for gpu usage. _Note: this does not working within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score, precision_recall_fscore_support\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_from_disk\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34m5e-5\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--n_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--training_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    args, _ = parser.parse_known_args()\n",
      "\n",
      "    \u001b[37m# Set up logging\u001b[39;49;00m\n",
      "    logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "\n",
      "    logging.basicConfig(\n",
      "        level=logging.getLevelName(\u001b[33m\"\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "        handlers=[logging.StreamHandler(sys.stdout)],\n",
      "        \u001b[36mformat\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# load datasets\u001b[39;49;00m\n",
      "    train_dataset = load_from_disk(args.training_dir)\n",
      "    test_dataset = load_from_disk(args.test_dir)\n",
      "\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded train_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(train_dataset)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded test_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(test_dataset)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# compute metrics function for binary classification\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(pred):\n",
      "        labels = pred.label_ids\n",
      "        preds = pred.predictions.argmax(-\u001b[34m1\u001b[39;49;00m)\n",
      "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\u001b[33m\"\u001b[39;49;00m\u001b[33mbinary\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        acc = accuracy_score(labels, preds)\n",
      "        \u001b[34mreturn\u001b[39;49;00m {\u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: acc, \u001b[33m\"\u001b[39;49;00m\u001b[33mf1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: f1, \u001b[33m\"\u001b[39;49;00m\u001b[33mprecision\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: precision, \u001b[33m\"\u001b[39;49;00m\u001b[33mrecall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: recall}\n",
      "\n",
      "    \u001b[37m# download model from model hub\u001b[39;49;00m\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(args.model_name)\n",
      "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
      "\n",
      "    \u001b[37m# define training args\u001b[39;49;00m\n",
      "    training_args = TrainingArguments(\n",
      "        output_dir=args.model_dir,\n",
      "        num_train_epochs=args.epochs,\n",
      "        per_device_train_batch_size=args.train_batch_size,\n",
      "        per_device_eval_batch_size=args.eval_batch_size,\n",
      "        warmup_steps=args.warmup_steps,\n",
      "        evaluation_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        logging_dir=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.output_data_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/logs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        learning_rate=\u001b[36mfloat\u001b[39;49;00m(args.learning_rate),\n",
      "    )\n",
      "\n",
      "    \u001b[37m# create Trainer instance\u001b[39;49;00m\n",
      "    trainer = Trainer(\n",
      "        model=model,\n",
      "        args=training_args,\n",
      "        compute_metrics=compute_metrics,\n",
      "        train_dataset=train_dataset,\n",
      "        eval_dataset=test_dataset,\n",
      "        tokenizer=tokenizer,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# train model\u001b[39;49;00m\n",
      "    trainer.train()\n",
      "\n",
      "    \u001b[37m# evaluate model\u001b[39;49;00m\n",
      "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
      "\n",
      "    \u001b[37m# writes eval result to file which can be accessed later in s3 ouput\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.output_data_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33meval_results.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m writer:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Eval results *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m \u001b[36msorted\u001b[39;49;00m(eval_result.items()):\n",
      "            writer.write(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mkey\u001b[33m}\u001b[39;49;00m\u001b[33m = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mvalue\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Saves the model to s3\u001b[39;49;00m\n",
      "    trainer.save_model(args.model_dir)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./scripts/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'train_batch_size': 32,\n",
    "                 'model_name':'distilbert-base-uncased'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.6',\n",
    "                            pytorch_version='1.7',\n",
    "                            py_version='py36',\n",
    "                            hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-09 18:56:44 Starting - Starting the training job...\n",
      "2021-08-09 18:57:07 Starting - Launching requested ML instancesProfilerReport-1628535404: InProgress\n",
      "......\n",
      "2021-08-09 18:58:07 Starting - Preparing the instances for training......\n",
      "2021-08-09 18:59:13 Downloading - Downloading input data...\n",
      "2021-08-09 18:59:33 Training - Downloading the training image...............\n",
      "2021-08-09 19:02:15 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-08-09 19:02:16,022 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-08-09 19:02:16,046 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-08-09 19:02:22,270 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-08-09 19:02:22,785 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 32,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2021-08-09-18-56-43-941\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-618469898284/huggingface-pytorch-training-2021-08-09-18-56-43-941/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-618469898284/huggingface-pytorch-training-2021-08-09-18-56-43-941/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2021-08-09-18-56-43-941\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-618469898284/huggingface-pytorch-training-2021-08-09-18-56-43-941/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2021-08-09 19:02:27,645 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m2021-08-09 19:02:27,645 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m2021-08-09 19:02:27,935 - filelock - INFO - Lock 140642889424568 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-08-09 19:02:28,001 - filelock - INFO - Lock 140642889424568 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-08-09 19:02:28,069 - filelock - INFO - Lock 140642678913344 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m2021-08-09 19:02:33,152 - filelock - INFO - Lock 140642678913344 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m2021-08-09 19:02:34,025 - filelock - INFO - Lock 140642678483152 acquired on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m2021-08-09 19:02:34,111 - filelock - INFO - Lock 140642678483152 released on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m2021-08-09 19:02:34,176 - filelock - INFO - Lock 140642678481472 acquired on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m2021-08-09 19:02:34,273 - filelock - INFO - Lock 140642678481472 released on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m2021-08-09 19:02:34,469 - filelock - INFO - Lock 140642678483152 acquired on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\u001b[0m\n",
      "\u001b[34m2021-08-09 19:02:34,533 - filelock - INFO - Lock 140642678483152 released on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.494 algo-1:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.646 algo-1:26 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.647 algo-1:26 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.647 algo-1:26 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.648 algo-1:26 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.649 algo-1:26 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.831 algo-1:26 INFO hook.py:591] name:distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.831 algo-1:26 INFO hook.py:591] name:distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.831 algo-1:26 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.831 algo-1:26 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.831 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.831 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.831 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.831 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.832 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.832 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.832 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.832 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.832 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.832 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.832 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.832 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.832 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.833 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.833 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.833 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.833 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.833 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.833 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.833 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.833 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.833 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.833 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.834 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.834 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.834 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.834 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.834 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.834 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.834 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.834 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.835 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.835 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.835 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.835 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.835 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.835 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.835 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.836 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.836 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.836 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.836 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.836 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.836 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.836 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.837 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.837 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.837 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.837 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.837 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.837 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.838 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.838 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.838 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.838 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.838 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.838 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.839 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.839 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.839 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.839 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.839 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.839 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.840 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.840 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.840 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.840 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.840 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.840 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.840 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.840 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.840 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.840 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.841 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.841 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.841 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.841 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.841 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.841 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.841 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.842 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.842 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.842 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.842 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.842 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.842 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.842 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.842 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.842 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.843 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.843 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.843 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.843 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.843 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.843 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.843 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.843 algo-1:26 INFO hook.py:591] name:pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.843 algo-1:26 INFO hook.py:591] name:pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.844 algo-1:26 INFO hook.py:591] name:classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.844 algo-1:26 INFO hook.py:591] name:classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.844 algo-1:26 INFO hook.py:593] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.844 algo-1:26 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-08-09 19:02:39.847 algo-1:26 INFO hook.py:488] Hook is writing from the hook with pid: 26\n",
      "\u001b[0m\n",
      "\u001b[34m{'loss': 0.3604, 'learning_rate': 5e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1920931339263916, 'eval_accuracy': 0.928, 'eval_f1': 0.9282296650717703, 'eval_precision': 0.9291558571143484, 'eval_recall': 0.9273053176658036, 'eval_runtime': 50.7539, 'eval_samples_per_second': 197.029, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 489.3435, 'train_samples_per_second': 1.598, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 442/442 [00:00<00:00, 452kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]#015Downloading:   1%|          | 2.88M/268M [00:00<00:09, 28.8MB/s]#015Downloading:   3%|▎         | 6.87M/268M [00:00<00:08, 31.4MB/s]#015Downloading:   4%|▍         | 11.8M/268M [00:00<00:07, 35.2MB/s]#015Downloading:   6%|▋         | 17.1M/268M [00:00<00:06, 39.1MB/s]#015Downloading:   8%|▊         | 22.4M/268M [00:00<00:05, 42.5MB/s]#015Downloading:  10%|█         | 27.7M/268M [00:00<00:05, 45.3MB/s]#015Downloading:  12%|█▏        | 33.1M/268M [00:00<00:04, 47.5MB/s]#015Downloading:  14%|█▍        | 38.5M/268M [00:00<00:04, 49.4MB/s]#015Downloading:  16%|█▋        | 44.0M/268M [00:00<00:04, 50.8MB/s]#015Downloading:  18%|█▊        | 49.2M/268M [00:01<00:04, 51.3MB/s]#015Downloading:  20%|██        | 54.6M/268M [00:01<00:04, 51.9MB/s]#015Downloading:  22%|██▏       | 59.9M/268M [00:01<00:03, 52.3MB/s]#015Downloading:  24%|██▍       | 65.3M/268M [00:01<00:03, 52.7MB/s]#015Downloading:  26%|██▋       | 70.7M/268M [00:01<00:03, 53.3MB/s]#015Downloading:  28%|██▊       | 76.2M/268M [00:01<00:03, 53.7MB/s]#015Downloading:  30%|███       | 81.7M/268M [00:01<00:03, 54.0MB/s]#015Downloading:  33%|███▎      | 87.2M/268M [00:01<00:03, 54.2MB/s]#015Downloading:  35%|███▍      | 92.6M/268M [00:01<00:03, 54.3MB/s]#015Downloading:  37%|███▋      | 98.1M/268M [00:01<00:03, 54.3MB/s]#015Downloading:  39%|███▊      | 104M/268M [00:02<00:03, 54.0MB/s] #015Downloading:  41%|████      | 109M/268M [00:02<00:02, 54.2MB/s]#015Downloading:  43%|████▎     | 115M/268M [00:02<00:02, 54.6MB/s]#015Downloading:  45%|████▍     | 120M/268M [00:02<00:02, 54.8MB/s]#015Downloading:  47%|████▋     | 126M/268M [00:02<00:02, 54.8MB/s]#015Downloading:  49%|████▉     | 131M/268M [00:02<00:02, 53.9MB/s]#015Downloading:  51%|█████     | 136M/268M [00:02<00:02, 53.9MB/s]#015Downloading:  53%|█████▎    | 142M/268M [00:02<00:02, 55.7MB/s]#015Downloading:  55%|█████▌    | 149M/268M [00:02<00:02, 57.1MB/s]#015Downloading:  58%|█████▊    | 154M/268M [00:02<00:01, 57.7MB/s]#015Downloading:  60%|█████▉    | 160M/268M [00:03<00:01, 57.9MB/s]#015Downloading:  62%|██████▏   | 166M/268M [00:03<00:01, 58.8MB/s]#015Downloading:  64%|██████▍   | 173M/268M [00:03<00:01, 59.6MB/s]#015Downloading:  67%|██████▋   | 179M/268M [00:03<00:01, 60.2MB/s]#015Downloading:  69%|██████▉   | 185M/268M [00:03<00:01, 60.6MB/s]#015Downloading:  71%|███████▏  | 191M/268M [00:03<00:01, 61.0MB/s]#015Downloading:  74%|███████▎  | 197M/268M [00:03<00:01, 61.1MB/s]#015Downloading:  76%|███████▌  | 203M/268M [00:03<00:01, 61.2MB/s]#015Downloading:  78%|███████▊  | 209M/268M [00:03<00:00, 61.2MB/s]#015Downloading:  80%|████████  | 216M/268M [00:03<00:00, 61.4MB/s]#015Downloading:  83%|████████▎ | 222M/268M [00:04<00:00, 58.1MB/s]#015Downloading:  85%|████████▍ | 228M/268M [00:04<00:00, 58.2MB/s]#015Downloading:  87%|████████▋ | 234M/268M [00:04<00:00, 58.4MB/s]#015Downloading:  89%|████████▉ | 239M/268M [00:04<00:00, 58.5MB/s]#015Downloading:  92%|█████████▏| 245M/268M [00:04<00:00, 58.6MB/s]#015Downloading:  94%|█████████▎| 251M/268M [00:04<00:00, 58.7MB/s]#015Downloading:  96%|█████████▌| 257M/268M [00:04<00:00, 58.8MB/s]#015Downloading:  98%|█████████▊| 263M/268M [00:04<00:00, 58.9MB/s]#015Downloading: 100%|██████████| 268M/268M [00:04<00:00, 55.7MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 9.79MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 466k/466k [00:00<00:00, 16.1MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 31.5kB/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/782 [00:00<?, ?it/s]#015  0%|          | 1/782 [00:02<29:18,  2.25s/it]#015  0%|          | 2/782 [00:02<22:37,  1.74s/it]#015  0%|          | 3/782 [00:03<17:53,  1.38s/it]#015  1%|          | 4/782 [00:03<14:35,  1.13s/it]#015  1%|          | 5/782 [00:04<12:17,  1.05it/s]#015  1%|          | 6/782 [00:04<10:39,  1.21it/s]#015  1%|          | 7/782 [00:05<09:29,  1.36it/s]#015  1%|          | 8/782 [00:06<08:46,  1.47it/s]#015  1%|          | 9/782 [00:06<08:19,  1.55it/s]#015  1%|▏         | 10/782 [00:07<08:00,  1.61it/s]#015  1%|▏         | 11/782 [00:07<07:40,  1.67it/s]#015  2%|▏         | 12/782 [00:08<07:24,  1.73it/s]#015  2%|▏         | 13/782 [00:08<07:15,  1.77it/s]#015  2%|▏         | 14/782 [00:09<07:10,  1.78it/s]#015  2%|▏         | 15/782 [00:09<07:03,  1.81it/s]#015  2%|▏         | 16/782 [00:10<07:02,  1.81it/s]#015  2%|▏         | 17/782 [00:10<06:58,  1.83it/s]#015  2%|▏         | 18/782 [00:11<06:58,  1.83it/s]#015  2%|▏         | 19/782 [00:12<06:57,  1.83it/s]#015  3%|▎         | 20/782 [00:12<06:53,  1.84it/s]#015  3%|▎         | 21/782 [00:13<06:55,  1.83it/s]#015  3%|▎         | 22/782 [00:13<07:06,  1.78it/s]#015  3%|▎         | 23/782 [00:14<06:59,  1.81it/s]#015  3%|▎         | 24/782 [00:14<06:55,  1.82it/s]#015  3%|▎         | 25/782 [00:15<06:52,  1.84it/s]#015  3%|▎         | 26/782 [00:15<06:49,  1.85it/s]#015  3%|▎         | 27/782 [00:16<06:56,  1.81it/s]#015  4%|▎         | 28/782 [00:16<06:52,  1.83it/s]#015  4%|▎         | 29/782 [00:17<06:53,  1.82it/s]#015  4%|▍         | 30/782 [00:18<06:52,  1.82it/s]#015  4%|▍         | 31/782 [00:18<06:51,  1.82it/s]#015  4%|▍         | 32/782 [00:19<06:50,  1.83it/s]#015  4%|▍         | 33/782 [00:19<06:46,  1.84it/s]#015  4%|▍         | 34/782 [00:20<06:43,  1.85it/s]#015  4%|▍         | 35/782 [00:20<06:44,  1.85it/s]#015  5%|▍         | 36/782 [00:21<06:41,  1.86it/s]#015  5%|▍         | 37/782 [00:21<06:43,  1.85it/s]#015  5%|▍         | 38/782 [00:22<06:48,  1.82it/s]#015  5%|▍         | 39/782 [00:22<06:53,  1.80it/s]#015  5%|▌         | 40/782 [00:23<06:49,  1.81it/s]#015  5%|▌         | 41/782 [00:24<06:48,  1.81it/s]#015  5%|▌         | 42/782 [00:24<06:49,  1.80it/s]#015  5%|▌         | 43/782 [00:25<06:44,  1.83it/s]#015  6%|▌         | 44/782 [00:25<06:43,  1.83it/s]#015  6%|▌         | 45/782 [00:26<06:50,  1.80it/s]#015  6%|▌         | 46/782 [00:26<06:50,  1.79it/s]#015  6%|▌         | 47/782 [00:27<06:50,  1.79it/s]#015  6%|▌         | 48/782 [00:27<06:47,  1.80it/s]#015  6%|▋         | 49/782 [00:28<06:42,  1.82it/s]#015  6%|▋         | 50/782 [00:29<06:37,  1.84it/s]#015  7%|▋         | 51/782 [00:29<06:34,  1.85it/s]#015  7%|▋         | 52/782 [00:30<06:34,  1.85it/s]#015  7%|▋         | 53/782 [00:30<06:35,  1.85it/s]#015  7%|▋         | 54/782 [00:31<06:33,  1.85it/s]#015  7%|▋         | 55/782 [00:31<06:31,  1.86it/s]#015  7%|▋         | 56/782 [00:32<06:36,  1.83it/s]#015  7%|▋         | 57/782 [00:32<06:37,  1.82it/s]#015  7%|▋         | 58/782 [00:33<06:33,  1.84it/s]#015  8%|▊         | 59/782 [00:33<06:32,  1.84it/s]#015  8%|▊         | 60/782 [00:34<06:28,  1.86it/s]#015  8%|▊         | 61/782 [00:34<06:28,  1.86it/s]#015  8%|▊         | 62/782 [00:35<06:29,  1.85it/s]#015  8%|▊         | 63/782 [00:36<06:27,  1.86it/s]#015  8%|▊         | 64/782 [00:36<06:28,  1.85it/s]#015  8%|▊         | 65/782 [00:37<06:27,  1.85it/s]#015  8%|▊         | 66/782 [00:37<06:28,  1.84it/s]#015  9%|▊         | 67/782 [00:38<06:29,  1.83it/s]#015  9%|▊         | 68/782 [00:38<06:28,  1.84it/s]#015  9%|▉         | 69/782 [00:39<06:30,  1.82it/s]#015  9%|▉         | 70/782 [00:39<06:29,  1.83it/s]#015  9%|▉         | 71/782 [00:40<06:26,  1.84it/s]#015  9%|▉         | 72/782 [00:40<06:25,  1.84it/s]#015  9%|▉         | 73/782 [00:41<06:25,  1.84it/s]#015  9%|▉         | 74/782 [00:42<06:27,  1.83it/s]#015 10%|▉         | 75/782 [00:42<06:30,  1.81it/s]#015 10%|▉         | 76/782 [00:43<06:36,  1.78it/s]#015 10%|▉         | 77/782 [00:43<06:34,  1.79it/s]#015 10%|▉         | 78/782 [00:44<06:35,  1.78it/s]#015 10%|█         | 79/782 [00:44<06:33,  1.79it/s]#015 10%|█         | 80/782 [00:45<06:29,  1.80it/s]#015 10%|█         | 81/782 [00:45<06:27,  1.81it/s]#015 10%|█         | 82/782 [00:46<06:27,  1.81it/s]#015 11%|█         | 83/782 [00:47<06:24,  1.82it/s]#015 11%|█         | 84/782 [00:47<06:25,  1.81it/s]#015 11%|█         | 85/782 [00:48<06:24,  1.81it/s]#015 11%|█         | 86/782 [00:48<06:22,  1.82it/s]#015 11%|█         | 87/782 [00:49<06:22,  1.82it/s]#015 11%|█▏        | 88/782 [00:49<06:21,  1.82it/s]#015 11%|█▏        | 89/782 [00:50<06:20,  1.82it/s]#015 12%|█▏        | 90/782 [00:50<06:21,  1.81it/s]#015 12%|█▏        | 91/782 [00:51<06:20,  1.82it/s]#015 12%|█▏        | 92/782 [00:52<06:19,  1.82it/s]#015 12%|█▏        | 93/782 [00:52<06:18,  1.82it/s]#015 12%|█▏        | 94/782 [00:53<06:17,  1.82it/s]#015 12%|█▏        | 95/782 [00:53<06:19,  1.81it/s]#015 12%|█▏        | 96/782 [00:54<06:18,  1.81it/s]#015 12%|█▏        | 97/782 [00:54<06:16,  1.82it/s]#015 13%|█▎        | 98/782 [00:55<06:14,  1.83it/s]#015 13%|█▎        | 99/782 [00:55<06:13,  1.83it/s]#015 13%|█▎        | 100/782 [00:56<06:13,  1.83it/s]#015 13%|█▎        | 101/782 [00:56<06:12,  1.83it/s]#015 13%|█▎        | 102/782 [00:57<06:12,  1.82it/s]#015 13%|█▎        | 103/782 [00:58<06:16,  1.80it/s]#015 13%|█▎        | 104/782 [00:58<06:15,  1.81it/s]#015 13%|█▎        | 105/782 [00:59<06:25,  1.76it/s]#015 14%|█▎        | 106/782 [00:59<06:21,  1.77it/s]#015 14%|█▎        | 107/782 [01:00<06:16,  1.79it/s]#015 14%|█▍        | 108/782 [01:00<06:13,  1.81it/s]#015 14%|█▍        | 109/782 [01:01<06:11,  1.81it/s]#015 14%|█▍        | 110/782 [01:01<06:10,  1.81it/s]#015 14%|█▍        | 111/782 [01:02<06:08,  1.82it/s]#015 14%|█▍        | 112/782 [01:03<06:08,  1.82it/s]#015 14%|█▍        | 113/782 [01:03<06:08,  1.81it/s]#015 15%|█▍        | 114/782 [01:04<06:09,  1.81it/s]#015 15%|█▍        | 115/782 [01:04<06:08,  1.81it/s]#015 15%|█▍        | 116/782 [01:05<06:07,  1.81it/s]#015 15%|█▍        | 117/782 [01:05<06:06,  1.82it/s]#015 15%|█▌        | 118/782 [01:06<06:04,  1.82it/s]#015 15%|█▌        | 119/782 [01:06<06:03,  1.82it/s]#015 15%|█▌        | 120/782 [01:07<06:03,  1.82it/s]#015 15%|█▌        | 121/782 [01:08<06:05,  1.81it/s]#015 16%|█▌        | 122/782 [01:08<06:05,  1.81it/s]#015 16%|█▌        | 123/782 [01:09<06:05,  1.80it/s]#015 16%|█▌        | 124/782 [01:09<06:04,  1.81it/s]#015 16%|█▌        | 125/782 [01:10<06:05,  1.80it/s]#015 16%|█▌        | 126/782 [01:10<06:02,  1.81it/s]#015 16%|█▌        | 127/782 [01:11<06:02,  1.81it/s]#015 16%|█▋        | 128/782 [01:11<06:00,  1.81it/s]#015 16%|█▋        | 129/782 [01:12<05:58,  1.82it/s]#015 17%|█▋        | 130/782 [01:12<05:57,  1.82it/s]#015 17%|█▋        | 131/782 [01:13<06:04,  1.79it/s]#015 17%|█▋        | 132/782 [01:14<06:00,  1.80it/s]#015 17%|█▋        | 133/782 [01:14<05:58,  1.81it/s]#015 17%|█▋        | 134/782 [01:15<05:57,  1.81it/s]#015 17%|█▋        | 135/782 [01:15<05:55,  1.82it/s]#015 17%|█▋        | 136/782 [01:16<05:56,  1.81it/s]#015 18%|█▊        | 137/782 [01:16<05:55,  1.81it/s]#015 18%|█▊        | 138/782 [01:17<05:54,  1.82it/s]#015 18%|█▊        | 139/782 [01:17<05:52,  1.82it/s]#015 18%|█▊        | 140/782 [01:18<05:52,  1.82it/s]#015 18%|█▊        | 141/782 [01:19<05:52,  1.82it/s]#015 18%|█▊        | 142/782 [01:19<05:50,  1.83it/s]#015 18%|█▊        | 143/782 [01:20<05:49,  1.83it/s]#015 18%|█▊        | 144/782 [01:20<05:50,  1.82it/s]#015 19%|█▊        | 145/782 [01:21<05:49,  1.82it/s]#015 19%|█▊        | 146/782 [01:21<05:49,  1.82it/s]#015 19%|█▉        | 147/782 [01:22<05:48,  1.82it/s]#015 19%|█▉        | 148/782 [01:22<05:49,  1.82it/s]#015 19%|█▉        | 149/782 [01:23<05:50,  1.81it/s]#015 19%|█▉        | 150/782 [01:24<05:49,  1.81it/s]#015 19%|█▉        | 151/782 [01:24<05:53,  1.79it/s]#015 19%|█▉        | 152/782 [01:25<05:54,  1.78it/s]#015 20%|█▉        | 153/782 [01:25<05:53,  1.78it/s]#015 20%|█▉        | 154/782 [01:26<05:51,  1.79it/s]#015 20%|█▉        | 155/782 [01:26<05:57,  1.75it/s]#015 20%|█▉        | 156/782 [01:27<05:52,  1.77it/s]#015 20%|██        | 157/782 [01:27<05:49,  1.79it/s]#015 20%|██        | 158/782 [01:28<05:49,  1.79it/s]#015 20%|██        | 159/782 [01:29<05:51,  1.77it/s]#015 20%|██        | 160/782 [01:29<05:49,  1.78it/s]#015 21%|██        | 161/782 [01:30<05:46,  1.79it/s]#015 21%|██        | 162/782 [01:30<05:42,  1.81it/s]#015 21%|██        | 163/782 [01:31<05:40,  1.82it/s]#015 21%|██        | 164/782 [01:31<05:39,  1.82it/s]#015 21%|██        | 165/782 [01:32<05:39,  1.82it/s]#015 21%|██        | 166/782 [01:32<05:38,  1.82it/s]#015 21%|██▏       | 167/782 [01:33<05:40,  1.80it/s]#015 21%|██▏       | 168/782 [01:34<05:42,  1.79it/s]#015 22%|██▏       | 169/782 [01:34<05:48,  1.76it/s]#015 22%|██▏       | 170/782 [01:35<05:44,  1.78it/s]#015 22%|██▏       | 171/782 [01:35<05:40,  1.80it/s]#015 22%|██▏       | 172/782 [01:36<05:38,  1.80it/s]#015 22%|██▏       | 173/782 [01:36<05:47,  1.75it/s]#015 22%|██▏       | 174/782 [01:37<05:43,  1.77it/s]#015 22%|██▏       | 175/782 [01:38<05:39,  1.79it/s]#015 23%|██▎       | 176/782 [01:38<05:39,  1.79it/s]#015 23%|██▎       | 177/782 [01:39<05:38,  1.79it/s]#015 23%|██▎       | 178/782 [01:39<05:37,  1.79it/s]#015 23%|██▎       | 179/782 [01:40<05:35,  1.80it/s]#015 23%|██▎       | 180/782 [01:40<05:34,  1.80it/s]#015 23%|██▎       | 181/782 [01:41<05:32,  1.81it/s]#015 23%|██▎       | 182/782 [01:41<05:31,  1.81it/s]#015 23%|██▎       | 183/782 [01:42<05:30,  1.81it/s]#015 24%|██▎       | 184/782 [01:42<05:28,  1.82it/s]#015 24%|██▎       | 185/782 [01:43<05:27,  1.82it/s]#015 24%|██▍       | 186/782 [01:44<05:27,  1.82it/s]#015 24%|██▍       | 187/782 [01:44<05:27,  1.82it/s]#015 24%|██▍       | 188/782 [01:45<05:27,  1.81it/s]#015 24%|██▍       | 189/782 [01:45<05:26,  1.81it/s]#015 24%|██▍       | 190/782 [01:46<05:26,  1.82it/s]#015 24%|██▍       | 191/782 [01:46<05:25,  1.81it/s]#015 25%|██▍       | 192/782 [01:47<05:26,  1.81it/s]#015 25%|██▍       | 193/782 [01:47<05:26,  1.80it/s]#015 25%|██▍       | 194/782 [01:48<05:36,  1.75it/s]#015 25%|██▍       | 195/782 [01:49<05:38,  1.73it/s]#015 25%|██▌       | 196/782 [01:49<05:32,  1.76it/s]#015 25%|██▌       | 197/782 [01:50<05:30,  1.77it/s]#015 25%|██▌       | 198/782 [01:50<05:28,  1.78it/s]#015 25%|██▌       | 199/782 [01:51<05:35,  1.74it/s]#015 26%|██▌       | 200/782 [01:51<05:29,  1.76it/s]#015 26%|██▌       | 201/782 [01:52<05:26,  1.78it/s]#015 26%|██▌       | 202/782 [01:53<05:24,  1.79it/s]#015 26%|██▌       | 203/782 [01:53<05:21,  1.80it/s]#015 26%|██▌       | 204/782 [01:54<05:20,  1.81it/s]#015 26%|██▌       | 205/782 [01:54<05:18,  1.81it/s]#015 26%|██▋       | 206/782 [01:55<05:17,  1.82it/s]#015 26%|██▋       | 207/782 [01:55<05:16,  1.82it/s]#015 27%|██▋       | 208/782 [01:56<05:16,  1.82it/s]#015 27%|██▋       | 209/782 [01:56<05:17,  1.81it/s]#015 27%|██▋       | 210/782 [01:57<05:16,  1.81it/s]#015 27%|██▋       | 211/782 [01:58<05:15,  1.81it/s]#015 27%|██▋       | 212/782 [01:58<05:13,  1.82it/s]#015 27%|██▋       | 213/782 [01:59<05:15,  1.81it/s]#015 27%|██▋       | 214/782 [01:59<05:14,  1.80it/s]#015 27%|██▋       | 215/782 [02:00<05:15,  1.80it/s]#015 28%|██▊       | 216/782 [02:00<05:13,  1.80it/s]#015 28%|██▊       | 217/782 [02:01<05:17,  1.78it/s]#015 28%|██▊       | 218/782 [02:01<05:16,  1.78it/s]#015 28%|██▊       | 219/782 [02:02<05:14,  1.79it/s]#015 28%|██▊       | 220/782 [02:03<05:13,  1.79it/s]#015 28%|██▊       | 221/782 [02:03<05:10,  1.81it/s]#015 28%|██▊       | 222/782 [02:04<05:10,  1.80it/s]#015 29%|██▊       | 223/782 [02:04<05:09,  1.81it/s]#015 29%|██▊       | 224/782 [02:05<05:07,  1.81it/s]#015 29%|██▉       | 225/782 [02:05<05:07,  1.81it/s]#015 29%|██▉       | 226/782 [02:06<05:08,  1.80it/s]#015 29%|██▉       | 227/782 [02:06<05:06,  1.81it/s]#015 29%|██▉       | 228/782 [02:07<05:05,  1.81it/s]#015 29%|██▉       | 229/782 [02:08<05:04,  1.82it/s]#015 29%|██▉       | 230/782 [02:08<05:06,  1.80it/s]#015 30%|██▉       | 231/782 [02:09<05:06,  1.79it/s]#015 30%|██▉       | 232/782 [02:09<05:04,  1.80it/s]#015 30%|██▉       | 233/782 [02:10<05:05,  1.80it/s]#015 30%|██▉       | 234/782 [02:10<05:04,  1.80it/s]#015 30%|███       | 235/782 [02:11<05:07,  1.78it/s]#015 30%|███       | 236/782 [02:11<05:04,  1.79it/s]#015 30%|███       | 237/782 [02:12<05:03,  1.80it/s]#015 30%|███       | 238/782 [02:13<05:01,  1.80it/s]#015 31%|███       | 239/782 [02:13<05:00,  1.81it/s]#015 31%|███       | 240/782 [02:14<04:59,  1.81it/s]#015 31%|███       | 241/782 [02:14<04:59,  1.80it/s]#015 31%|███       | 242/782 [02:15<04:59,  1.80it/s]#015 31%|███       | 243/782 [02:15<04:59,  1.80it/s]#015 31%|███       | 244/782 [02:16<04:57,  1.81it/s]#015 31%|███▏      | 245/782 [02:16<04:57,  1.80it/s]#015 31%|███▏      | 246/782 [02:17<04:57,  1.80it/s]#015 32%|███▏      | 247/782 [02:18<04:56,  1.81it/s]#015 32%|███▏      | 248/782 [02:18<04:55,  1.81it/s]#015 32%|███▏      | 249/782 [02:19<04:55,  1.81it/s]#015 32%|███▏      | 250/782 [02:19<04:53,  1.81it/s]#015 32%|███▏      | 251/782 [02:20<04:55,  1.80it/s]#015 32%|███▏      | 252/782 [02:20<04:54,  1.80it/s]#015 32%|███▏      | 253/782 [02:21<04:53,  1.80it/s]#015 32%|███▏      | 254/782 [02:21<04:52,  1.81it/s]#015 33%|███▎      | 255/782 [02:22<04:50,  1.82it/s]#015 33%|███▎      | 256/782 [02:22<04:49,  1.82it/s]#015 33%|███▎      | 257/782 [02:23<04:48,  1.82it/s]#015 33%|███▎      | 258/782 [02:24<04:48,  1.82it/s]#015 33%|███▎      | 259/782 [02:24<04:52,  1.79it/s]#015 33%|███▎      | 260/782 [02:25<04:50,  1.80it/s]#015 33%|███▎      | 261/782 [02:25<04:53,  1.78it/s]#015 34%|███▎      | 262/782 [02:26<04:52,  1.78it/s]#015 34%|███▎      | 263/782 [02:26<04:55,  1.76it/s]#015 34%|███▍      | 264/782 [02:27<04:53,  1.77it/s]#015 34%|███▍      | 265/782 [02:28<04:50,  1.78it/s]#015 34%|███▍      | 266/782 [02:28<04:48,  1.79it/s]#015 34%|███▍      | 267/782 [02:29<04:48,  1.79it/s]#015 34%|███▍      | 268/782 [02:29<04:46,  1.79it/s]#015 34%|███▍      | 269/782 [02:30<04:45,  1.80it/s]#015 35%|███▍      | 270/782 [02:30<04:44,  1.80it/s]#015 35%|███▍      | 271/782 [02:31<04:43,  1.80it/s]#015 35%|███▍      | 272/782 [02:31<04:44,  1.80it/s]#015 35%|███▍      | 273/782 [02:32<04:43,  1.80it/s]#015 35%|███▌      | 274/782 [02:33<04:41,  1.80it/s]#015 35%|███▌      | 275/782 [02:33<04:40,  1.81it/s]#015 35%|███▌      | 276/782 [02:34<04:39,  1.81it/s]#015 35%|███▌      | 277/782 [02:34<04:40,  1.80it/s]#015 36%|███▌      | 278/782 [02:35<04:39,  1.81it/s]#015 36%|███▌      | 279/782 [02:35<04:36,  1.82it/s]#015 36%|███▌      | 280/782 [02:36<04:36,  1.82it/s]#015 36%|███▌      | 281/782 [02:36<04:36,  1.81it/s]#015 36%|███▌      | 282/782 [02:37<04:35,  1.82it/s]#015 36%|███▌      | 283/782 [02:38<04:35,  1.81it/s]#015 36%|███▋      | 284/782 [02:38<04:35,  1.81it/s]#015 36%|███▋      | 285/782 [02:39<04:39,  1.78it/s]#015 37%|███▋      | 286/782 [02:39<04:38,  1.78it/s]#015 37%|███▋      | 287/782 [02:40<04:39,  1.77it/s]#015 37%|███▋      | 288/782 [02:40<04:37,  1.78it/s]#015 37%|███▋      | 289/782 [02:41<04:36,  1.79it/s]#015 37%|███▋      | 290/782 [02:41<04:34,  1.80it/s]#015 37%|███▋      | 291/782 [02:42<04:33,  1.79it/s]#015 37%|███▋      | 292/782 [02:43<04:32,  1.80it/s]#015 37%|███▋      | 293/782 [02:43<04:33,  1.79it/s]#015 38%|███▊      | 294/782 [02:44<04:31,  1.80it/s]#015 38%|███▊      | 295/782 [02:44<04:30,  1.80it/s]#015 38%|███▊      | 296/782 [02:45<04:29,  1.80it/s]#015 38%|███▊      | 297/782 [02:45<04:28,  1.81it/s]#015 38%|███▊      | 298/782 [02:46<04:28,  1.80it/s]#015 38%|███▊      | 299/782 [02:46<04:27,  1.81it/s]#015 38%|███▊      | 300/782 [02:47<04:26,  1.81it/s]#015 38%|███▊      | 301/782 [02:48<04:26,  1.80it/s]#015 39%|\u001b[0m\n",
      "\u001b[34m███▊      | 302/782 [02:48<04:26,  1.80it/s]#015 39%|███▊      | 303/782 [02:49<04:27,  1.79it/s]#015 39%|███▉      | 304/782 [02:49<04:27,  1.78it/s]#015 39%|███▉      | 305/782 [02:50<04:26,  1.79it/s]#015 39%|███▉      | 306/782 [02:50<04:24,  1.80it/s]#015 39%|███▉      | 307/782 [02:51<04:23,  1.80it/s]#015 39%|███▉      | 308/782 [02:51<04:21,  1.81it/s]#015 40%|███▉      | 309/782 [02:52<04:21,  1.81it/s]#015 40%|███▉      | 310/782 [02:53<04:20,  1.81it/s]#015 40%|███▉      | 311/782 [02:53<04:19,  1.82it/s]#015 40%|███▉      | 312/782 [02:54<04:18,  1.82it/s]#015 40%|████      | 313/782 [02:54<04:20,  1.80it/s]#015 40%|████      | 314/782 [02:55<04:19,  1.81it/s]#015 40%|████      | 315/782 [02:55<04:19,  1.80it/s]#015 40%|████      | 316/782 [02:56<04:18,  1.81it/s]#015 41%|████      | 317/782 [02:56<04:17,  1.81it/s]#015 41%|████      | 318/782 [02:57<04:17,  1.80it/s]#015 41%|████      | 319/782 [02:58<04:16,  1.81it/s]#015 41%|████      | 320/782 [02:58<04:15,  1.81it/s]#015 41%|████      | 321/782 [02:59<04:14,  1.81it/s]#015 41%|████      | 322/782 [02:59<04:15,  1.80it/s]#015 41%|████▏     | 323/782 [03:00<04:14,  1.81it/s]#015 41%|████▏     | 324/782 [03:00<04:14,  1.80it/s]#015 42%|████▏     | 325/782 [03:01<04:12,  1.81it/s]#015 42%|████▏     | 326/782 [03:01<04:13,  1.80it/s]#015 42%|████▏     | 327/782 [03:02<04:13,  1.79it/s]#015 42%|████▏     | 328/782 [03:03<04:18,  1.75it/s]#015 42%|████▏     | 329/782 [03:03<04:15,  1.77it/s]#015 42%|████▏     | 330/782 [03:04<04:13,  1.79it/s]#015 42%|████▏     | 331/782 [03:04<04:11,  1.80it/s]#015 42%|████▏     | 332/782 [03:05<04:11,  1.79it/s]#015 43%|████▎     | 333/782 [03:05<04:09,  1.80it/s]#015 43%|████▎     | 334/782 [03:06<04:08,  1.80it/s]#015 43%|████▎     | 335/782 [03:06<04:08,  1.80it/s]#015 43%|████▎     | 336/782 [03:07<04:07,  1.80it/s]#015 43%|████▎     | 337/782 [03:08<04:07,  1.80it/s]#015 43%|████▎     | 338/782 [03:08<04:12,  1.76it/s]#015 43%|████▎     | 339/782 [03:09<04:10,  1.77it/s]#015 43%|████▎     | 340/782 [03:09<04:09,  1.77it/s]#015 44%|████▎     | 341/782 [03:10<04:08,  1.77it/s]#015 44%|████▎     | 342/782 [03:10<04:05,  1.79it/s]#015 44%|████▍     | 343/782 [03:11<04:05,  1.79it/s]#015 44%|████▍     | 344/782 [03:11<04:04,  1.79it/s]#015 44%|████▍     | 345/782 [03:12<04:03,  1.79it/s]#015 44%|████▍     | 346/782 [03:13<04:02,  1.80it/s]#015 44%|████▍     | 347/782 [03:13<04:02,  1.80it/s]#015 45%|████▍     | 348/782 [03:14<04:00,  1.80it/s]#015 45%|████▍     | 349/782 [03:14<04:04,  1.77it/s]#015 45%|████▍     | 350/782 [03:15<04:02,  1.78it/s]#015 45%|████▍     | 351/782 [03:15<04:01,  1.78it/s]#015 45%|████▌     | 352/782 [03:16<04:00,  1.79it/s]#015 45%|████▌     | 353/782 [03:17<03:58,  1.80it/s]#015 45%|████▌     | 354/782 [03:17<03:58,  1.80it/s]#015 45%|████▌     | 355/782 [03:18<03:57,  1.79it/s]#015 46%|████▌     | 356/782 [03:18<03:57,  1.80it/s]#015 46%|████▌     | 357/782 [03:19<03:56,  1.80it/s]#015 46%|████▌     | 358/782 [03:19<03:55,  1.80it/s]#015 46%|████▌     | 359/782 [03:20<03:55,  1.79it/s]#015 46%|████▌     | 360/782 [03:20<03:55,  1.79it/s]#015 46%|████▌     | 361/782 [03:21<03:54,  1.80it/s]#015 46%|████▋     | 362/782 [03:22<03:53,  1.80it/s]#015 46%|████▋     | 363/782 [03:22<03:53,  1.79it/s]#015 47%|████▋     | 364/782 [03:23<03:52,  1.80it/s]#015 47%|████▋     | 365/782 [03:23<03:51,  1.80it/s]#015 47%|████▋     | 366/782 [03:24<03:53,  1.78it/s]#015 47%|████▋     | 367/782 [03:24<03:54,  1.77it/s]#015 47%|████▋     | 368/782 [03:25<03:52,  1.78it/s]#015 47%|████▋     | 369/782 [03:25<03:54,  1.76it/s]#015 47%|████▋     | 370/782 [03:26<03:54,  1.76it/s]#015 47%|████▋     | 371/782 [03:27<03:59,  1.72it/s]#015 48%|████▊     | 372/782 [03:27<03:54,  1.74it/s]#015 48%|████▊     | 373/782 [03:28<03:53,  1.75it/s]#015 48%|████▊     | 374/782 [03:28<03:50,  1.77it/s]#015 48%|████▊     | 375/782 [03:29<03:48,  1.78it/s]#015 48%|████▊     | 376/782 [03:29<03:46,  1.79it/s]#015 48%|████▊     | 377/782 [03:30<03:47,  1.78it/s]#015 48%|████▊     | 378/782 [03:31<03:45,  1.79it/s]#015 48%|████▊     | 379/782 [03:31<03:43,  1.80it/s]#015 49%|████▊     | 380/782 [03:32<03:42,  1.80it/s]#015 49%|████▊     | 381/782 [03:32<03:40,  1.82it/s]#015 49%|████▉     | 382/782 [03:33<03:39,  1.82it/s]#015 49%|████▉     | 383/782 [03:33<03:39,  1.82it/s]#015 49%|████▉     | 384/782 [03:34<03:39,  1.81it/s]#015 49%|████▉     | 385/782 [03:34<03:38,  1.82it/s]#015 49%|████▉     | 386/782 [03:35<03:38,  1.81it/s]#015 49%|████▉     | 387/782 [03:36<03:38,  1.81it/s]#015 50%|████▉     | 388/782 [03:36<03:38,  1.80it/s]#015 50%|████▉     | 389/782 [03:37<03:38,  1.80it/s]#015 50%|████▉     | 390/782 [03:37<03:36,  1.81it/s]#015 50%|█████     | 391/782 [03:38<03:38,  1.79it/s]#015 50%|█████     | 392/782 [03:38<03:37,  1.79it/s]#015 50%|█████     | 393/782 [03:39<03:43,  1.74it/s]#015 50%|█████     | 394/782 [03:39<03:39,  1.77it/s]#015 51%|█████     | 395/782 [03:40<03:38,  1.77it/s]#015 51%|█████     | 396/782 [03:41<03:36,  1.78it/s]#015 51%|█████     | 397/782 [03:41<03:35,  1.78it/s]#015 51%|█████     | 398/782 [03:42<03:34,  1.79it/s]#015 51%|█████     | 399/782 [03:42<03:34,  1.79it/s]#015 51%|█████     | 400/782 [03:43<03:32,  1.80it/s]#015 51%|█████▏    | 401/782 [03:43<03:31,  1.80it/s]#015 51%|█████▏    | 402/782 [03:44<03:29,  1.81it/s]#015 52%|█████▏    | 403/782 [03:44<03:29,  1.81it/s]#015 52%|█████▏    | 404/782 [03:45<03:29,  1.80it/s]#015 52%|█████▏    | 405/782 [03:46<03:29,  1.80it/s]#015 52%|█████▏    | 406/782 [03:46<03:28,  1.80it/s]#015 52%|█████▏    | 407/782 [03:47<03:29,  1.79it/s]#015 52%|█████▏    | 408/782 [03:47<03:27,  1.80it/s]#015 52%|█████▏    | 409/782 [03:48<03:27,  1.79it/s]#015 52%|█████▏    | 410/782 [03:48<03:26,  1.80it/s]#015 53%|█████▎    | 411/782 [03:49<03:25,  1.81it/s]#015 53%|█████▎    | 412/782 [03:49<03:24,  1.81it/s]#015 53%|█████▎    | 413/782 [03:50<03:24,  1.81it/s]#015 53%|█████▎    | 414/782 [03:51<03:23,  1.81it/s]#015 53%|█████▎    | 415/782 [03:51<03:23,  1.81it/s]#015 53%|█████▎    | 416/782 [03:52<03:23,  1.80it/s]#015 53%|█████▎    | 417/782 [03:52<03:22,  1.80it/s]#015 53%|█████▎    | 418/782 [03:53<03:28,  1.75it/s]#015 54%|█████▎    | 419/782 [03:53<03:26,  1.76it/s]#015 54%|█████▎    | 420/782 [03:54<03:23,  1.78it/s]#015 54%|█████▍    | 421/782 [03:54<03:21,  1.79it/s]#015 54%|█████▍    | 422/782 [03:55<03:20,  1.80it/s]#015 54%|█████▍    | 423/782 [03:56<03:19,  1.80it/s]#015 54%|█████▍    | 424/782 [03:56<03:18,  1.80it/s]#015 54%|█████▍    | 425/782 [03:57<03:17,  1.80it/s]#015 54%|█████▍    | 426/782 [03:57<03:18,  1.79it/s]#015 55%|█████▍    | 427/782 [03:58<03:18,  1.79it/s]#015 55%|█████▍    | 428/782 [03:58<03:19,  1.78it/s]#015 55%|█████▍    | 429/782 [03:59<03:17,  1.79it/s]#015 55%|█████▍    | 430/782 [04:00<03:16,  1.79it/s]#015 55%|█████▌    | 431/782 [04:00<03:15,  1.80it/s]#015 55%|█████▌    | 432/782 [04:01<03:16,  1.78it/s]#015 55%|█████▌    | 433/782 [04:01<03:15,  1.78it/s]#015 55%|█████▌    | 434/782 [04:02<03:14,  1.79it/s]#015 56%|█████▌    | 435/782 [04:02<03:14,  1.79it/s]#015 56%|█████▌    | 436/782 [04:03<03:19,  1.74it/s]#015 56%|█████▌    | 437/782 [04:04<03:22,  1.70it/s]#015 56%|█████▌    | 438/782 [04:04<03:23,  1.69it/s]#015 56%|█████▌    | 439/782 [04:05<03:19,  1.72it/s]#015 56%|█████▋    | 440/782 [04:05<03:16,  1.74it/s]#015 56%|█████▋    | 441/782 [04:06<03:13,  1.76it/s]#015 57%|█████▋    | 442/782 [04:06<03:12,  1.76it/s]#015 57%|█████▋    | 443/782 [04:07<03:10,  1.78it/s]#015 57%|█████▋    | 444/782 [04:07<03:09,  1.78it/s]#015 57%|█████▋    | 445/782 [04:08<03:07,  1.79it/s]#015 57%|█████▋    | 446/782 [04:09<03:06,  1.80it/s]#015 57%|█████▋    | 447/782 [04:09<03:05,  1.80it/s]#015 57%|█████▋    | 448/782 [04:10<03:07,  1.78it/s]#015 57%|█████▋    | 449/782 [04:10<03:05,  1.80it/s]#015 58%|█████▊    | 450/782 [04:11<03:04,  1.80it/s]#015 58%|█████▊    | 451/782 [04:11<03:04,  1.79it/s]#015 58%|█████▊    | 452/782 [04:12<03:04,  1.79it/s]#015 58%|█████▊    | 453/782 [04:13<03:05,  1.78it/s]#015 58%|█████▊    | 454/782 [04:13<03:03,  1.79it/s]#015 58%|█████▊    | 455/782 [04:14<03:02,  1.79it/s]#015 58%|█████▊    | 456/782 [04:14<03:01,  1.79it/s]#015 58%|█████▊    | 457/782 [04:15<03:02,  1.78it/s]#015 59%|█████▊    | 458/782 [04:15<03:00,  1.79it/s]#015 59%|█████▊    | 459/782 [04:16<02:59,  1.80it/s]#015 59%|█████▉    | 460/782 [04:16<02:59,  1.80it/s]#015 59%|█████▉    | 461/782 [04:17<02:58,  1.80it/s]#015 59%|█████▉    | 462/782 [04:18<03:00,  1.78it/s]#015 59%|█████▉    | 463/782 [04:18<02:58,  1.79it/s]#015 59%|█████▉    | 464/782 [04:19<02:57,  1.79it/s]#015 59%|█████▉    | 465/782 [04:19<02:56,  1.80it/s]#015 60%|█████▉    | 466/782 [04:20<02:55,  1.80it/s]#015 60%|█████▉    | 467/782 [04:20<02:55,  1.80it/s]#015 60%|█████▉    | 468/782 [04:21<02:54,  1.80it/s]#015 60%|█████▉    | 469/782 [04:21<02:53,  1.80it/s]#015 60%|██████    | 470/782 [04:22<02:54,  1.79it/s]#015 60%|██████    | 471/782 [04:23<02:58,  1.74it/s]#015 60%|██████    | 472/782 [04:23<02:56,  1.76it/s]#015 60%|██████    | 473/782 [04:24<02:53,  1.78it/s]#015 61%|██████    | 474/782 [04:24<02:53,  1.77it/s]#015 61%|██████    | 475/782 [04:25<02:51,  1.79it/s]#015 61%|██████    | 476/782 [04:25<02:52,  1.78it/s]#015 61%|██████    | 477/782 [04:26<02:52,  1.77it/s]#015 61%|██████    | 478/782 [04:27<02:52,  1.76it/s]#015 61%|██████▏   | 479/782 [04:27<02:51,  1.77it/s]#015 61%|██████▏   | 480/782 [04:28<02:50,  1.77it/s]#015 62%|██████▏   | 481/782 [04:28<02:48,  1.78it/s]#015 62%|██████▏   | 482/782 [04:29<02:48,  1.78it/s]#015 62%|██████▏   | 483/782 [04:29<02:46,  1.79it/s]#015 62%|██████▏   | 484/782 [04:30<02:45,  1.80it/s]#015 62%|██████▏   | 485/782 [04:30<02:44,  1.81it/s]#015 62%|██████▏   | 486/782 [04:31<02:44,  1.80it/s]#015 62%|██████▏   | 487/782 [04:32<02:43,  1.81it/s]#015 62%|██████▏   | 488/782 [04:32<02:42,  1.80it/s]#015 63%|██████▎   | 489/782 [04:33<02:42,  1.81it/s]#015 63%|██████▎   | 490/782 [04:33<02:41,  1.81it/s]#015 63%|██████▎   | 491/782 [04:34<02:41,  1.80it/s]#015 63%|██████▎   | 492/782 [04:34<02:40,  1.80it/s]#015 63%|██████▎   | 493/782 [04:35<02:40,  1.80it/s]#015 63%|██████▎   | 494/782 [04:35<02:42,  1.77it/s]#015 63%|██████▎   | 495/782 [04:36<02:41,  1.78it/s]#015 63%|██████▎   | 496/782 [04:37<02:40,  1.78it/s]#015 64%|██████▎   | 497/782 [04:37<02:39,  1.79it/s]#015 64%|██████▎   | 498/782 [04:38<02:38,  1.79it/s]#015 64%|██████▍   | 499/782 [04:38<02:37,  1.79it/s]#015 64%|██████▍   | 500/782 [04:39<02:36,  1.80it/s]#015                                                 #015#015 64%|██████▍   | 500/782 [04:39<02:36,  1.80it/s]#015 64%|██████▍   | 501/782 [04:41<05:35,  1.19s/it]#015 64%|██████▍   | 502/782 [04:42<04:40,  1.00s/it]#015 64%|██████▍   | 503/782 [04:43<04:01,  1.15it/s]#015 64%|██████▍   | 504/782 [04:43<03:34,  1.30it/s]#015 65%|██████▍   | 505/782 [04:44<03:15,  1.42it/s]#015 65%|██████▍   | 506/782 [04:44<03:02,  1.51it/s]#015 65%|██████▍   | 507/782 [04:45<02:52,  1.60it/s]#015 65%|██████▍   | 508/782 [04:45<02:45,  1.66it/s]#015 65%|██████▌   | 509/782 [04:46<02:40,  1.70it/s]#015 65%|██████▌   | 510/782 [04:46<02:38,  1.72it/s]#015 65%|██████▌   | 511/782 [04:47<02:36,  1.73it/s]#015 65%|██████▌   | 512/782 [04:48<02:34,  1.75it/s]#015 66%|██████▌   | 513/782 [04:48<02:32,  1.77it/s]#015 66%|██████▌   | 514/782 [04:49<02:30,  1.78it/s]#015 66%|██████▌   | 515/782 [04:49<02:29,  1.79it/s]#015 66%|██████▌   | 516/782 [04:50<02:28,  1.79it/s]#015 66%|██████▌   | 517/782 [04:50<02:27,  1.80it/s]#015 66%|██████▌   | 518/782 [04:51<02:28,  1.77it/s]#015 66%|██████▋   | 519/782 [04:51<02:28,  1.78it/s]#015 66%|██████▋   | 520/782 [04:52<02:27,  1.78it/s]#015 67%|██████▋   | 521/782 [04:53<02:26,  1.79it/s]#015 67%|██████▋   | 522/782 [04:53<02:25,  1.79it/s]#015 67%|██████▋   | 523/782 [04:54<02:23,  1.80it/s]#015 67%|██████▋   | 524/782 [04:54<02:22,  1.81it/s]#015 67%|██████▋   | 525/782 [04:55<02:21,  1.81it/s]#015 67%|██████▋   | 526/782 [04:55<02:21,  1.82it/s]#015 67%|██████▋   | 527/782 [04:56<02:20,  1.82it/s]#015 68%|██████▊   | 528/782 [04:56<02:19,  1.82it/s]#015 68%|██████▊   | 529/782 [04:57<02:19,  1.82it/s]#015 68%|██████▊   | 530/782 [04:58<02:19,  1.81it/s]#015 68%|██████▊   | 531/782 [04:58<02:18,  1.81it/s]#015 68%|██████▊   | 532/782 [04:59<02:18,  1.80it/s]#015 68%|██████▊   | 533/782 [04:59<02:18,  1.80it/s]#015 68%|██████▊   | 534/782 [05:00<02:19,  1.77it/s]#015 68%|██████▊   | 535/782 [05:00<02:18,  1.79it/s]#015 69%|██████▊   | 536/782 [05:01<02:17,  1.79it/s]#015 69%|██████▊   | 537/782 [05:01<02:17,  1.79it/s]#015 69%|██████▉   | 538/782 [05:02<02:16,  1.79it/s]#015 69%|██████▉   | 539/782 [05:03<02:15,  1.79it/s]#015 69%|██████▉   | 540/782 [05:03<02:14,  1.80it/s]#015 69%|██████▉   | 541/782 [05:04<02:15,  1.77it/s]#015 69%|██████▉   | 542/782 [05:04<02:14,  1.78it/s]#015 69%|██████▉   | 543/782 [05:05<02:13,  1.79it/s]#015 70%|██████▉   | 544/782 [05:05<02:11,  1.80it/s]#015 70%|██████▉   | 545/782 [05:06<02:11,  1.81it/s]#015 70%|██████▉   | 546/782 [05:06<02:10,  1.81it/s]#015 70%|██████▉   | 547/782 [05:07<02:09,  1.81it/s]#015 70%|███████   | 548/782 [05:08<02:12,  1.77it/s]#015 70%|███████   | 549/782 [05:08<02:10,  1.78it/s]#015 70%|███████   | 550/782 [05:09<02:09,  1.79it/s]#015 70%|███████   | 551/782 [05:09<02:12,  1.74it/s]#015 71%|███████   | 552/782 [05:10<02:10,  1.77it/s]#015 71%|███████   | 553/782 [05:10<02:08,  1.78it/s]#015 71%|███████   | 554/782 [05:11<02:07,  1.79it/s]#015 71%|███████   | 555/782 [05:12<02:06,  1.80it/s]#015 71%|███████   | 556/782 [05:12<02:07,  1.77it/s]#015 71%|███████   | 557/782 [05:13<02:06,  1.78it/s]#015 71%|███████▏  | 558/782 [05:13<02:04,  1.79it/s]#015 71%|███████▏  | 559/782 [05:14<02:03,  1.80it/s]#015 72%|███████▏  | 560/782 [05:14<02:03,  1.80it/s]#015 72%|███████▏  | 561/782 [05:15<02:06,  1.74it/s]#015 72%|███████▏  | 562/782 [05:16<02:05,  1.75it/s]#015 72%|███████▏  | 563/782 [05:16<02:04,  1.76it/s]#015 72%|███████▏  | 564/782 [05:17<02:03,  1.77it/s]#015 72%|█████�\u001b[0m\n",
      "\u001b[34m2021-08-09 19:11:41,204 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m�█▏  | 565/782 [05:17<02:02,  1.78it/s]#015 72%|███████▏  | 566/782 [05:18<02:00,  1.79it/s]#015 73%|███████▎  | 567/782 [05:18<02:00,  1.79it/s]#015 73%|███████▎  | 568/782 [05:19<02:00,  1.78it/s]#015 73%|███████▎  | 569/782 [05:19<01:58,  1.79it/s]#015 73%|███████▎  | 570/782 [05:20<01:59,  1.77it/s]#015 73%|███████▎  | 571/782 [05:21<01:58,  1.77it/s]#015 73%|███████▎  | 572/782 [05:21<01:58,  1.77it/s]#015 73%|███████▎  | 573/782 [05:22<01:57,  1.78it/s]#015 73%|███████▎  | 574/782 [05:22<01:56,  1.79it/s]#015 74%|███████▎  | 575/782 [05:23<01:55,  1.79it/s]#015 74%|███████▎  | 576/782 [05:23<01:55,  1.79it/s]#015 74%|███████▍  | 577/782 [05:24<01:55,  1.77it/s]#015 74%|███████▍  | 578/782 [05:24<01:55,  1.77it/s]#015 74%|███████▍  | 579/782 [05:25<01:55,  1.75it/s]#015 74%|███████▍  | 580/782 [05:26<01:57,  1.72it/s]#015 74%|███████▍  | 581/782 [05:26<01:56,  1.73it/s]#015 74%|███████▍  | 582/782 [05:27<01:54,  1.74it/s]#015 75%|███████▍  | 583/782 [05:27<01:53,  1.76it/s]#015 75%|███████▍  | 584/782 [05:28<01:51,  1.78it/s]#015 75%|███████▍  | 585/782 [05:28<01:49,  1.79it/s]#015 75%|███████▍  | 586/782 [05:29<01:49,  1.79it/s]#015 75%|███████▌  | 587/782 [05:30<01:48,  1.80it/s]#015 75%|███████▌  | 588/782 [05:30<01:47,  1.80it/s]#015 75%|███████▌  | 589/782 [05:31<01:47,  1.79it/s]#015 75%|███████▌  | 590/782 [05:31<01:46,  1.80it/s]#015 76%|███████▌  | 591/782 [05:32<01:46,  1.79it/s]#015 76%|███████▌  | 592/782 [05:32<01:46,  1.79it/s]#015 76%|███████▌  | 593/782 [05:33<01:45,  1.79it/s]#015 76%|███████▌  | 594/782 [05:33<01:44,  1.79it/s]#015 76%|███████▌  | 595/782 [05:34<01:44,  1.79it/s]#015 76%|███████▌  | 596/782 [05:35<01:43,  1.79it/s]#015 76%|███████▋  | 597/782 [05:35<01:42,  1.80it/s]#015 76%|███████▋  | 598/782 [05:36<01:42,  1.80it/s]#015 77%|███████▋  | 599/782 [05:36<01:41,  1.81it/s]#015 77%|███████▋  | 600/782 [05:37<01:41,  1.79it/s]#015 77%|███████▋  | 601/782 [05:37<01:40,  1.80it/s]#015 77%|███████▋  | 602/782 [05:38<01:40,  1.79it/s]#015 77%|███████▋  | 603/782 [05:38<01:39,  1.80it/s]#015 77%|███████▋  | 604/782 [05:39<01:38,  1.81it/s]#015 77%|███████▋  | 605/782 [05:40<01:37,  1.81it/s]#015 77%|███████▋  | 606/782 [05:40<01:36,  1.82it/s]#015 78%|███████▊  | 607/782 [05:41<01:36,  1.81it/s]#015 78%|███████▊  | 608/782 [05:41<01:35,  1.81it/s]#015 78%|███████▊  | 609/782 [05:42<01:35,  1.81it/s]#015 78%|███████▊  | 610/782 [05:42<01:35,  1.80it/s]#015 78%|███████▊  | 611/782 [05:43<01:34,  1.80it/s]#015 78%|███████▊  | 612/782 [05:43<01:34,  1.81it/s]#015 78%|███████▊  | 613/782 [05:44<01:33,  1.81it/s]#015 79%|███████▊  | 614/782 [05:45<01:33,  1.80it/s]#015 79%|███████▊  | 615/782 [05:45<01:33,  1.79it/s]#015 79%|███████▉  | 616/782 [05:46<01:32,  1.79it/s]#015 79%|███████▉  | 617/782 [05:46<01:31,  1.79it/s]#015 79%|███████▉  | 618/782 [05:47<01:30,  1.80it/s]#015 79%|███████▉  | 619/782 [05:47<01:30,  1.81it/s]#015 79%|███████▉  | 620/782 [05:48<01:30,  1.80it/s]#015 79%|███████▉  | 621/782 [05:48<01:29,  1.81it/s]#015 80%|███████▉  | 622/782 [05:49<01:28,  1.80it/s]#015 80%|███████▉  | 623/782 [05:50<01:28,  1.80it/s]#015 80%|███████▉  | 624/782 [05:50<01:27,  1.81it/s]#015 80%|███████▉  | 625/782 [05:51<01:26,  1.81it/s]#015 80%|████████  | 626/782 [05:51<01:26,  1.81it/s]#015 80%|████████  | 627/782 [05:52<01:25,  1.81it/s]#015 80%|████████  | 628/782 [05:52<01:24,  1.81it/s]#015 80%|████████  | 629/782 [05:53<01:24,  1.82it/s]#015 81%|████████  | 630/782 [05:53<01:24,  1.80it/s]#015 81%|████████  | 631/782 [05:54<01:24,  1.79it/s]#015 81%|████████  | 632/782 [05:55<01:24,  1.78it/s]#015 81%|████████  | 633/782 [05:55<01:24,  1.77it/s]#015 81%|████████  | 634/782 [05:56<01:23,  1.78it/s]#015 81%|████████  | 635/782 [05:56<01:22,  1.79it/s]#015 81%|████████▏ | 636/782 [05:57<01:21,  1.80it/s]#015 81%|████████▏ | 637/782 [05:57<01:20,  1.80it/s]#015 82%|████████▏ | 638/782 [05:58<01:19,  1.80it/s]#015 82%|████████▏ | 639/782 [05:58<01:19,  1.81it/s]#015 82%|████████▏ | 640/782 [05:59<01:18,  1.81it/s]#015 82%|████████▏ | 641/782 [06:00<01:18,  1.81it/s]#015 82%|████████▏ | 642/782 [06:00<01:17,  1.80it/s]#015 82%|████████▏ | 643/782 [06:01<01:17,  1.80it/s]#015 82%|████████▏ | 644/782 [06:01<01:16,  1.80it/s]#015 82%|████████▏ | 645/782 [06:02<01:16,  1.80it/s]#015 83%|████████▎ | 646/782 [06:02<01:16,  1.77it/s]#015 83%|████████▎ | 647/782 [06:03<01:15,  1.78it/s]#015 83%|████████▎ | 648/782 [06:03<01:14,  1.79it/s]#015 83%|████████▎ | 649/782 [06:04<01:14,  1.79it/s]#015 83%|████████▎ | 650/782 [06:05<01:13,  1.79it/s]#015 83%|████████▎ | 651/782 [06:05<01:13,  1.78it/s]#015 83%|████████▎ | 652/782 [06:06<01:12,  1.78it/s]#015 84%|████████▎ | 653/782 [06:06<01:12,  1.79it/s]#015 84%|████████▎ | 654/782 [06:07<01:11,  1.80it/s]#015 84%|████████▍ | 655/782 [06:07<01:11,  1.79it/s]#015 84%|████████▍ | 656/782 [06:08<01:10,  1.80it/s]#015 84%|████████▍ | 657/782 [06:08<01:09,  1.81it/s]#015 84%|████████▍ | 658/782 [06:09<01:08,  1.81it/s]#015 84%|████████▍ | 659/782 [06:10<01:08,  1.81it/s]#015 84%|████████▍ | 660/782 [06:10<01:07,  1.81it/s]#015 85%|████████▍ | 661/782 [06:11<01:09,  1.75it/s]#015 85%|████████▍ | 662/782 [06:11<01:07,  1.77it/s]#015 85%|████████▍ | 663/782 [06:12<01:06,  1.78it/s]#015 85%|████████▍ | 664/782 [06:12<01:06,  1.78it/s]#015 85%|████████▌ | 665/782 [06:13<01:05,  1.79it/s]#015 85%|████████▌ | 666/782 [06:14<01:04,  1.80it/s]#015 85%|████████▌ | 667/782 [06:14<01:03,  1.80it/s]#015 85%|████████▌ | 668/782 [06:15<01:03,  1.80it/s]#015 86%|████████▌ | 669/782 [06:15<01:02,  1.80it/s]#015 86%|████████▌ | 670/782 [06:16<01:01,  1.81it/s]#015 86%|████████▌ | 671/782 [06:16<01:01,  1.80it/s]#015 86%|████████▌ | 672/782 [06:17<01:00,  1.81it/s]#015 86%|████████▌ | 673/782 [06:17<01:00,  1.80it/s]#015 86%|████████▌ | 674/782 [06:18<01:00,  1.78it/s]#015 86%|████████▋ | 675/782 [06:19<00:59,  1.79it/s]#015 86%|████████▋ | 676/782 [06:19<00:59,  1.79it/s]#015 87%|████████▋ | 677/782 [06:20<00:58,  1.79it/s]#015 87%|████████▋ | 678/782 [06:20<00:57,  1.80it/s]#015 87%|████████▋ | 679/782 [06:21<00:57,  1.80it/s]#015 87%|████████▋ | 680/782 [06:21<00:56,  1.81it/s]#015 87%|████████▋ | 681/782 [06:22<00:56,  1.79it/s]#015 87%|████████▋ | 682/782 [06:22<00:56,  1.78it/s]#015 87%|████████▋ | 683/782 [06:23<00:55,  1.79it/s]#015 87%|████████▋ | 684/782 [06:24<00:55,  1.76it/s]#015 88%|████████▊ | 685/782 [06:24<00:55,  1.76it/s]#015 88%|████████▊ | 686/782 [06:25<00:54,  1.78it/s]#015 88%|████████▊ | 687/782 [06:25<00:53,  1.77it/s]#015 88%|████████▊ | 688/782 [06:26<00:53,  1.76it/s]#015 88%|████████▊ | 689/782 [06:26<00:53,  1.75it/s]#015 88%|████████▊ | 690/782 [06:27<00:52,  1.76it/s]#015 88%|████████▊ | 691/782 [06:28<00:51,  1.77it/s]#015 88%|████████▊ | 692/782 [06:28<00:50,  1.78it/s]#015 89%|████████▊ | 693/782 [06:29<00:50,  1.76it/s]#015 89%|████████▊ | 694/782 [06:29<00:49,  1.77it/s]#015 89%|████████▉ | 695/782 [06:30<00:48,  1.78it/s]#015 89%|████████▉ | 696/782 [06:30<00:48,  1.79it/s]#015 89%|████████▉ | 697/782 [06:31<00:47,  1.79it/s]#015 89%|████████▉ | 698/782 [06:31<00:46,  1.79it/s]#015 89%|████████▉ | 699/782 [06:32<00:46,  1.80it/s]#015 90%|████████▉ | 700/782 [06:33<00:46,  1.78it/s]#015 90%|████████▉ | 701/782 [06:33<00:45,  1.76it/s]#015 90%|████████▉ | 702/782 [06:34<00:44,  1.79it/s]#015 90%|████████▉ | 703/782 [06:34<00:44,  1.79it/s]#015 90%|█████████ | 704/782 [06:35<00:43,  1.80it/s]#015 90%|█████████ | 705/782 [06:35<00:42,  1.80it/s]#015 90%|█████████ | 706/782 [06:36<00:42,  1.80it/s]#015 90%|█████████ | 707/782 [06:36<00:41,  1.80it/s]#015 91%|█████████ | 708/782 [06:37<00:40,  1.81it/s]#015 91%|█████████ | 709/782 [06:38<00:40,  1.81it/s]#015 91%|█████████ | 710/782 [06:38<00:39,  1.81it/s]#015 91%|█████████ | 711/782 [06:39<00:39,  1.79it/s]#015 91%|█████████ | 712/782 [06:39<00:39,  1.76it/s]#015 91%|█████████ | 713/782 [06:40<00:38,  1.77it/s]#015 91%|█████████▏| 714/782 [06:40<00:38,  1.78it/s]#015 91%|█████████▏| 715/782 [06:41<00:37,  1.78it/s]#015 92%|█████████▏| 716/782 [06:42<00:38,  1.73it/s]#015 92%|█████████▏| 717/782 [06:42<00:37,  1.75it/s]#015 92%|█████████▏| 718/782 [06:43<00:36,  1.75it/s]#015 92%|█████████▏| 719/782 [06:43<00:35,  1.77it/s]#015 92%|█████████▏| 720/782 [06:44<00:34,  1.78it/s]#015 92%|█████████▏| 721/782 [06:44<00:34,  1.78it/s]#015 92%|█████████▏| 722/782 [06:45<00:33,  1.79it/s]#015 92%|█████████▏| 723/782 [06:45<00:32,  1.79it/s]#015 93%|█████████▎| 724/782 [06:46<00:32,  1.79it/s]#015 93%|█████████▎| 725/782 [06:47<00:31,  1.79it/s]#015 93%|█████████▎| 726/782 [06:47<00:31,  1.80it/s]#015 93%|█████████▎| 727/782 [06:48<00:30,  1.79it/s]#015 93%|█████████▎| 728/782 [06:48<00:30,  1.80it/s]#015 93%|█████████▎| 729/782 [06:49<00:29,  1.80it/s]#015 93%|█████████▎| 730/782 [06:49<00:28,  1.80it/s]#015 93%|█████████▎| 731/782 [06:50<00:28,  1.79it/s]#015 94%|█████████▎| 732/782 [06:51<00:27,  1.79it/s]#015 94%|█████████▎| 733/782 [06:51<00:27,  1.79it/s]#015 94%|█████████▍| 734/782 [06:52<00:26,  1.80it/s]#015 94%|█████████▍| 735/782 [06:52<00:26,  1.79it/s]#015 94%|█████████▍| 736/782 [06:53<00:25,  1.80it/s]#015 94%|█████████▍| 737/782 [06:53<00:25,  1.79it/s]#015 94%|█████████▍| 738/782 [06:54<00:24,  1.80it/s]#015 95%|█████████▍| 739/782 [06:54<00:23,  1.80it/s]#015 95%|█████████▍| 740/782 [06:55<00:23,  1.79it/s]#015 95%|█████████▍| 741/782 [06:56<00:22,  1.79it/s]#015 95%|█████████▍| 742/782 [06:56<00:22,  1.80it/s]#015 95%|█████████▌| 743/782 [06:57<00:21,  1.80it/s]#015 95%|█████████▌| 744/782 [06:57<00:21,  1.81it/s]#015 95%|█████████▌| 745/782 [06:58<00:20,  1.80it/s]#015 95%|█████████▌| 746/782 [06:58<00:20,  1.77it/s]#015 96%|█████████▌| 747/782 [06:59<00:19,  1.79it/s]#015 96%|█████████▌| 748/782 [06:59<00:19,  1.79it/s]#015 96%|█████████▌| 749/782 [07:00<00:18,  1.78it/s]#015 96%|█████████▌| 750/782 [07:01<00:17,  1.79it/s]#015 96%|█████████▌| 751/782 [07:01<00:17,  1.79it/s]#015 96%|█████████▌| 752/782 [07:02<00:16,  1.80it/s]#015 96%|█████████▋| 753/782 [07:02<00:16,  1.80it/s]#015 96%|█████████▋| 754/782 [07:03<00:15,  1.80it/s]#015 97%|█████████▋| 755/782 [07:03<00:15,  1.78it/s]#015 97%|█████████▋| 756/782 [07:04<00:14,  1.77it/s]#015 97%|█████████▋| 757/782 [07:04<00:13,  1.79it/s]#015 97%|█████████▋| 758/782 [07:05<00:13,  1.79it/s]#015 97%|█████████▋| 759/782 [07:06<00:12,  1.79it/s]#015 97%|█████████▋| 760/782 [07:06<00:12,  1.78it/s]#015 97%|█████████▋| 761/782 [07:07<00:12,  1.72it/s]#015 97%|█████████▋| 762/782 [07:07<00:11,  1.75it/s]#015 98%|█████████▊| 763/782 [07:08<00:10,  1.77it/s]#015 98%|█████████▊| 764/782 [07:08<00:10,  1.76it/s]#015 98%|█████████▊| 765/782 [07:09<00:09,  1.76it/s]#015 98%|█████████▊| 766/782 [07:10<00:09,  1.77it/s]#015 98%|█████████▊| 767/782 [07:10<00:08,  1.77it/s]#015 98%|█████████▊| 768/782 [07:11<00:07,  1.78it/s]#015 98%|█████████▊| 769/782 [07:11<00:07,  1.79it/s]#015 98%|█████████▊| 770/782 [07:12<00:06,  1.79it/s]#015 99%|█████████▊| 771/782 [07:12<00:06,  1.80it/s]#015 99%|█████████▊| 772/782 [07:13<00:05,  1.81it/s]#015 99%|█████████▉| 773/782 [07:13<00:04,  1.81it/s]#015 99%|█████████▉| 774/782 [07:14<00:04,  1.80it/s]#015 99%|█████████▉| 775/782 [07:15<00:03,  1.81it/s]#015 99%|█████████▉| 776/782 [07:15<00:03,  1.81it/s]#015 99%|█████████▉| 777/782 [07:16<00:02,  1.81it/s]#015 99%|█████████▉| 778/782 [07:16<00:02,  1.81it/s]#015100%|█████████▉| 779/782 [07:17<00:01,  1.81it/s]#015100%|█████████▉| 780/782 [07:17<00:01,  1.81it/s]#015100%|█████████▉| 781/782 [07:18<00:00,  1.82it/s]#015100%|██████████| 782/782 [07:18<00:00,  2.23it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/157 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  1%|▏         | 2/157 [00:00<00:25,  6.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  2%|▏         | 3/157 [00:00<00:32,  4.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  3%|▎         | 4/157 [00:00<00:37,  4.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  3%|▎         | 5/157 [00:01<00:40,  3.74it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  4%|▍         | 6/157 [00:01<00:42,  3.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  4%|▍         | 7/157 [00:01<00:44,  3.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  5%|▌         | 8/157 [00:02<00:45,  3.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  6%|▌         | 9/157 [00:02<00:45,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  6%|▋         | 10/157 [00:02<00:46,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  7%|▋         | 11/157 [00:03<00:46,  3.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  8%|▊         | 12/157 [00:03<00:46,  3.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  8%|▊         | 13/157 [00:03<00:46,  3.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  9%|▉         | 14/157 [00:04<00:46,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 10%|▉         | 15/157 [00:04<00:45,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 10%|█         | 16/157 [00:04<00:45,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 11%|█         | 17/157 [00:05<00:45,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 11%|█▏        | 18/157 [00:05<00:45,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 12%|█▏        | 19/157 [00:05<00:45,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 13%|█▎        | 20/157 [00:06<00:45,  3.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 13%|█▎        | 21/157 [00:06<00:44,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 14%|█▍        | 22/157 [00:06<00:44,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 15%|█▍        | 23/157 [00:07<00:44,  3.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 15%|█▌        | 24/157 [00:07<00:43,  3.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 16%|█▌        | 25/157 [00:07<00:43,  3.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 17%|█▋        | 26/157 [00:08<00:42,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 17%|█▋        | 27/157 [00:08<00:42,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 18%|█▊        | 28/157 [00:08<00:41,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 18%|█▊        | 29/157 [00:09<00:41,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 19%|█▉        | 30/157 [00:09<00:41,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 20%|█▉        | 31/157 [00:09<00:40,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 20%|██        | 32/157 [00:10<00:40,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 21%|██        | 33/157 [00:10<00:40,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 22%|██▏       | 34/157 [00:10<00:40,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 22%|██▏       | 35/157 [00:11<00:39,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 23%|██▎       | 36/157 [00:11<00:39,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 24%|██▎       | 37/157 [00:11<00:38,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 24%|██▍       | 38/157 [00:12<00:38,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 25%|██▍       | 39/157 [00:12<00:38,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 25%|██▌       | 40/157 [00:12<00:37,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 26%|██▌       | 41/157 [00:13<00:37,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 27%|██▋       | 42/157 [00:13<00:37,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 27%|██▋       | 43/157 [00:13<00:37,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 28%|██▊       | 44/157 [00:13<00:36,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 29%|██▊       | 45/157 [00:14<00:36,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 29%|██▉       | 46/157 [00:14<00:36,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 30%|██▉       | 47/157 [00:14<00:35,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 31%|███       | 48/157 [00:15<00:35,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 31%|███       | 49/157 [00:15<00:35,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 32%|███▏      | 50/157 [00:15<00:34,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 32%|███▏      | 51/157 [00:16<00:34,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 33%|███▎      | 52/157 [00:16<00:34,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 34%|███▍      | 53/157 [00:16<00:33,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 34%|███▍      | 54/157 [00:17<00:33,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 35%|███▌      | 55/157 [00:17<00:33,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 36%|███▌      | 56/157 [00:17<00:33,  3.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 36%|███▋      | 57/157 [00:18<00:32,  3.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 37%|███▋      | 58/157 [00:18<00:32,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 38%|███▊      | 59/157 [00:18<00:31,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 38%|███▊      | 60/157 [00:19<00:31,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 39%|███▉      | 61/157 [00:19<00:31,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 39%|███▉      | 62/157 [00:19<00:30,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 40%|████      | 63/157 [00:20<00:30,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 41%|████      | 64/157 [00:20<00:30,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 41%|████▏     | 65/157 [00:20<00:30,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 42%|████▏     | 66/157 [00:21<00:29,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 43%|████▎     | 67/157 [00:21<00:29,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 43%|████▎     | 68/157 [00:21<00:29,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 44%|████▍     | 69/157 [00:22<00:28,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 45%|████▍     | 70/157 [00:22<00:28,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 45%|████▌     | 71/157 [00:22<00:28,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 46%|████▌     | 72/157 [00:23<00:27,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 46%|████▋     | 73/157 [00:23<00:27,  3.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 47%|████▋     | 74/157 [00:23<00:27,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 48%|████▊     | 75/157 [00:24<00:26,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 48%|████▊     | 76/157 [00:24<00:26,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 49%|████▉     | 77/157 [00:24<00:25,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 50%|████▉     | 78/157 [00:25<00:25,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 50%|█████     | 79/157 [00:25<00:25,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 51%|█████     | 80/157 [00:25<00:24,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 52%|█████▏    | 81/157 [00:26<00:24,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 52%|█████▏    | 82/157 [00:26<00:24,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 53%|█████▎    | 83/157 [00:26<00:23,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 54%|█████▎    | 84/157 [00:27<00:23,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 54%|█████▍    | 85/157 [00:27<00:23,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 55%|█████▍    | 86/157 [00:27<00:23,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 55%|█████▌    | 87/157 [00:27<00:22,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 56%|█████▌    | 88/157 [00:28<00:22,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 57%|█████▋    | 89/157 [00:28<00:22,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 57%|█████▋    | 90/157 [00:28<00:21,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 58%|█████▊    | 91/157 [00:29<00:21,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 59%|█████▊    | 92/157 [00:29<00:21,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 59%|█████▉    | 93/157 [00:29<00:20,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 60%|█████▉    | 94/157 [00:30<00:20,  3.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 61%|██████    | 95/157 [00:30<00:20,  3.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 61%|██████    | 96/157 [00:30<00:19,  3.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 62%|██████▏   | 97/157 [00:31<00:19,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 62%|██████▏   | 98/157 [00:31<00:19,  3.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 63%|██████▎   | 99/157 [00:31<00:18,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 64%|██████▎   | 100/157 [00:32<00:18,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 64%|██████▍   | 101/157 [00:32<00:18,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 65%|██████▍   | 102/157 [00:32<00:17,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 66%|██████▌   | 103/157 [00:33<00:17,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 66%|██████▌   | 104/157 [00:33<00:17,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 67%|██████▋   | 105/157 [00:33<00:16,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 68%|██████▊   | 106/157 [00:34<00:16,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 68%|██████▊   | 107/157 [00:34<00:16,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 69%|██████▉   | 108/157 [00:34<00:15,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 69%|██████▉   | 109/157 [00:35<00:15,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 70%|███████   | 110/157 [00:35<00:15,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 71%|███████   | 111/157 [00:35<00:14,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 71%|███████▏  | 112/157 [00:36<00:14,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 72%|███████▏  | 113/157 [00:36<00:14,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 73%|███████▎  | 114/157 [00:36<00:13,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 73%|███████▎  | 115/157 [00:37<00:13,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 74%|███████▍  | 116/157 [00:37<00:13,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 75%|███████▍  | 117/157 [00:37<00:12,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 75%|███████▌  | 118/157 [00:38<00:12,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 76%|███████▌  | 119/157 [00:38<00:12,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 76%|███████▋  | 120/157 [00:38<00:11,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 77%|███████▋  | 121/157 [00:39<00:11,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 78%|███████▊  | 122/157 [00:39<00:11,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 78%|███████▊  | 123/157 [00:39<00:10,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 79%|███████▉  | 124/157 [00:39<00:10,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 80%|███████▉  | 125/157 [00:40<00:10,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 80%|████████  | 126/157 [00:40<00:10,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 81%|████████  | 127/157 [00:40<00:09,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 82%|████████▏ | 128/157 [00:41<00:09,  3.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 82%|████████▏ | 129/157 [00:41<00:09,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 83%|████████▎ | 130/157 [00:41<00:08,  3.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 83%|████████▎ | 131/157 [00:42<00:08,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 84%|████████▍ | 132/157 [00:42<00:08,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 85%|████████▍ | 133/157 [00:42<00:07,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 85%|████████▌ | 134/157 [00:43<00:07,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 86%|████████▌ | 135/157 [00:43<00:07,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 87%|████████▋ | 136/157 [00:43<00:06,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 87%|████████▋ | 137/157 [00:44<00:06,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 88%|████████▊ | 138/157 [00:44<00:06,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 89%|████████▊ | 139/157 [00:44<00:05,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 89%|████████▉ | 140/157 [00:45<00:05,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 90%|████████▉ | 141/157 [00:45<00:05,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 90%|█████████ | 142/157 [00:45<00:04,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 91%|█████████ | 143/157 [00:46<00:04,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 92%|█████████▏| 144/157 [00:46<00:04,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 92%|█████████▏| 145/157 [00:46<00:03,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 93%|█████████▎| 146/157 [00:47<00:03,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 94%|█████████▎| 147/157 [00:47<00:03,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 94%|█████████▍| 148/157 [00:47<00:02,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 95%|█████████▍| 149/157 [00:48<00:02,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 96%|█████████▌| 150/157 [00:48<00:02,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 96%|█████████▌| 151/157 [00:48<00:01,  3.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 97%|█████████▋| 152/157 [00:49<00:01,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 97%|█████████▋| 153/157 [00:49<00:01,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 98%|█████████▊| 154/157 [00:49<00:00,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 99%|█████████▊| 155/157 [00:50<00:00,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 99%|█████████▉| 156/157 [00:50<00:00,  3.10it/s]#033[A#015                                                 #015\u001b[0m\n",
      "\u001b[34m#015                                                 #015#033[A#015100%|██████████| 782/782 [08:09<00:00,  2.23it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 157/157 [00:50<00:00,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                                 #033[A#015                                                 #015#015100%|██████████| 782/782 [08:09<00:00,  2.23it/s]#015100%|██████████| 782/782 [08:09<00:00,  1.60it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/157 [00:00<?, ?it/s]#015  1%|▏         | 2/157 [00:00<00:25,  6.19it/s]#015  2%|▏         | 3/157 [00:00<00:32,  4.76it/s]#015  3%|▎         | 4/157 [00:00<00:37,  4.10it/s]#015  3%|▎         | 5/157 [00:01<00:40,  3.74it/s]#015  4%|▍         | 6/157 [00:01<00:42,  3.52it/s]#015  4%|▍         | 7/157 [00:01<00:44,  3.38it/s]#015  5%|▌         | 8/157 [00:02<00:45,  3.29it/s]#015  6%|▌         | 9/157 [00:02<00:45,  3.23it/s]#015  6%|▋         | 10/157 [00:02<00:46,  3.17it/s]#015  7%|▋         | 11/157 [00:03<00:46,  3.15it/s]#015  8%|▊         | 12/157 [00:03<00:46,  3.14it/s]#015  8%|▊         | 13/157 [00:03<00:46,  3.12it/s]#015  9%|▉         | 14/157 [00:04<00:45,  3.11it/s]#015 10%|▉         | 15/157 [00:04<00:45,  3.11it/s]#015 10%|█         | 16/157 [00:04<00:45,  3.10it/s]#015 11%|█         | 17/157 [00:05<00:45,  3.10it/s]#015 11%|█▏        | 18/157 [00:05<00:44,  3.10it/s]#015 12%|█▏        | 19/157 [00:05<00:44,  3.09it/s]#015 13%|█▎        | 20/157 [00:06<00:44,  3.09it/s]#015 13%|█▎        | 21/157 [00:06<00:44,  3.09it/s]#015 14%|█▍        | 22/157 [00:06<00:43,  3.09it/s]#015 15%|█▍        | 23/157 [00:07<00:43,  3.09it/s]#015 15%|█▌        | 24/157 [00:07<00:43,  3.09it/s]#015 16%|█▌        | 25/157 [00:07<00:42,  3.09it/s]#015 17%|█▋        | 26/157 [00:08<00:42,  3.09it/s]#015 17%|█▋        | 27/157 [00:08<00:42,  3.09it/s]#015 18%|█▊        | 28/157 [00:08<00:41,  3.09it/s]#015 18%|█▊        | 29/157 [00:09<00:41,  3.09it/s]#015 19%|█▉        | 30/157 [00:09<00:41,  3.09it/s]#015 20%|█▉        | 31/157 [00:09<00:40,  3.10it/s]#015 20%|██        | 32/157 [00:10<00:40,  3.09it/s]#015 21%|██        | 33/157 [00:10<00:40,  3.09it/s]#015 22%|██▏       | 34/157 [00:10<00:40,  3.07it/s]#015 22%|██▏       | 35/157 [00:11<00:39,  3.07it/s]#015 23%|██▎       | 36/157 [00:11<00:39,  3.08it/s]#015 24%|██▎       | 37/157 [00:11<00:38,  3.08it/s]#015 24%|██▍       | 38/157 [00:11<00:38,  3.09it/s]#015 25%|██▍       | 39/157 [00:12<00:38,  3.09it/s]#015 25%|██▌       | 40/157 [00:12<00:38,  3.05it/s]#015 26%|██▌       | 41/157 [00:12<00:38,  3.05it/s]#015 27%|██▋       | 42/157 [00:13<00:37,  3.06it/s]#015 27%|██▋       | 43/157 [00:13<00:37,  3.07it/s]#015 28%|██▊       | 44/157 [00:13<00:36,  3.08it/s]#015 29%|██▊       | 45/157 [00:14<00:36,  3.09it/s]#015 29%|██▉       | 46/157 [00:14<00:35,  3.09it/s]#015 30%|██▉       | 47/157 [00:14<00:35,  3.07it/s]#015 31%|███       | 48/157 [00:15<00:35,  3.07it/s]#015 31%|███       | 49/157 [00:15<00:35,  3.07it/s]#015 32%|███▏      | 50/157 [00:15<00:34,  3.08it/s]#015 32%|███▏      | 51/157 [00:16<00:34,  3.08it/s]#015 33%|███▎      | 52/157 [00:16<00:34,  3.08it/s]#015 34%|███▍      | 53/157 [00:16<00:33,  3.07it/s]#015 34%|███▍      | 54/157 [00:17<00:33,  3.07it/s]#015 35%|███▌      | 55/157 [00:17<00:33,  3.05it/s]#015 36%|███▌      | 56/157 [00:17<00:32,  3.07it/s]#015 36%|███▋      | 57/157 [00:18<00:32,  3.08it/s]#015 37%|███▋      | 58/157 [00:18<00:32,  3.08it/s]#015 38%|███▊      | 59/157 [00:18<00:31,  3.09it/s]#015 38%|███▊      | 60/157 [00:19<00:31,  3.07it/s]#015 39%|███▉      | 61/157 [00:19<00:31,  3.08it/s]#015 39%|███▉      | 62/157 [00:19<00:30,  3.08it/s]#015 40%|████      | 63/157 [00:20<00:30,  3.08it/s]#015 41%|████      | 64/157 [00:20<00:30,  3.09it/s]#015 41%|████▏     | 65/157 [00:20<00:29,  3.09it/s]#015 42%|████▏     | 66/157 [00:21<00:29,  3.09it/s]#015 43%|████▎     | 67/157 [00:21<00:29,  3.09it/s]#015 43%|████▎     | 68/157 [00:21<00:28,  3.09it/s]#015 44%|████▍     | 69/157 [00:22<00:28,  3.09it/s]#015 45%|████▍     | 70/157 [00:22<00:28,  3.09it/s]#015 45%|████▌     | 71/157 [00:22<00:27,  3.09it/s]#015 46%|████▌     | 72/157 [00:23<00:27,  3.09it/s]#015 46%|████▋     | 73/157 [00:23<00:27,  3.08it/s]#015 47%|████▋     | 74/157 [00:23<00:26,  3.09it/s]#015 48%|████▊     | 75/157 [00:23<00:26,  3.09it/s]#015 48%|████▊     | 76/157 [00:24<00:26,  3.09it/s]#015 49%|████▉     | 77/157 [00:24<00:25,  3.09it/s]#015 50%|████▉     | 78/157 [00:24<00:25,  3.09it/s]#015 50%|█████     | 79/157 [00:25<00:25,  3.09it/s]#015 51%|█████     | 80/157 [00:25<00:25,  3.08it/s]#015 52%|█████▏    | 81/157 [00:25<00:24,  3.06it/s]#015 52%|█████▏    | 82/157 [00:26<00:24,  3.07it/s]#015 53%|█████▎    | 83/157 [00:26<00:24,  3.07it/s]#015 54%|█████▎    | 84/157 [00:26<00:23,  3.08it/s]#015 54%|█████▍    | 85/157 [00:27<00:23,  3.07it/s]#015 55%|█████▍    | 86/157 [00:27<00:23,  3.08it/s]#015 55%|█████▌    | 87/157 [00:27<00:22,  3.08it/s]#015 56%|█████▌    | 88/157 [00:28<00:22,  3.08it/s]#015 57%|█████▋    | 89/157 [00:28<00:22,  3.08it/s]#015 57%|█████▋    | 90/157 [00:28<00:21,  3.09it/s]#015 58%|█████▊    | 91/157 [00:29<00:21,  3.09it/s]#015 59%|█████▊    | 92/157 [00:29<00:21,  3.09it/s]#015 59%|█████▉    | 93/157 [00:29<00:20,  3.09it/s]#015 60%|█████▉    | 94/157 [00:30<00:20,  3.09it/s]#015 61%|██████    | 95/157 [00:30<00:20,  3.09it/s]#015 61%|██████    | 96/157 [00:30<00:19,  3.09it/s]#015 62%|██████▏   | 97/157 [00:31<00:19,  3.08it/s]#015 62%|██████▏   | 98/157 [00:31<00:19,  3.08it/s]#015 63%|██████▎   | 99/157 [00:31<00:18,  3.08it/s]#015 64%|██████▎   | 100/157 [00:32<00:18,  3.08it/s]#015 64%|██████▍   | 101/157 [00:32<00:18,  3.08it/s]#015 65%|██████▍   | 102/157 [00:32<00:17,  3.09it/s]#015 66%|██████▌   | 103/157 [00:33<00:17,  3.08it/s]#015 66%|██████▌   | 104/157 [00:33<00:17,  3.07it/s]#015 67%|██████▋   | 105/157 [00:33<00:16,  3.07it/s]#015 68%|██████▊   | 106/157 [00:34<00:16,  3.06it/s]#015 68%|██████▊   | 107/157 [00:34<00:16,  3.06it/s]#015 69%|██████▉   | 108/157 [00:34<00:16,  3.06it/s]#015 69%|██████▉   | 109/157 [00:35<00:15,  3.05it/s]#015 70%|███████   | 110/157 [00:35<00:15,  3.04it/s]#015 71%|███████   | 111/157 [00:35<00:15,  3.05it/s]#015 71%|███████▏  | 112/157 [00:36<00:14,  3.05it/s]#015 72%|███████▏  | 113/157 [00:36<00:14,  3.05it/s]#015 73%|███████▎  | 114/157 [00:36<00:14,  3.06it/s]#015 73%|███████▎  | 115/157 [00:37<00:13,  3.07it/s]#015 74%|███████▍  | 116/157 [00:37<00:13,  3.07it/s]#015 75%|███████▍  | 117/157 [00:37<00:13,  3.07it/s]#015 75%|███████▌  | 118/157 [00:37<00:12,  3.08it/s]#015 76%|███████▌  | 119/157 [00:38<00:12,  3.08it/s]#015 76%|███████▋  | 120/157 [00:38<00:11,  3.09it/s]#015 77%|███████▋  | 121/157 [00:38<00:11,  3.09it/s]#015 78%|███████▊  | 122/157 [00:39<00:11,  3.09it/s]#015 78%|███████▊  | 123/157 [00:39<00:11,  3.09it/s]#015 79%|███████▉  | 124/157 [00:39<00:10,  3.09it/s]#015 80%|███████▉  | 125/157 [00:40<00:10,  3.09it/s]#015 80%|████████  | 126/157 [00:40<00:10,  3.09it/s]#015 81%|████████  | 127/157 [00:40<00:09,  3.09it/s]#015 82%|████████▏ | 128/157 [00:41<00:09,  3.09it/s]#015 82%|████████▏ | 129/157 [00:41<00:09,  3.09it/s]#015 83%|████████▎ | 130/157 [00:41<00:08,  3.09it/s]#015 83%|████████▎ | 131/157 [00:42<00:08,  3.09it/s]#015 84%|████████▍ | 132/157 [00:42<00:08,  3.09it/s]#015 85%|████████▍ | 133/157 [00:42<00:07,  3.09it/s]#015 85%|████████▌ | 134/157 [00:43<00:07,  3.09it/s]#015 86%|████████▌ | 135/157 [00:43<00:07,  3.08it/s]#015 87%|████████▋ | 136/157 [00:43<00:06,  3.08it/s]#015 87%|████████▋ | 137/157 [00:44<00:06,  3.08it/s]#015 88%|████████▊ | 138/157 [00:44<00:06,  3.08it/s]#015 89%|████████▊ | 139/157 [00:44<00:05,  3.09it/s]#015 89%|████████▉ | 140/157 [00:45<00:05,  3.09it/s]#015 90%|████████▉ | 141/157 [00:45<00:05,  3.09it/s]#015 90%|█████████ | 142/157 [00:45<00:04,  3.09it/s]#015 91%|█████████ | 143/157 [00:46<00:04,  3.09it/s]#015 92%|█████████▏| 144/157 [00:46<00:04,  3.09it/s]#015 92%|█████████▏| 145/157 [00:46<00:03,  3.09it/s]#015 93%|█████████▎| 146/157 [00:47<00:03,  3.09it/s]#015 94%|█████████▎| 147/157 [00:47<00:03,  3.09it/s]#015 94%|█████████▍| 148/157 [00:47<00:02,  3.09it/s]#015 95%|█████████▍| 149/157 [00:48<00:02,  3.09it/s]#015 96%|█████████▌| 150/157 [00:48<00:02,  3.09it/s]#015 96%|█████████▌| 151/157 [00:48<00:01,  3.09it/s]#015 97%|█████████▋| 152/157 [00:48<00:01,  3.09it/s]#015 97%|█████████▋| 153/157 [00:49<00:01,  3.09it/s]#015 98%|█████████▊| 154/157 [00:49<00:00,  3.09it/s]#015 99%|█████████▊| 155/157 [00:49<00:00,  3.08it/s]#015 99%|█████████▉| 156/157 [00:50<00:00,  3.07it/s]#015100%|██████████| 157/157 [00:50<00:00,  3.12it/s]\n",
      "\u001b[0m\n",
      "\n",
      "2021-08-09 19:12:25 Uploading - Uploading generated training model\n",
      "2021-08-09 19:14:54 Completed - Training job completed\n",
      "ProfilerReport-1628535404: IssuesFound\n",
      "Training seconds: 926\n",
      "Billable seconds: 926\n"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLUE - Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmorgan\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -qqq wandb --upgrade\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.sagemaker_auth(path=\"scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'output_dir': 'tmp',\n",
    "                #'epochs': 1,\n",
    "                #'train_batch_size': 32,\n",
    "                 'do_train': True,\n",
    "                'do_eval': True,\n",
    "                'fp16': True,\n",
    "                'per_device_train_batch_size': 32,\n",
    "                'per_device_eval_batch_size': 32,\n",
    "                'num_train_epochs': 2,\n",
    "                 'model_name_or_path':'distilbert-base-uncased', \n",
    "                 'task_name' :'MRPC',\n",
    "                 'learning_rate': 1e-4,\n",
    "                 'max_steps': 300,\n",
    "                 'logging_steps': 10,\n",
    "                 'evaluation_strategy' : 'steps',\n",
    "                 'report_to': 'wandb',\n",
    "                 'run_name': 'morg_demo'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='run_glue.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.6',\n",
    "                            pytorch_version='1.7',\n",
    "                            py_version='py36',\n",
    "                            hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-09 22:33:46 Starting - Starting the training job...\n",
      "2021-08-09 22:34:09 Starting - Launching requested ML instancesProfilerReport-1628548426: InProgress\n",
      "......\n",
      "2021-08-09 22:35:10 Starting - Preparing the instances for training......\n",
      "2021-08-09 22:36:15 Downloading - Downloading input data\n",
      "2021-08-09 22:36:15 Training - Downloading the training image..................\n",
      "2021-08-09 22:39:17 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-08-09 22:39:18,103 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-08-09 22:39:18,126 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-08-09 22:39:18,133 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-08-09 22:39:18,431 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting datasets>=1.11.0\n",
      "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\u001b[0m\n",
      "\u001b[34mCollecting wandb\n",
      "  Downloading wandb-0.11.2-py2.py3-none-any.whl (1.8 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.11.0->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets>=1.11.0->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.11.0->-r requirements.txt (line 1)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.11.0->-r requirements.txt (line 1)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.11.0->-r requirements.txt (line 1)) (0.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.11.0->-r requirements.txt (line 1)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.11.0->-r requirements.txt (line 1)) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.42 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.11.0->-r requirements.txt (line 1)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets>=1.11.0->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.11.0->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow!=4.0.0,>=1.0.0\n",
      "  Downloading pyarrow-5.0.0-cp36-cp36m-manylinux2014_x86_64.whl (23.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.11.0->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.11.0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets>=1.11.0->-r requirements.txt (line 1)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.11.0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.11.0->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.11.0->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.11.0->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mCollecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting subprocess32>=3.5.3\n",
      "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 2)) (3.17.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 2)) (5.8.0)\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\u001b[0m\n",
      "\u001b[34mCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mCollecting configparser>=3.8.1\n",
      "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 2)) (7.1.2)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.0 in /opt/conda/lib/python3.6/site-packages (from GitPython>=1.0.0->wandb->-r requirements.txt (line 2)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34mCollecting smmap<5,>=3.0.1\n",
      "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets>=1.11.0->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets>=1.11.0->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.11.0->-r requirements.txt (line 1)) (2021.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: promise, subprocess32, pathtools\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=f7df2e71ad875e517ead7b9e0652e4211be6d1e8164ae43d0cf3aa0e947f3dbf\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "  Building wheel for subprocess32 (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for subprocess32 (setup.py): finished with status 'done'\n",
      "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6487 sha256=705d8c37e2068cb1ad867b03b1b9be6f83c16e83e15f05be160fb8febfe58ab4\n",
      "  Stored in directory: /root/.cache/pip/wheels/44/3a/ab/102386d84fe551b6cedb628ed1e74c5f5be76af8b909aeda09\n",
      "  Building wheel for pathtools (setup.py): started\n",
      "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8785 sha256=6fb18fa97021ad4a4dd7de245bd97cb76561f25ffd9a0144357f7375d982f830\n",
      "  Stored in directory: /root/.cache/pip/wheels/42/ea/90/e37d463fb3b03848bf715080595de62545266f53dd546b2497\u001b[0m\n",
      "\u001b[34mSuccessfully built promise subprocess32 pathtools\u001b[0m\n",
      "\u001b[34mInstalling collected packages: urllib3, smmap, gitdb, subprocess32, shortuuid, sentry-sdk, pyarrow, promise, pathtools, GitPython, docker-pycreds, configparser, wandb, datasets\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled urllib3-1.25.11\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 4.0.0\n",
      "    Uninstalling pyarrow-4.0.0:\n",
      "      Successfully uninstalled pyarrow-4.0.0\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 1.6.2\n",
      "    Uninstalling datasets-1.6.2:\n",
      "      Successfully uninstalled datasets-1.6.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed GitPython-3.1.18 configparser-5.0.2 datasets-1.11.0 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 promise-2.3 pyarrow-5.0.0 sentry-sdk-1.3.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 urllib3-1.26.6 wandb-0.11.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "\u001b[34m2021-08-09 22:39:29,206 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"task_name\": \"MRPC\",\n",
      "        \"evaluation_strategy\": \"steps\",\n",
      "        \"per_device_eval_batch_size\": 32,\n",
      "        \"max_steps\": 300,\n",
      "        \"do_train\": true,\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"do_eval\": true,\n",
      "        \"report_to\": \"wandb\",\n",
      "        \"output_dir\": \"tmp\",\n",
      "        \"per_device_train_batch_size\": 32,\n",
      "        \"learning_rate\": 0.0001,\n",
      "        \"run_name\": \"morg_demo\",\n",
      "        \"logging_steps\": 10,\n",
      "        \"model_name_or_path\": \"distilbert-base-uncased\",\n",
      "        \"fp16\": true\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2021-08-09-22-33-46-228\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-618469898284/huggingface-pytorch-training-2021-08-09-22-33-46-228/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_glue\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_glue.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_eval\":true,\"do_train\":true,\"evaluation_strategy\":\"steps\",\"fp16\":true,\"learning_rate\":0.0001,\"logging_steps\":10,\"max_steps\":300,\"model_name_or_path\":\"distilbert-base-uncased\",\"num_train_epochs\":2,\"output_dir\":\"tmp\",\"per_device_eval_batch_size\":32,\"per_device_train_batch_size\":32,\"report_to\":\"wandb\",\"run_name\":\"morg_demo\",\"task_name\":\"MRPC\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_glue.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_glue\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-618469898284/huggingface-pytorch-training-2021-08-09-22-33-46-228/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":true,\"do_train\":true,\"evaluation_strategy\":\"steps\",\"fp16\":true,\"learning_rate\":0.0001,\"logging_steps\":10,\"max_steps\":300,\"model_name_or_path\":\"distilbert-base-uncased\",\"num_train_epochs\":2,\"output_dir\":\"tmp\",\"per_device_eval_batch_size\":32,\"per_device_train_batch_size\":32,\"report_to\":\"wandb\",\"run_name\":\"morg_demo\",\"task_name\":\"MRPC\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2021-08-09-22-33-46-228\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-618469898284/huggingface-pytorch-training-2021-08-09-22-33-46-228/source/sourcedir.tar.gz\",\"module_name\":\"run_glue\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_glue.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--evaluation_strategy\",\"steps\",\"--fp16\",\"True\",\"--learning_rate\",\"0.0001\",\"--logging_steps\",\"10\",\"--max_steps\",\"300\",\"--model_name_or_path\",\"distilbert-base-uncased\",\"--num_train_epochs\",\"2\",\"--output_dir\",\"tmp\",\"--per_device_eval_batch_size\",\"32\",\"--per_device_train_batch_size\",\"32\",\"--report_to\",\"wandb\",\"--run_name\",\"morg_demo\",\"--task_name\",\"MRPC\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_TASK_NAME=MRPC\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=300\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_REPORT_TO=wandb\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=tmp\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_RUN_NAME=morg_demo\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 run_glue.py --do_eval True --do_train True --evaluation_strategy steps --fp16 True --learning_rate 0.0001 --logging_steps 10 --max_steps 300 --model_name_or_path distilbert-base-uncased --num_train_epochs 2 --output_dir tmp --per_device_eval_batch_size 32 --per_device_train_batch_size 32 --report_to wandb --run_name morg_demo --task_name MRPC\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-u4q7ypta\u001b[0m\n",
      "\u001b[34m  Installing build dependencies: started\u001b[0m\n",
      "\u001b[34m  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34m  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0) (2021.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0) (0.0.45)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0) (4.0.1)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.0.12\n",
      "  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0) (0.10.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.0.12->transformers==4.10.0.dev0) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.10.0.dev0) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.10.0.dev0) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0.dev0) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0.dev0) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0.dev0) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0.dev0) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0.dev0) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0.dev0) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0.dev0) (1.16.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517): started\u001b[0m\n",
      "\u001b[34m  Building wheel for transformers (PEP 517): finished with status 'done'\n",
      "  Created wheel for transformers: filename=transformers-4.10.0.dev0-py3-none-any.whl size=2633708 sha256=004a403e703eb109b30e09458d2857bd2f79e004c1bb076682e08f46fa5f9ef3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-u1q_tzsf/wheels/83/0d/25/e0bd4216d0278109b70f8a6ada5ea35eebbad3de134e416ba3\u001b[0m\n",
      "\u001b[34mSuccessfully built transformers\u001b[0m\n",
      "\u001b[34mInstalling collected packages: huggingface-hub, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.8\n",
      "    Uninstalling huggingface-hub-0.0.8:\n",
      "      Successfully uninstalled huggingface-hub-0.0.8\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.6.1\n",
      "    Uninstalling transformers-4.6.1:\n",
      "      Successfully uninstalled transformers-4.6.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed huggingface-hub-0.0.15 transformers-4.10.0.dev0\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:48 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_steps=10,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34mfp16=True,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0001,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=-1,\u001b[0m\n",
      "\u001b[34mlog_level_replica=-1,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=tmp/runs/Aug09_22-41-48_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_steps=10,\u001b[0m\n",
      "\u001b[34mlogging_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=SchedulerType.LINEAR,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=300,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=2.0,\u001b[0m\n",
      "\u001b[34moutput_dir=tmp,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=32,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=32,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=tmp,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=None,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=['wandb'],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=morg_demo,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:48 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/glue/glue.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpe1573_h5\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:48 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/glue/glue.py in cache at /root/.cache/huggingface/datasets/downloads/6abc06ca6fa5847ff1b45d1ebbf9b0f34206d5e0461dbcb313e387be81362f82.759f3e257a3fad0984d9f8ba9a26479d341795eb50fa64e4c1de40f1fc421313.py\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:48 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6abc06ca6fa5847ff1b45d1ebbf9b0f34206d5e0461dbcb313e387be81362f82.759f3e257a3fad0984d9f8ba9a26479d341795eb50fa64e4c1de40f1fc421313.py\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:48 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/glue/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpl398s88q\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:48 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/glue/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/aaa9b0c899269eda04e157d2725bd19d5a84deb10eaa4ace87afe42fc57fa428.082d8848abcb8cddda90647ec069014ca338abd4f45e0a83c6df1ece0d45476a\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:48 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/aaa9b0c899269eda04e157d2725bd19d5a84deb10eaa4ace87afe42fc57fa428.082d8848abcb8cddda90647ec069014ca338abd4f45e0a83c6df1ece0d45476a\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:48 - INFO - datasets.load - Creating main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:48 - INFO - datasets.load - Creating specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:48 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:48 - INFO - datasets.load - Copying dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:48 - INFO - datasets.load - Creating metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:48 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:48 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:48 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:49 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/mrpc_dev_ids.tsv not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpmvk2tlyy\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:49 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/mrpc_dev_ids.tsv in cache at /root/.cache/huggingface/datasets/downloads/393f97e0117f7c5fc1053fba02fb070b221bc0da0eafbd6b57e8cdf7621a4e64\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:49 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/393f97e0117f7c5fc1053fba02fb070b221bc0da0eafbd6b57e8cdf7621a4e64\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:50 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpedlkg3yc\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:51 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt in cache at /root/.cache/huggingface/datasets/downloads/7c6c4f66e416181b62e136ddd5834ec10afe3aac4f7a327b81ca74025ea69529\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:51 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/7c6c4f66e416181b62e136ddd5834ec10afe3aac4f7a327b81ca74025ea69529\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:51 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpnh1tvapy\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:52 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt in cache at /root/.cache/huggingface/datasets/downloads/d0f75e90c732a9847ec38471fddece4ebcaad09dd1958467e2b00c6a3cbd31a9\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:52 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/d0f75e90c732a9847ec38471fddece4ebcaad09dd1958467e2b00c6a3cbd31a9\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:52 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:52 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:52 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:52 - INFO - datasets.builder - Generating split train\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:53 - INFO - datasets.builder - Generating split validation\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:53 - INFO - datasets.builder - Generating split test\u001b[0m\n",
      "\u001b[34m08/09/2021 22:41:53 - INFO - datasets.utils.info_utils - All the splits matched successfully.\u001b[0m\n",
      "\u001b[34mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-09 22:41:53,293 >> https://huggingface.co/distilbert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn18a985x\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-09 22:41:53,355 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-09 22:41:53,355 >> creating metadata file for /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:545] 2021-08-09 22:41:53,355 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:581] 2021-08-09 22:41:53,356 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.10.0.dev0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-09 22:41:53,420 >> https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpae7bm5ky\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-09 22:41:53,504 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-09 22:41:53,504 >> creating metadata file for /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:545] 2021-08-09 22:41:53,570 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:581] 2021-08-09 22:41:53,570 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.10.0.dev0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-09 22:41:53,698 >> https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1j1cz7vs\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-09 22:41:53,790 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-09 22:41:53,790 >> creating metadata file for /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-09 22:41:53,855 >> https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpaens3f_r\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-09 22:41:53,962 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-09 22:41:53,962 >> creating metadata file for /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-09 22:41:54,153 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-09 22:41:54,154 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-09 22:41:54,154 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-09 22:41:54,154 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-09 22:41:54,154 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:545] 2021-08-09 22:41:54,221 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:581] 2021-08-09 22:41:54,222 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.10.0.dev0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-09 22:41:54,348 >> https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptgwb7zrf\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-09 22:41:59,373 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-09 22:41:59,374 >> creating metadata file for /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1275] 2021-08-09 22:41:59,374 >> loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1512] 2021-08-09 22:42:00,317 >> Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1523] 2021-08-09 22:42:00,317 >> Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-313f9d86098852ea.arrow\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-4bad7de35ac2c7b1.arrow\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-05333cf856efebea.arrow\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:00 - INFO - __main__ - Sample 2619 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 2916, 'input_ids': [101, 1996, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 3943, 5530, 1997, 5491, 2041, 16992, 9989, 2114, 2032, 1012, 102, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 1037, 3943, 1011, 3931, 19238, 3661, 2000, 1996, 2457, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .'}.\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:00 - INFO - __main__ - Sample 456 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 509, 'input_ids': [101, 18178, 8661, 4584, 2551, 2005, 1996, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 2279, 4465, 1005, 1055, 4883, 2602, 1999, 2162, 1011, 7950, 18178, 2818, 17238, 1012, 102, 4584, 1999, 18178, 2818, 17238, 1005, 1055, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1010, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 4465, 1005, 1055, 4883, 2602, 1999, 1996, 2162, 1011, 25537, 2555, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\"}.\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:00 - INFO - __main__ - Sample 102 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 116, 'input_ids': [101, 3115, 1004, 3532, 1005, 1055, 3156, 4518, 5950, 17795, 6430, 1018, 1012, 2871, 2685, 2000, 5818, 2509, 1012, 2753, 1010, 2096, 17235, 2850, 4160, 17795, 3062, 1020, 1012, 1019, 2685, 2000, 1015, 1010, 18744, 1012, 2753, 1012, 102, 1996, 3115, 1004, 3532, 1005, 1055, 3156, 5950, 2001, 2039, 1015, 1012, 4293, 2685, 1010, 2030, 1014, 1012, 2324, 3867, 1010, 2000, 5989, 2581, 1012, 6273, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\"}.\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:00 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/glue/glue.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpzifili49\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:01 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/glue/glue.py in cache at /root/.cache/huggingface/datasets/downloads/e2fd9a26b406081bfc6236a0c5241049d26f43d1fc603a2225542bcc0111449e.d4934ed99ee0ef4a5d0d757367c52f72d6ae30a68db09205c01563cab11c5a0d.py\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:01 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/e2fd9a26b406081bfc6236a0c5241049d26f43d1fc603a2225542bcc0111449e.d4934ed99ee0ef4a5d0d757367c52f72d6ae30a68db09205c01563cab11c5a0d.py\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:01 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:01 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:01 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:01 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/glue/dataset_infos.json\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:01 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:404] 2021-08-09 22:42:05,240 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:421] 2021-08-09 22:42:05,240 >> Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:05,241 >> The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1171] 2021-08-09 22:42:05,261 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1172] 2021-08-09 22:42:05,261 >>   Num examples = 3668\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1173] 2021-08-09 22:42:05,261 >>   Num Epochs = 3\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1174] 2021-08-09 22:42:05,261 >>   Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1175] 2021-08-09 22:42:05,261 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1176] 2021-08-09 22:42:05,262 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1177] 2021-08-09 22:42:05,262 >>   Total optimization steps = 300\u001b[0m\n",
      "\u001b[34m[INFO|integrations.py:447] 2021-08-09 22:42:05,264 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.101 algo-1:49 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.166 algo-1:49 INFO profiler_config_parser.py:102] User has disabled profiler.#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.167 algo-1:49 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.167 algo-1:49 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.168 algo-1:49 INFO hook.py:255] Saving to /opt/ml/output/tensors#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.168 algo-1:49 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.335 algo-1:49 INFO hook.py:591] name:distilbert.embeddings.word_embeddings.weight count_params:23440896#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.335 algo-1:49 INFO hook.py:591] name:distilbert.embeddings.position_embeddings.weight count_params:393216#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.335 algo-1:49 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.weight count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.335 algo-1:49 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.336 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.336 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.336 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.336 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.336 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.336 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.336 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.336 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.336 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.337 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.337 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.337 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.337 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.337 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.337 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.weight count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.337 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.337 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.337 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.337 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.337 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.338 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.338 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.338 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.338 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.338 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.338 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.338 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.338 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.338 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.338 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.338 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.weight count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.339 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.339 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.339 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.339 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.339 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.339 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.339 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.339 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.339 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.339 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.339 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.340 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.340 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.340 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.340 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.340 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.weight count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.340 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.340 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.340 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.340 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.340 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.340 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.340 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.341 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.341 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.341 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.341 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.341 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.341 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.341 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.341 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.341 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.weight count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.341 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.342 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.342 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.342 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.342 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.342 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.342 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.342 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.342 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.342 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.342 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.342 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.342 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.343 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.343 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.343 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.weight count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.343 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.343 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.343 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.343 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.343 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.343 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.343 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.344 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.344 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.344 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.344 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.344 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.344 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.344 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.344 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.344 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.weight count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.344 algo-1:49 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.344 algo-1:49 INFO hook.py:591] name:pre_classifier.weight count_params:589824#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.344 algo-1:49 INFO hook.py:591] name:pre_classifier.bias count_params:768#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.345 algo-1:49 INFO hook.py:591] name:classifier.weight count_params:1536#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.345 algo-1:49 INFO hook.py:591] name:classifier.bias count_params:2#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.345 algo-1:49 INFO hook.py:593] Total Trainable Params: 66955010#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.345 algo-1:49 INFO hook.py:425] Monitoring the collections: losses#015\u001b[0m\n",
      "\u001b[34m[2021-08-09 22:42:08.347 algo-1:49 INFO hook.py:488] Hook is writing from the hook with pid: 49#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.6087, 'learning_rate': 9.666666666666667e-05, 'epoch': 0.09}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:10,273 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:10,276 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:10,277 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:10,277 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:10 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.5910881757736206, 'eval_accuracy': 0.7034313725490197, 'eval_f1': 0.819672131147541, 'eval_combined_score': 0.7615517518482804, 'eval_runtime': 0.536, 'eval_samples_per_second': 761.165, 'eval_steps_per_second': 24.253, 'epoch': 0.09}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.597, 'learning_rate': 9.333333333333334e-05, 'epoch': 0.17}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:11,699 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:11,702 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:11,702 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:11,703 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:12 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.6060909628868103, 'eval_accuracy': 0.6838235294117647, 'eval_f1': 0.7929373996789728, 'eval_combined_score': 0.7383804645453688, 'eval_runtime': 0.5346, 'eval_samples_per_second': 763.123, 'eval_steps_per_second': 24.315, 'epoch': 0.17}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.5786, 'learning_rate': 9e-05, 'epoch': 0.26}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:13,203 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:13,206 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:13,207 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:13,207 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:13 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.553865373134613, 'eval_accuracy': 0.7107843137254902, 'eval_f1': 0.8249258160237388, 'eval_combined_score': 0.7678550648746145, 'eval_runtime': 0.5315, 'eval_samples_per_second': 767.679, 'eval_steps_per_second': 24.46, 'epoch': 0.26}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.5633, 'learning_rate': 8.666666666666667e-05, 'epoch': 0.35}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:14,628 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:14,631 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:14,631 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:14,632 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:15 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.5776156187057495, 'eval_accuracy': 0.6985294117647058, 'eval_f1': 0.8193832599118943, 'eval_combined_score': 0.7589563358383, 'eval_runtime': 0.5274, 'eval_samples_per_second': 773.649, 'eval_steps_per_second': 24.651, 'epoch': 0.35}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.595, 'learning_rate': 8.333333333333334e-05, 'epoch': 0.43}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:16,051 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:16,055 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:16,055 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:16,055 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:16 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.4865941107273102, 'eval_accuracy': 0.7156862745098039, 'eval_f1': 0.8253012048192772, 'eval_combined_score': 0.7704937396645406, 'eval_runtime': 0.5234, 'eval_samples_per_second': 779.508, 'eval_steps_per_second': 24.837, 'epoch': 0.43}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.5788, 'learning_rate': 8.066666666666667e-05, 'epoch': 0.52}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:17,427 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:17,430 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:17,430 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:17,430 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:17 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.48674845695495605, 'eval_accuracy': 0.7598039215686274, 'eval_f1': 0.8093385214007782, 'eval_combined_score': 0.7845712214847028, 'eval_runtime': 0.5316, 'eval_samples_per_second': 767.454, 'eval_steps_per_second': 24.453, 'epoch': 0.52}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.545, 'learning_rate': 7.733333333333333e-05, 'epoch': 0.61}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:18,871 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:18,874 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:18,875 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:18,875 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:19 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.4672367572784424, 'eval_accuracy': 0.7892156862745098, 'eval_f1': 0.8585526315789473, 'eval_combined_score': 0.8238841589267285, 'eval_runtime': 0.5279, 'eval_samples_per_second': 772.814, 'eval_steps_per_second': 24.624, 'epoch': 0.61}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.5521, 'learning_rate': 7.4e-05, 'epoch': 0.7}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:20,327 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:20,331 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:20,332 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:20,332 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:20 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.441427081823349, 'eval_accuracy': 0.7745098039215687, 'eval_f1': 0.853968253968254, 'eval_combined_score': 0.8142390289449113, 'eval_runtime': 0.527, 'eval_samples_per_second': 774.234, 'eval_steps_per_second': 24.669, 'epoch': 0.7}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.4547, 'learning_rate': 7.066666666666667e-05, 'epoch': 0.78}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:21,743 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:21,746 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:21,746 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:21,746 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:22 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.4163244664669037, 'eval_accuracy': 0.8137254901960784, 'eval_f1': 0.8637992831541219, 'eval_combined_score': 0.8387623866751002, 'eval_runtime': 0.5264, 'eval_samples_per_second': 775.133, 'eval_steps_per_second': 24.698, 'epoch': 0.78}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.4891, 'learning_rate': 6.733333333333333e-05, 'epoch': 0.87}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:23,350 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:23,353 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:23,354 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:23,354 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:23 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3741160035133362, 'eval_accuracy': 0.8455882352941176, 'eval_f1': 0.8904347826086957, 'eval_combined_score': 0.8680115089514067, 'eval_runtime': 0.5275, 'eval_samples_per_second': 773.493, 'eval_steps_per_second': 24.646, 'epoch': 0.87}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.4199, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.96}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:24,772 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:24,775 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:24,775 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:24,775 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:25 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.371240496635437, 'eval_accuracy': 0.8578431372549019, 'eval_f1': 0.901360544217687, 'eval_combined_score': 0.8796018407362944, 'eval_runtime': 0.5277, 'eval_samples_per_second': 773.169, 'eval_steps_per_second': 24.635, 'epoch': 0.96}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.3714, 'learning_rate': 6.066666666666667e-05, 'epoch': 1.04}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:26,194 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:26,197 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:26,198 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:26,198 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:26 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.35753363370895386, 'eval_accuracy': 0.8578431372549019, 'eval_f1': 0.9003436426116839, 'eval_combined_score': 0.8790933899332929, 'eval_runtime': 0.526, 'eval_samples_per_second': 775.594, 'eval_steps_per_second': 24.713, 'epoch': 1.04}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.3389, 'learning_rate': 5.7333333333333336e-05, 'epoch': 1.13}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:27,607 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:27,610 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:27,610 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:27,610 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:28 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3693159520626068, 'eval_accuracy': 0.8406862745098039, 'eval_f1': 0.875717017208413, 'eval_combined_score': 0.8582016458591084, 'eval_runtime': 0.5269, 'eval_samples_per_second': 774.366, 'eval_steps_per_second': 24.673, 'epoch': 1.13}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.2724, 'learning_rate': 5.4000000000000005e-05, 'epoch': 1.22}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:29,020 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:29,023 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:29,024 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:29,024 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:29 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.35654473304748535, 'eval_accuracy': 0.8480392156862745, 'eval_f1': 0.8963210702341137, 'eval_combined_score': 0.8721801429601941, 'eval_runtime': 0.5376, 'eval_samples_per_second': 758.956, 'eval_steps_per_second': 24.182, 'epoch': 1.22}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.3138, 'learning_rate': 5.0666666666666674e-05, 'epoch': 1.3}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:30,484 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:30,487 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:30,487 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:30,488 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:31 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.35413166880607605, 'eval_accuracy': 0.8529411764705882, 'eval_f1': 0.894736842105263, 'eval_combined_score': 0.8738390092879256, 'eval_runtime': 0.5315, 'eval_samples_per_second': 767.589, 'eval_steps_per_second': 24.457, 'epoch': 1.3}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.2857, 'learning_rate': 4.7333333333333336e-05, 'epoch': 1.39}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:31,914 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:31,917 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:31,917 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:31,917 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:32 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3836260139942169, 'eval_accuracy': 0.8480392156862745, 'eval_f1': 0.8916083916083916, 'eval_combined_score': 0.869823803647333, 'eval_runtime': 0.5327, 'eval_samples_per_second': 765.915, 'eval_steps_per_second': 24.404, 'epoch': 1.39}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.2823, 'learning_rate': 4.4000000000000006e-05, 'epoch': 1.48}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:33,342 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:33,345 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:33,345 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:33,345 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:33 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.4614521861076355, 'eval_accuracy': 0.8137254901960784, 'eval_f1': 0.8733333333333333, 'eval_combined_score': 0.8435294117647059, 'eval_runtime': 0.5321, 'eval_samples_per_second': 766.725, 'eval_steps_per_second': 24.43, 'epoch': 1.48}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.3208, 'learning_rate': 4.066666666666667e-05, 'epoch': 1.57}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:34,787 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:34,790 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:34,790 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:34,790 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:35 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3768213391304016, 'eval_accuracy': 0.8259803921568627, 'eval_f1': 0.8725314183123878, 'eval_combined_score': 0.8492559052346252, 'eval_runtime': 0.5266, 'eval_samples_per_second': 774.855, 'eval_steps_per_second': 24.689, 'epoch': 1.57}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.2606, 'learning_rate': 3.733333333333334e-05, 'epoch': 1.65}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:36,197 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:36,200 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:36,200 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:36,200 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:36 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.42845991253852844, 'eval_accuracy': 0.821078431372549, 'eval_f1': 0.87893864013267, 'eval_combined_score': 0.8500085357526095, 'eval_runtime': 0.5261, 'eval_samples_per_second': 775.461, 'eval_steps_per_second': 24.708, 'epoch': 1.65}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.329, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.74}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:37,720 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:37,723 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:37,723 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:37,723 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:38 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.34887808561325073, 'eval_accuracy': 0.8333333333333334, 'eval_f1': 0.8763636363636363, 'eval_combined_score': 0.8548484848484849, 'eval_runtime': 0.5247, 'eval_samples_per_second': 777.523, 'eval_steps_per_second': 24.774, 'epoch': 1.74}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.2913, 'learning_rate': 3.066666666666667e-05, 'epoch': 1.83}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:39,153 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:39,158 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:39,158 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:39,158 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:39 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.35790595412254333, 'eval_accuracy': 0.8553921568627451, 'eval_f1': 0.8998302207130731, 'eval_combined_score': 0.877611188787909, 'eval_runtime': 0.531, 'eval_samples_per_second': 768.379, 'eval_steps_per_second': 24.483, 'epoch': 1.83}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.263, 'learning_rate': 2.733333333333333e-05, 'epoch': 1.91}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:40,584 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:40,589 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:40,589 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:40,589 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:41 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3602008521556854, 'eval_accuracy': 0.8480392156862745, 'eval_f1': 0.8945578231292517, 'eval_combined_score': 0.8712985194077632, 'eval_runtime': 0.5291, 'eval_samples_per_second': 771.171, 'eval_steps_per_second': 24.572, 'epoch': 1.91}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.2807, 'learning_rate': 2.4e-05, 'epoch': 2.0}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:42,015 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:42,020 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:42,020 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:42,020 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:42 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.42147666215896606, 'eval_accuracy': 0.8480392156862745, 'eval_f1': 0.8966666666666667, 'eval_combined_score': 0.8723529411764706, 'eval_runtime': 0.5306, 'eval_samples_per_second': 768.884, 'eval_steps_per_second': 24.499, 'epoch': 2.0}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.1493, 'learning_rate': 2.0666666666666666e-05, 'epoch': 2.09}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:43,438 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:43,441 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:43,441 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:43,442 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:43 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3926781415939331, 'eval_accuracy': 0.8602941176470589, 'eval_f1': 0.900523560209424, 'eval_combined_score': 0.8804088389282414, 'eval_runtime': 0.5281, 'eval_samples_per_second': 772.605, 'eval_steps_per_second': 24.617, 'epoch': 2.09}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.1437, 'learning_rate': 1.7333333333333336e-05, 'epoch': 2.17}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:44,864 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:44,868 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:44,869 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:44,869 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:45 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.4639076590538025, 'eval_accuracy': 0.8406862745098039, 'eval_f1': 0.8903878583473862, 'eval_combined_score': 0.8655370664285951, 'eval_runtime': 0.5263, 'eval_samples_per_second': 775.204, 'eval_steps_per_second': 24.7, 'epoch': 2.17}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.1144, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.26}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:46,281 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:46,286 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:46,286 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:46,286 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:46 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.4905291497707367, 'eval_accuracy': 0.8382352941176471, 'eval_f1': 0.8892617449664431, 'eval_combined_score': 0.8637485195420451, 'eval_runtime': 0.5325, 'eval_samples_per_second': 766.187, 'eval_steps_per_second': 24.413, 'epoch': 2.26}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.1365, 'learning_rate': 1.0666666666666667e-05, 'epoch': 2.35}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:47,703 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:47,706 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:47,706 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:47,706 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:48 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.42370814085006714, 'eval_accuracy': 0.8529411764705882, 'eval_f1': 0.8924731182795699, 'eval_combined_score': 0.872707147375079, 'eval_runtime': 0.536, 'eval_samples_per_second': 761.185, 'eval_steps_per_second': 24.253, 'epoch': 2.35}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.1009, 'learning_rate': 7.333333333333334e-06, 'epoch': 2.43}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:49,121 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:49,124 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:49,124 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:49,124 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:49 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.44494566321372986, 'eval_accuracy': 0.8431372549019608, 'eval_f1': 0.8881118881118882, 'eval_combined_score': 0.8656245715069245, 'eval_runtime': 0.5282, 'eval_samples_per_second': 772.48, 'eval_steps_per_second': 24.613, 'epoch': 2.43}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.1453, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.52}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:50,550 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:50,554 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:50,554 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:50,555 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:51 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.4933505654335022, 'eval_accuracy': 0.8455882352941176, 'eval_f1': 0.8919382504288165, 'eval_combined_score': 0.8687632428614671, 'eval_runtime': 0.5354, 'eval_samples_per_second': 762.018, 'eval_steps_per_second': 24.28, 'epoch': 2.52}#015\u001b[0m\n",
      "\u001b[34m{'loss': 0.0893, 'learning_rate': 6.666666666666667e-07, 'epoch': 2.61}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:52,096 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:52,104 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:52,104 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:52,104 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:52 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.5071321725845337, 'eval_accuracy': 0.8357843137254902, 'eval_f1': 0.8862478777589134, 'eval_combined_score': 0.8610160957422018, 'eval_runtime': 0.5316, 'eval_samples_per_second': 767.423, 'eval_steps_per_second': 24.452, 'epoch': 2.61}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1369] 2021-08-09 22:42:52,637 >> #015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m{'train_runtime': 47.3751, 'train_samples_per_second': 202.638, 'train_steps_per_second': 6.332, 'train_loss': 0.34905500809351603, 'epoch': 2.61}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1928] 2021-08-09 22:42:52,639 >> Saving model checkpoint to tmp#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:379] 2021-08-09 22:42:52,640 >> Configuration saved in tmp/config.json#015\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1001] 2021-08-09 22:42:53,309 >> Model weights saved in tmp/pytorch_model.bin#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2003] 2021-08-09 22:42:53,310 >> tokenizer config file saved in tmp/tokenizer_config.json#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2009] 2021-08-09 22:42:53,310 >> Special tokens file saved in tmp/special_tokens_map.json#015\u001b[0m\n",
      "\u001b[34m***** train metrics *****#015\n",
      "  epoch                    =       2.61#015\n",
      "  train_loss               =     0.3491#015\n",
      "  train_runtime            = 0:00:47.37#015\n",
      "  train_samples            =       3668#015\n",
      "  train_samples_per_second =    202.638#015\n",
      "  train_steps_per_second   =      6.332#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:53 - INFO - __main__ - *** Evaluate ***#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:53,355 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:53,358 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:53,358 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:53,358 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m08/09/2021 22:42:53 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow#015\u001b[0m\n",
      "\u001b[34m***** eval metrics *****#015\n",
      "  epoch                   =       2.61#015\n",
      "  eval_accuracy           =     0.8358#015\n",
      "  eval_combined_score     =      0.861#015\n",
      "  eval_f1                 =     0.8862#015\n",
      "  eval_loss               =     0.5071#015\n",
      "  eval_runtime            = 0:00:00.52#015\n",
      "  eval_samples            =        408#015\n",
      "  eval_samples_per_second =    774.012#015\n",
      "  eval_steps_per_second   =     24.662#015\u001b[0m\n",
      "\u001b[34m  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-u4q7ypta\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/7.78k [00:00<?, ?B/s]#015Downloading: 28.8kB [00:00, 17.4MB/s]                   \u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/4.47k [00:00<?, ?B/s]#015Downloading: 28.7kB [00:00, 20.5MB/s]                   \u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#015Downloading: 0.00B [00:00, ?B/s]#033[A#015Downloading: 6.22kB [00:00, 4.31MB/s]\u001b[0m\n",
      "\u001b[34m#015 33%|███▎      | 1/3 [00:01<00:02,  1.04s/it]\u001b[0m\n",
      "\u001b[34m#015Downloading: 0.00B [00:00, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Downloading: 44.9kB [00:00, 243kB/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Downloading: 271kB [00:00, 321kB/s] #033[A\u001b[0m\n",
      "\u001b[34m#015Downloading: 764kB [00:00, 446kB/s]#033[A#015Downloading: 1.05MB [00:00, 2.20MB/s]\u001b[0m\n",
      "\u001b[34m#015 67%|██████▋   | 2/3 [00:02<00:01,  1.17s/it]\u001b[0m\n",
      "\u001b[34m#015Downloading: 0.00B [00:00, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Downloading: 54.3kB [00:00, 298kB/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Downloading: 272kB [00:00, 403kB/s] #033[A#015Downloading: 441kB [00:00, 1.18MB/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 3/3 [00:03<00:00,  1.23s/it]#015100%|██████████| 3/3 [00:03<00:00,  1.29s/it]\u001b[0m\n",
      "\u001b[34m#0150 examples [00:00, ? examples/s]#0151550 examples [00:00, 15486.65 examples/s]#0153073 examples [00:00, 15406.46 examples/s]#015                                          #015#0150 examples [00:00, ? examples/s]#015                                #015#0150 examples [00:00, ? examples/s]#015                                #015#015  0%|          | 0/3 [00:00<?, ?it/s]#015100%|██████████| 3/3 [00:00<00:00, 566.52it/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-09 22:41:53,293 >> https://huggingface.co/distilbert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn18a985x\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 442/442 [00:00<00:00, 628kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-09 22:41:53,355 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-09 22:41:53,355 >> creating metadata file for /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:545] 2021-08-09 22:41:53,355 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:581] 2021-08-09 22:41:53,356 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m2021-08-09 22:43:00,082 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.10.0.dev0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-09 22:41:53,420 >> https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpae7bm5ky\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 35.9kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-09 22:41:53,504 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-09 22:41:53,504 >> creating metadata file for /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:545] 2021-08-09 22:41:53,570 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:581] 2021-08-09 22:41:53,570 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.10.0.dev0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-09 22:41:53,698 >> https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1j1cz7vs\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 9.12MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-09 22:41:53,790 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-09 22:41:53,790 >> creating metadata file for /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-09 22:41:53,855 >> https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpaens3f_r\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 466k/466k [00:00<00:00, 12.1MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-09 22:41:53,962 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-09 22:41:53,962 >> creating metadata file for /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-09 22:41:54,153 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-09 22:41:54,154 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-09 22:41:54,154 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-09 22:41:54,154 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-09 22:41:54,154 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:545] 2021-08-09 22:41:54,221 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:581] 2021-08-09 22:41:54,222 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.10.0.dev0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-09 22:41:54,348 >> https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptgwb7zrf\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]#015Downloading:   1%|▏         | 3.62M/268M [00:00<00:07, 36.1MB/s]#015Downloading:   3%|▎         | 8.94M/268M [00:00<00:06, 40.0MB/s]#015Downloading:   5%|▌         | 14.3M/268M [00:00<00:05, 43.3MB/s]#015Downloading:   7%|▋         | 19.8M/268M [00:00<00:05, 46.2MB/s]#015Downloading:   9%|▉         | 25.2M/268M [00:00<00:05, 48.2MB/s]#015Downloading:  11%|█▏        | 30.7M/268M [00:00<00:04, 50.2MB/s]#015Downloading:  14%|█▎        | 36.3M/268M [00:00<00:04, 51.8MB/s]#015Downloading:  16%|█▌        | 41.9M/268M [00:00<00:04, 53.0MB/s]#015Downloading:  18%|█▊        | 47.6M/268M [00:00<00:04, 54.1MB/s]#015Downloading:  20%|█▉        | 53.0M/268M [00:01<00:03, 54.0MB/s]#015Downloading:  22%|██▏       | 58.3M/268M [00:01<00:03, 52.5MB/s]#015Downloading:  24%|██▍       | 64.0M/268M [00:01<00:03, 53.6MB/s]#015Downloading:  26%|██▌       | 69.6M/268M [00:01<00:03, 54.5MB/s]#015Downloading:  28%|██▊       | 75.3M/268M [00:01<00:03, 55.1MB/s]#015Downloading:  30%|███       | 80.8M/268M [00:01<00:03, 54.8MB/s]#015Downloading:  32%|███▏      | 86.5M/268M [00:01<00:03, 55.4MB/s]#015Downloading:  34%|███▍      | 92.2M/268M [00:01<00:03, 55.9MB/s]#015Downloading:  37%|███▋      | 97.9M/268M [00:01<00:03, 56.1MB/s]#015Downloading:  39%|███▊      | 104M/268M [00:01<00:02, 56.3MB/s] #015Downloading:  41%|████      | 109M/268M [00:02<00:02, 56.5MB/s]#015Downloading:  43%|████▎     | 115M/268M [00:02<00:02, 56.5MB/s]#015Downloading:  45%|████▍     | 121M/268M [00:02<00:02, 56.5MB/s]#015Downloading:  47%|████▋     | 126M/268M [00:02<00:02, 56.5MB/s]#015Downloading:  49%|████▉     | 132M/268M [00:02<00:02, 56.7MB/s]#015Downloading:  51%|█████▏    | 138M/268M [00:02<00:02, 56.0MB/s]#015Downloading:  53%|█████▎    | 143M/268M [00:02<00:02, 56.1MB/s]#015Downloading:  56%|█████▌    | 149M/268M [00:02<00:02, 56.2MB/s]#015Downloading:  58%|█████▊    | 154M/268M [00:02<00:02, 56.3MB/s]#015Downloading:  60%|█████▉    | 160M/268M [00:02<00:01, 56.5MB/s]#015Downloading:  62%|██████▏   | 166M/268M [00:03<00:01, 56.6MB/s]#015Downloading:  64%|██████▍   | 172M/268M [00:03<00:01, 56.6MB/s]#015Downloading:  66%|██████▌   | 177M/268M [00:03<00:01, 56.7MB/s]#015Downloading:  68%|██████▊   | 183M/268M [00:03<00:01, 56.8MB/s]#015Downloading:  70%|███████   | 189M/268M [00:03<00:01, 56.8MB/s]#015Downloading:  73%|███████▎  | 194M/268M [00:03<00:01, 56.1MB/s]#015Downloading:  75%|███████▍  | 200M/268M [00:03<00:01, 55.9MB/s]#015Downloading:  77%|███████▋  | 205M/268M [00:03<00:01, 54.8MB/s]#015Downloading:  79%|███████▉  | 211M/268M [00:03<00:01, 55.3MB/s]#015Downloading:  81%|████████  | 217M/268M [00:03<00:00, 55.7MB/s]#015Downloading:  83%|████████▎ | 222M/268M [00:04<00:00, 55.9MB/s]#015Downloading:  85%|████████▌ | 228M/268M [00:04<00:00, 56.1MB/s]#015Downloading:  87%|████████▋ | 234M/268M [00:04<00:00, 56.3MB/s]#015Downloading:  89%|████████▉ | 239M/268M [00:04<00:00, 56.4MB/s]#015Downloading:  91%|█████████▏| 245M/268M [00:04<00:00, 56.5MB/s]#015Downloading:  94%|█████████▎| 251M/268M [00:04<00:00, 55.8MB/s]#015Downloading:  96%|█████████▌| 256M/268M [00:04<00:00, 56.0MB/s]#015Downloading:  98%|█████████▊| 262M/268M [00:04<00:00, 56.2MB/s]#015Downloading: 100%|█████████▉| 268M/268M [00:04<00:00, 56.6MB/s]#015Downloading: 100%|██████████| 268M/268M [00:04<00:00, 55.4MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-09 22:41:59,373 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-09 22:41:59,374 >> creating metadata file for /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1275] 2021-08-09 22:41:59,374 >> loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1512] 2021-08-09 22:42:00,317 >> Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1523] 2021-08-09 22:42:00,317 >> Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015Running tokenizer on dataset:   0%|          | 0/4 [00:00<?, ?ba/s]#015Running tokenizer on dataset:  25%|██▌       | 1/4 [00:00<00:00,  7.97ba/s]#015Running tokenizer on dataset:  75%|███████▌  | 3/4 [00:00<00:00,  9.17ba/s]#015Running tokenizer on dataset: 100%|██████████| 4/4 [00:00<00:00, 12.43ba/s]\u001b[0m\n",
      "\u001b[34m#015Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]#015Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00, 24.76ba/s]\u001b[0m\n",
      "\u001b[34m#015Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]#015Running tokenizer on dataset: 100%|██████████| 2/2 [00:00<00:00, 15.25ba/s]#015Running tokenizer on dataset: 100%|██████████| 2/2 [00:00<00:00, 15.22ba/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/1.86k [00:00<?, ?B/s]#015Downloading: 5.78kB [00:00, 5.09MB/s]                   \u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:404] 2021-08-09 22:42:05,240 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:421] 2021-08-09 22:42:05,240 >> Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:05,241 >> The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1171] 2021-08-09 22:42:05,261 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1172] 2021-08-09 22:42:05,261 >>   Num examples = 3668\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1173] 2021-08-09 22:42:05,261 >>   Num Epochs = 3\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1174] 2021-08-09 22:42:05,261 >>   Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1175] 2021-08-09 22:42:05,261 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1176] 2021-08-09 22:42:05,262 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1177] 2021-08-09 22:42:05,262 >>   Total optimization steps = 300\u001b[0m\n",
      "\u001b[34m[INFO|integrations.py:447] 2021-08-09 22:42:05,264 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: morgan (use `wandb login --relogin` to force relogin)\n",
      "\u001b[0m\n",
      "\u001b[34mCondaEnvException: Unable to determine environment\n",
      "\u001b[0m\n",
      "\u001b[34mPlease re-run this command with one of the following options:\n",
      "\u001b[0m\n",
      "\u001b[34m* Provide an environment name via --name or -n\u001b[0m\n",
      "\u001b[34m* Re-run this command inside an activated conda environment.\n",
      "\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.11.2\u001b[0m\n",
      "\u001b[34mwandb: Syncing run morg_demo\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/morgan/hf-sagemaker\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/morgan/hf-sagemaker/runs/huggingface-pytorch-training-2021-08-09-22-33-46-228-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20210809_224205-huggingface-pytorch-training-2021-08-09-22-33-46-228-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34m#015  0% 0/300 [00:00<?, ?it/s]#015  0% 1/300 [00:01<07:35,  1.52s/it]#015  1% 3/300 [00:01<05:24,  1.09s/it]#015  2% 5/300 [00:01<03:53,  1.26it/s]#015  2% 7/300 [00:02<02:52,  1.70it/s]#015  3% 9/300 [00:02<02:07,  2.29it/s]#015                                   #015#015  3% 10/300 [00:02<02:06,  2.29it/s][INFO|trainer.py:528] 2021-08-09 22:42:10,273 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:10,276 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:10,277 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:10,277 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 33.02it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 30.04it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.29it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.84it/s]#033[A#015                                    #015#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015  3% 10/300 [00:02<02:06,  2.29it/s]#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.84it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015  4% 11/300 [00:03<01:59,  2.41it/s]#015  4% 13/300 [00:03<01:30,  3.16it/s]#015  5% 15/300 [00:03<01:10,  4.03it/s]#015  6% 17/300 [00:03<00:56,  5.00it/s]#015  6% 19/300 [00:03<00:46,  6.02it/s]#015                                    #015#015  7% 20/300 [00:03<00:46,  6.02it/s][INFO|trainer.py:528] 2021-08-09 22:42:11,699 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:11,702 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:11,702 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:11,703 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 33.09it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 29.13it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 27.75it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.43it/s]#033[A#015                                    #015#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015  7% 20/300 [00:04<00:46,  6.02it/s]#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.43it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015  7% 21/300 [00:04<01:02,  4.47it/s]#015  8% 23/300 [00:04<00:50,  5.43it/s]#015  8% 25/300 [00:04<00:42,  6.43it/s]#015  9% 26/300 [00:04<00:38,  7.19it/s]#015  9% 27/300 [00:04<00:34,  7.82it/s]#015  9% 28/300 [00:05<00:33,  8.10it/s]#015 10% 30/300 [00:05<00:31,  8.60it/s]#015                                    #015#015 10% 30/300 [00:05<00:31,  8.60it/s][INFO|trainer.py:528] 2021-08-09 22:42:13,203 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:13,206 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:13,207 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:13,207 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 32.73it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 29.59it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 27.80it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.48it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                    #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.48it/s]#033[A#015 10% 30/300 [00:05<00:31,  8.60it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 10% 31/300 [00:05<01:12,  3.72it/s]#015 11% 33/300 [00:06<00:57,  4.64it/s]#015 12% 35/300 [00:06<00:46,  5.64it/s]#015 12% 37/300 [00:06<00:39,  6.63it/s]#015 13% 39/300 [00:06<00:34,  7.58it/s]#015                                    #015#015 13% 40/300 [00:06<00:34,  7.58it/s][INFO|trainer.py:528] 2021-08-09 22:42:14,628 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:14,631 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:14,631 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:14,632 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 32.89it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 29.94it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.12it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.65it/s]#033[A#015                                    #015#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015 13% 40/300 [00:07<00:34,  7.58it/s]#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.65it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 14% 41/300 [00:07<00:51,  5.03it/s]#015 14% 43/300 [00:07<00:42,  6.03it/s]#015 15% 45/300 [00:07<00:36,  7.02it/s]#015 16% 47/300 [00:07<00:31,  7.95it/s]#015 16% 49/300 [00:08<00:28,  8.74it/s]#015                                    #015#015 17% 50/300 [00:08<00:28,  8.74it/s][INFO|trainer.py:528] 2021-08-09 22:42:16,051 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:16,055 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:16,055 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:16,055 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 33.21it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 30.13it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.35it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.85it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                    #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.85it/s]#033[A#015 17% 50/300 [00:08<00:28,  8.74it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 17% 51/300 [00:08<00:46,  5.32it/s]#015 18% 53/300 [00:08<00:38,  6.44it/s]#015 18% 55/300 [00:09<00:33,  7.41it/s]#015 19% 57/300 [00:09<00:28,  8.43it/s]#015 20% 59/300 [00:09<00:26,  9.16it/s]#015                                    #015#015 20% 60/300 [00:09<00:26,  9.16it/s][INFO|trainer.py:528] 2021-08-09 22:42:17,427 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:17,430 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:17,430 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:17,430 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 31.71it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 29.14it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 27.72it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.35it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                    #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.35it/s]#033[A#015 20% 60/300 [00:10<00:26,  9.16it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 20% 61/300 [00:10<00:44,  5.39it/s]#015 21% 63/300 [00:10<00:37,  6.38it/s]#015 22% 65/300 [00:10<00:32,  7.30it/s]#015 22% 67/300 [00:10<00:28,  8.17it/s]#015 23% 69/300 [00:10<00:25,  8.89it/s]#015                                    #015#015 23% 70/300 [00:10<00:25,  8.89it/s][INFO|trainer.py:528] 2021-08-09 22:42:18,871 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:18,874 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:18,875 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:18,875 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 33.25it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 30.19it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.40it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.48it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                    #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.48it/s]#033[A#015 23% 70/300 [00:11<00:25,  8.89it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 24% 71/300 [00:11<00:42,  5.39it/s]#015 24% 73/300 [00:11<00:35,  6.37it/s]#015 25% 74/300 [00:11<00:31,  7.11it/s]#015 25% 76/300 [00:12<00:28,  7.96it/s]#015 26% 77/300 [00:12<00:26,  8.46it/s]#015 26% 79/300 [00:12<00:24,  9.12it/s]#015                                    #015#015 27% 80/300 [00:12<00:24,  9.12it/s][INFO|trainer.py:528] 2021-08-09 22:42:20,327 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:20,331 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:20,332 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:20,332 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 33.47it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 30.08it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.31it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.87it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                    #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.87it/s]#033[A#015 27% 80/300 [00:12<00:24,  9.12it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 27% 81/300 [00:13<00:40,  5.46it/s]#015 28% 83/300 [00:13<00:33,  6.43it/s]#015 28% 85/300 [00:13<00:29,  7.40it/s]#015 29% 87/300 [00:13<00:25,  8.28it/s]#015 30% 89/300 [00:13<00:23,  9.03it/s]#015                                    #015#015 30% 90/300 [00:13<00:23,  9.03it/s][INFO|trainer.py:528] 2021-08-09 22:42:21,743 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:21,746 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:21,746 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:21,746 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 33.03it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 30.06it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.08it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.64it/s]#033[A#015                                    #015#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015 30% 90/300 [00:14<00:23,  9.03it/s]#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.64it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 30% 91/300 [00:14<00:38,  5.43it/s]#015 31% 93/300 [00:14<00:32,  6.42it/s]#015 32% 95/300 [00:14<00:27,  7.40it/s]#015 32% 97/300 [00:14<00:24,  8.28it/s]#015 33% 99/300 [00:15<00:22,  9.03it/s]#015                                    #015#015 33% 100/300 [00:15<00:22,  9.03it/s][INFO|trainer.py:528] 2021-08-09 22:42:23,350 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:23,353 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:23,354 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:23,354 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 33.03it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 29.77it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.07it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.64it/s]#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015 33% 100/300 [00:15<00:22,  9.03it/s]#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.64it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 34% 101/300 [00:16<00:42,  4.69it/s]#015 34% 103/300 [00:16<00:34,  5.66it/s]#015 35% 105/300 [00:16<00:29,  6.67it/s]#015 36% 107/300 [00:16<00:25,  7.59it/s]#015 36% 109/300 [00:16<00:22,  8.44it/s]#015                                     #015#015 37% 110/300 [00:16<00:22,  8.44it/s][INFO|trainer.py:528] 2021-08-09 22:42:24,772 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:24,775 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:24,775 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:24,775 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 32.95it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 29.75it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.05it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.64it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.64it/s]#033[A#015 37% 110/300 [00:17<00:22,  8.44it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 37% 111/300 [00:17<00:35,  5.28it/s]#015 38% 113/300 [00:17<00:29,  6.27it/s]#015 38% 115/300 [00:17<00:25,  7.32it/s]#015 39% 117/300 [00:18<00:22,  8.21it/s]#015 40% 119/300 [00:18<00:20,  8.85it/s]#015                                     #015#015 40% 120/300 [00:18<00:20,  8.85it/s][INFO|trainer.py:528] 2021-08-09 22:42:26,194 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:26,197 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:26,198 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:26,198 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 32.92it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 29.94it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.15it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.67it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.67it/s]#033[A#015 40% 120/300 [00:18<00:20,  8.85it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 40% 121/300 [00:18<00:33,  5.37it/s]#015 41% 123/300 [00:19<00:27,  6.38it/s]#015 42% 125/300 [00:19<00:23,  7.34it/s]#015 42% 127/300 [00:19<00:21,  8.23it/s]#015 43% 129/300 [00:19<00:19,  8.94it/s]#015                                     #015#015 43% 130/300 [00:19<00:19,  8.94it/s][INFO|trainer.py:528] 2021-08-09 22:42:27,607 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:27,610 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:27,610 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:27,610 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 33.09it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 30.18it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.36it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.55it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.55it/s]#033[A#015 43% 130/300 [00:20<00:19,  8.94it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 44% 131/300 [00:20<00:31,  5.42it/s]#015 44% 133/300 [00:20<00:25,  6.44it/s]#015 45% 135/300 [00:20<00:22,  7.37it/s]#015 46% 137/300 [00:20<00:19,  8.24it/s]#015 46% 139/300 [00:21<00:17,  8.99it/s]#015                                     #015#015 47% 140/300 [00:21<00:17,  8.99it/s][INFO|trainer.py:528] 2021-08-09 22:42:29,020 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:29,023 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:29,024 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:29,024 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 30.02it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 28.18it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 27.09it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 26.93it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 26.93it/s]#033[A#015 47% 140/300 [00:21<00:17,  8.99it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 47% 141/300 [00:21<00:29,  5.34it/s]#015 48% 143/300 [00:21<00:24,  6.34it/s]#015 48% 145/300 [00:22<00:21,  7.29it/s]#015 49% 146/300 [00:22<00:19,  7.87it/s]#015 49% 148/300 [00:22<00:17,  8.65it/s]#015 50% 150/300 [00:22<00:16,  9.20it/s]#015                                     #015#015 50% 150/300 [00:22<00:16,  9.20it/s][INFO|trainer.py:528] 2021-08-09 22:42:30,484 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:30,487 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:30,487 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:30,488 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 32.28it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 29.49it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 27.83it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.38it/s]#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015 50% 150/300 [00:23<00:16,  9.20it/s]#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.38it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 51% 152/300 [00:23<00:27,  5.41it/s]#015 51% 154/300 [00:23<00:22,  6.40it/s]#015 52% 156/300 [00:23<00:19,  7.35it/s]#015 53% 158/300 [00:23<00:17,  8.25it/s]#015 53% 160/300 [00:24<00:15,  9.01it/s]#015                                     #015#015 53% 160/300 [00:24<00:15,  9.01it/s][INFO|trainer.py:528] 2021-08-09 22:42:31,914 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:31,917 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:31,917 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:31,917 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 31.58it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 29.14it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 27.65it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.31it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.31it/s]#033[A#015 53% 160/300 [00:24<00:15,  9.01it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 54% 162/300 [00:24<00:25,  5.39it/s]#015 55% 164/300 [00:24<00:21,  6.39it/s]#015 55% 166/300 [00:25<00:18,  7.35it/s]#015 56% 168/300 [00:25<00:16,  8.20it/s]#015 57% 170/300 [00:25<00:14,  8.94it/s]#015                                     #015#015 57% 170/300 [00:25<00:14,  8.94it/s][INFO|trainer.py:528] 2021-08-09 22:42:33,342 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:33,345 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:33,345 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:33,345 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 32.89it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 29.47it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 27.82it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.48it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.48it/s]#033[A#015 57% 170/300 [00:25<00:14,  8.94it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 57% 172/300 [00:26<00:23,  5.34it/s]#015 58% 174/300 [00:26<00:19,  6.31it/s]#015 59% 176/300 [00:26<00:17,  7.27it/s]#015 59% 178/300 [00:26<00:15,  8.10it/s]#015 60% 180/300 [00:26<00:13,  8.84it/s]#015                                     #015#015 60% 180/300 [00:26<00:13,  8.84it/s][INFO|trainer.py:528] 2021-08-09 22:42:34,787 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:34,790 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:34,790 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:34,790 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 33.07it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 30.09it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.04it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.64it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.64it/s]#033[A#015 60% 180/300 [00:27<00:13,  8.84it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 61% 182/300 [00:27<00:21,  5.40it/s]#015 61% 184/300 [00:27<00:18,  6.39it/s]#015 62% 186/300 [00:27<00:15,  7.37it/s]#015 63% 188/300 [00:28<00:13,  8.26it/s]#015 63% 190/300 [00:28<00:12,  9.00it/s]#015                                     #015#015 63% 190/300 [00:28<00:12,  9.00it/s][INFO|trainer.py:528] 2021-08-09 22:42:36,197 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:36,200 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:36,200 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:36,200 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 32.79it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 29.94it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.13it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.64it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.64it/s]#033[A#015 63% 190/300 [00:28<00:12,  9.00it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 64% 192/300 [00:29<00:20,  5.40it/s]#015 65% 194/300 [00:29<00:16,  6.40it/s]#015 65% 196/300 [00:29<00:14,  7.33it/s]#015 66% 198/300 [00:29<00:12,  8.20it/s]#015 67% 200/300 [00:29<00:12,  7.99it/s]#015                                     #015#015 67% 200/300 [00:29<00:12,  7.99it/s][INFO|trainer.py:528] 2021-08-09 22:42:37,720 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:37,723 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:37,723 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:37,723 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 33.22it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 30.19it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.30it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.79it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.79it/s]#033[A#015 67% 200/300 [00:30<00:12,  7.99it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 67% 201/300 [00:30<00:27,  3.63it/s]#015 68% 203/300 [00:30<00:21,  4.54it/s]#015 68% 205/300 [00:30<00:17,  5.49it/s]#015 69% 207/300 [00:30<00:14,  6.48it/s]#015 70% 209/300 [00:31<00:12,  7.43it/s]#015                                     #015#015 70% 210/300 [00:31<00:12,  7.43it/s][INFO|trainer.py:528] 2021-08-09 22:42:39,153 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:39,158 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:39,158 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:39,158 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 33.20it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 30.11it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.32it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.31it/s]#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015 70% 210/300 [00:31<00:12,  7.43it/s]#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.31it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 70% 211/300 [00:31<00:18,  4.93it/s]#015 71% 213/300 [00:32<00:14,  5.95it/s]#015 72% 215/300 [00:32<00:12,  6.87it/s]#015 72% 217/300 [00:32<00:10,  7.81it/s]#015 73% 219/300 [00:32<00:09,  8.63it/s]#015                                     #015#015 73% 220/300 [00:32<00:09,  8.63it/s][INFO|trainer.py:528] 2021-08-09 22:42:40,584 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:40,589 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:40,589 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:40,589 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 33.09it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 30.05it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.07it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.55it/s]#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015 73% 220/300 [00:33<00:09,  8.63it/s]#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.55it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 74% 221/300 [00:33<00:14,  5.30it/s]#015 74% 223/300 [00:33<00:12,  6.29it/s]#015 75% 225/300 [00:33<00:10,  7.19it/s]#015 76% 227/300 [00:33<00:09,  8.07it/s]#015 76% 229/300 [00:34<00:08,  8.81it/s]#015                                     #015#015 77% 230/300 [00:34<00:07,  8.81it/s][INFO|trainer.py:528] 2021-08-09 22:42:42,015 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:42,020 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:42,020 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:42,020 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 32.55it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 29.77it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.09it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.64it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.64it/s]#033[A#015 77% 230/300 [00:34<00:07,  8.81it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 77% 231/300 [00:34<00:12,  5.35it/s]#015 78% 233/300 [00:34<00:10,  6.36it/s]#015 78% 235/300 [00:35<00:08,  7.33it/s]#015 79% 237/300 [00:35<00:07,  8.22it/s]#015 80% 239/300 [00:35<00:06,  8.99it/s]#015                                     #015#015 80% 240/300 [00:35<00:06,  8.99it/s][INFO|trainer.py:528] 2021-08-09 22:42:43,438 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:43,441 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:43,441 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:43,442 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 33.01it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 29.96it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.21it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.72it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.72it/s]#033[A#015 80% 240/300 [00:36<00:06,  8.99it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 80% 241/300 [00:36<00:10,  5.40it/s]#015 81% 242/300 [00:36<00:09,  6.25it/s]#015 81% 244/300 [00:36<00:07,  7.26it/s]#015 82% 246/300 [00:36<00:06,  8.17it/s]#015 83% 248/300 [00:36<00:05,  8.91it/s]#015 83% 250/300 [00:36<00:05,  9.55it/s]#015                                     #015#015 83% 250/300 [00:36<00:05,  9.55it/s][INFO|trainer.py:528] 2021-08-09 22:42:44,864 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:44,868 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:44,869 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:44,869 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 33.15it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 30.16it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.36it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.86it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.86it/s]#033[A#015 83% 250/300 [00:37<00:05,  9.55it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 84% 252/300 [00:37<00:08,  5.55it/s]#015 85% 254/300 [00:37<00:07,  6.53it/s]#015 85% 256/300 [00:38<00:05,  7.52it/s]#015 86% 258/300 [00:38<00:05,  8.38it/s]#015 87% 260/300 [00:38<00:04,  9.07it/s]#015                                     #015#015 87% 260/300 [00:38<00:04,  9.07it/s][INFO|trainer.py:528] 2021-08-09 22:42:46,281 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:46,286 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:46,286 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:46,286 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 32.99it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 29.41it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 27.82it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.46it/s]#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015 87% 260/300 [00:38<00:04,  9.07it/s]#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.46it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 87% 262/300 [00:39<00:07,  5.42it/s]#015 88% 264/300 [00:39<00:05,  6.41it/s]#015 89% 266/300 [00:39<00:04,  7.39it/s]#015 89% 268/300 [00:39<00:03,  8.28it/s]#015 90% 270/300 [00:39<00:03,  8.97it/s]#015                                     #015#015 90% 270/300 [00:39<00:03,  8.97it/s][INFO|trainer.py:528] 2021-08-09 22:42:47,703 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:47,706 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:47,706 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:47,706 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 33.26it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 30.08it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 28.29it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 26.92it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 26.92it/s]#033[A#015 90% 270/300 [00:40<00:03,  8.97it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 91% 272/300 [00:40<00:05,  5.39it/s]#015 91% 274/300 [00:40<00:04,  6.42it/s]#015 92% 276/300 [00:40<00:03,  7.34it/s]#015 93% 278/300 [00:41<00:02,  8.23it/s]#015 93% 280/300 [00:41<00:02,  9.00it/s]#015                                     #015#015 93% 280/300 [00:41<00:02,  9.00it/s][INFO|trainer.py:528] 2021-08-09 22:42:49,121 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:49,124 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:49,124 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:49,124 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 31.49it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 29.22it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 27.76it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.47it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.47it/s]#033[A#015 93% 280/300 [00:41<00:02,  9.00it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 94% 282/300 [00:41<00:03,  5.39it/s]#015 95% 284/300 [00:42<00:02,  6.42it/s]#015 95% 286/300 [00:42<00:01,  7.30it/s]#015 96% 288/300 [00:42<00:01,  8.19it/s]#015 97% 290/300 [00:42<00:01,  8.92it/s]#015                                     #015#015 97% 290/300 [00:42<00:01,  8.92it/s][INFO|trainer.py:528] 2021-08-09 22:42:50,550 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:50,554 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:50,554 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:50,555 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 32.18it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 29.42it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 27.90it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.47it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.47it/s]#033[A#015 97% 290/300 [00:43<00:01,  8.92it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A#015 97% 292/300 [00:43<00:01,  5.36it/s]#015 98% 294/300 [00:43<00:00,  6.36it/s]#015 99% 296/300 [00:43<00:00,  7.15it/s]#015 99% 298/300 [00:43<00:00,  8.00it/s]#015100% 300/300 [00:44<00:00,  7.90it/s]#015                                     #015#015100% 300/300 [00:44<00:00,  7.90it/s][INFO|trainer.py:528] 2021-08-09 22:42:52,096 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:52,104 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:52,104 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:52,104 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 31% 4/13 [00:00<00:00, 31.46it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 54% 7/13 [00:00<00:00, 29.12it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015 77% 10/13 [00:00<00:00, 27.66it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.32it/s]#033[A#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#033[A#015                                     #015#015\u001b[0m\n",
      "\u001b[34m#015100% 13/13 [00:00<00:00, 27.32it/s]#033[A#015100% 300/300 [00:44<00:00,  7.90it/s]#015\u001b[0m\n",
      "\u001b[34m#015                                   #033[A[INFO|trainer.py:1369] 2021-08-09 22:42:52,637 >> #015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015                                     #015#015100% 300/300 [00:44<00:00,  7.90it/s]#015100% 300/300 [00:44<00:00,  6.71it/s]#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1928] 2021-08-09 22:42:52,639 >> Saving model checkpoint to tmp#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:379] 2021-08-09 22:42:52,640 >> Configuration saved in tmp/config.json#015\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1001] 2021-08-09 22:42:53,309 >> Model weights saved in tmp/pytorch_model.bin#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2003] 2021-08-09 22:42:53,310 >> tokenizer config file saved in tmp/tokenizer_config.json#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2009] 2021-08-09 22:42:53,310 >> Special tokens file saved in tmp/special_tokens_map.json#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-09 22:42:53,355 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-09 22:42:53,358 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-09 22:42:53,358 >>   Num examples = 408#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-09 22:42:53,358 >>   Batch size = 32#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/13 [00:00<?, ?it/s]#015 31% 4/13 [00:00<00:00, 33.12it/s]#015 54% 7/13 [00:00<00:00, 30.01it/s]#015 77% 10/13 [00:00<00:00, 28.21it/s]#015100% 13/13 [00:00<00:00, 27.61it/s]#015100% 13/13 [00:00<00:00, 26.76it/s]#015\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish, PID 134\u001b[0m\n",
      "\u001b[34mwandb: Program ended successfully.\u001b[0m\n",
      "\u001b[34mwandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)#015wandb: \\ 0.03MB of 0.03MB uploaded (0.00MB deduped)#015wandb: | 0.03MB of 0.26MB uploaded (0.00MB deduped)#015wandb: / 0.03MB of 0.26MB uploaded (0.00MB deduped)#015wandb: - 0.26MB of 0.26MB uploaded (0.00MB deduped)#015wandb: \\ 0.26MB of 0.26MB uploaded (0.00MB deduped)#015wandb: | 0.26MB of 0.26MB uploaded (0.00MB deduped)#015wandb: / 0.26MB of 0.26MB uploaded (0.00MB deduped)#015wandb: - 0.26MB of 0.26MB uploaded (0.00MB deduped)#015wandb: \\ 0.26MB of 0.26MB uploaded (0.00MB deduped)#015wandb:                                                                                \u001b[0m\n",
      "\u001b[34mwandb: Find user logs for this run at: /opt/ml/code/wandb/run-20210809_224205-huggingface-pytorch-training-2021-08-09-22-33-46-228-algo-1/logs/debug.log\u001b[0m\n",
      "\u001b[34mwandb: Find internal logs for this run at: /opt/ml/code/wandb/run-20210809_224205-huggingface-pytorch-training-2021-08-09-22-33-46-228-algo-1/logs/debug-internal.log\u001b[0m\n",
      "\u001b[34mwandb: Run summary:\u001b[0m\n",
      "\u001b[34mwandb:                                                          train/loss 0.0893\u001b[0m\n",
      "\u001b[34mwandb:                                                 train/learning_rate 0.0\u001b[0m\n",
      "\u001b[34mwandb:                                                         train/epoch 2.61\u001b[0m\n",
      "\u001b[34mwandb:                                                   train/global_step 300\u001b[0m\n",
      "\u001b[34mwandb:                                                            _runtime 47\u001b[0m\n",
      "\u001b[34mwandb:                                                          _timestamp 1628548973\u001b[0m\n",
      "\u001b[34mwandb:                                                               _step 61\u001b[0m\n",
      "\u001b[34mwandb:                                                           eval/loss 0.50713\u001b[0m\n",
      "\u001b[34mwandb:                                                       eval/accuracy 0.83578\u001b[0m\n",
      "\u001b[34mwandb:                                                             eval/f1 0.88625\u001b[0m\n",
      "\u001b[34mwandb:                                                 eval/combined_score 0.86102\u001b[0m\n",
      "\u001b[34mwandb:                                                        eval/runtime 0.5271\u001b[0m\n",
      "\u001b[34mwandb:                                             eval/samples_per_second 774.012\u001b[0m\n",
      "\u001b[34mwandb:                                               eval/steps_per_second 24.662\u001b[0m\n",
      "\u001b[34mwandb:                                                 train/train_runtime 47.3751\u001b[0m\n",
      "\u001b[34mwandb:                                      train/train_samples_per_second 202.638\u001b[0m\n",
      "\u001b[34mwandb:                                        train/train_steps_per_second 6.332\u001b[0m\n",
      "\u001b[34mwandb:                                                    train/total_flos 317126952382464.0\u001b[0m\n",
      "\u001b[34mwandb:                                                    train/train_loss 0.34906\u001b[0m\n",
      "\u001b[34mwandb: Run history:\u001b[0m\n",
      "\u001b[34mwandb:                       train/loss ███▇██▇▇▆▆▅▅▄▃▄▄▄▄▃▄▄▃▄▂▂▁▂▁▂▁\u001b[0m\n",
      "\u001b[34mwandb:              train/learning_rate ███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁\u001b[0m\n",
      "\u001b[34mwandb:                      train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\u001b[0m\n",
      "\u001b[34mwandb:                train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\u001b[0m\n",
      "\u001b[34mwandb:                         _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\u001b[0m\n",
      "\u001b[34mwandb:                       _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\u001b[0m\n",
      "\u001b[34mwandb:                            _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\u001b[0m\n",
      "\u001b[34mwandb:                        eval/loss ██▇▇▅▅▄▄▃▂▂▁▂▁▁▂▄▂▃▁▁▁▃▂▄▅▃▄▅▅▅\u001b[0m\n",
      "\u001b[34mwandb:                    eval/accuracy ▂▁▂▂▂▄▅▅▆▇██▇███▆▇▆▇████▇▇█▇▇▇▇\u001b[0m\n",
      "\u001b[34mwandb:                          eval/f1 ▃▁▃▃▃▂▅▅▆▇██▆██▇▆▆▇▆████▇▇▇▇▇▇▇\u001b[0m\n",
      "\u001b[34mwandb:              eval/combined_score ▂▁▂▂▃▃▅▅▆▇██▇██▇▆▆▇▇████▇▇█▇▇▇▇\u001b[0m\n",
      "\u001b[34mwandb:                     eval/runtime ▇▇▅▃▁▅▃▃▂▃▃▂▃█▅▆▅▃▂▂▅▄▅▃▂▅▇▃▇▅▃\u001b[0m\n",
      "\u001b[34mwandb:          eval/samples_per_second ▂▂▄▆█▄▆▆▇▆▆▇▆▁▄▃▄▆▇▇▄▅▄▆▇▃▂▆▂▄▆\u001b[0m\n",
      "\u001b[34mwandb:            eval/steps_per_second ▂▂▄▆█▄▆▆▇▆▆▇▆▁▄▃▄▆▇▇▄▅▄▆▇▃▂▆▂▄▆\u001b[0m\n",
      "\u001b[34mwandb:              train/train_runtime ▁\u001b[0m\n",
      "\u001b[34mwandb:   train/train_samples_per_second ▁\u001b[0m\n",
      "\u001b[34mwandb:     train/train_steps_per_second ▁\u001b[0m\n",
      "\u001b[34mwandb:                 train/total_flos ▁\u001b[0m\n",
      "\u001b[34mwandb:                 train/train_loss ▁\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Synced morg_demo: https://wandb.ai/morgan/hf-sagemaker/runs/huggingface-pytorch-training-2021-08-09-22-33-46-228-algo-1\n",
      "\u001b[0m\n",
      "\n",
      "2021-08-09 22:43:12 Uploading - Uploading generated training model\n",
      "2021-08-09 22:43:12 Completed - Training job completed\n",
      "Training seconds: 420\n",
      "Billable seconds: 420\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "# huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})\n",
    "# huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_input= {\"inputs\":\"I love using the new Inference DLC.\"}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container image used for training job: \n",
      "558105141721.dkr.ecr.us-east-1.amazonaws.com/huggingface-training:pytorch1.6.0-transformers4.2.2-tokenizers0.9.4-datasets1.2.1-py36-gpu-cu110\n",
      "\n",
      "s3 uri where the trained model is located: \n",
      "s3://philipps-sagemaker-bucket-us-east-1/huggingface-training-2021-02-04-16-47-39-189/output/model.tar.gz\n",
      "\n",
      "latest training job name for this estimator: \n",
      "huggingface-training-2021-02-04-16-47-39-189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach to old training job to an estimator \n",
    "\n",
    "In Sagemaker you can attach an old training job to an estimator to continue training, get results etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# job which is going to be attached to the estimator\n",
    "old_training_job_name=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-01-15 19:31:50 Starting - Preparing the instances for training\n",
      "2021-01-15 19:31:50 Downloading - Downloading input data\n",
      "2021-01-15 19:31:50 Training - Training image download completed. Training in progress.\n",
      "2021-01-15 19:31:50 Uploading - Uploading generated training model\n",
      "2021-01-15 19:31:50 Completed - Training job completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://philipps-sagemaker-bucket-eu-central-1/huggingface-sdk-extension-2021-01-15-19-14-13-725/output/model.tar.gz'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attach old training job\n",
    "huggingface_estimator_loaded = Estimator.attach(old_training_job_name)\n",
    "\n",
    "# get model output s3 from training job\n",
    "huggingface_estimator_loaded.model_data"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

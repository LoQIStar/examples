{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "<!--- @wandbcode{sagemaker-hf} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Sagemaker & Weights & Biases\n",
    "\n",
    "This notebook will demonstrate how to:\n",
    "- log the datasets to W&B Tables for EDA\n",
    "- train on the [`banking77`](https://huggingface.co/datasets/banking77) dataset\n",
    "- train in distributed mode\n",
    "- log experiment results to Weights & Biases\n",
    "- log the validation predictions to W&B Tables for model evaluation\n",
    "- save the model weigths to W&B Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/Za9P1sr.png\" width=\"400\" alt=\"Weights & Biases\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker is a comprehensive machine learning service. It is a tool that helps data scientists and developers to prepare, build, train, and deploy high-quality machine learning (ML) models by providing a rich set of orchestration tools and features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq wandb --upgrade\n",
    "!pip install -qq \"sagemaker>=2.48.0\" \"transformers>=4.6.1\" \"datasets[s3]>=1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::618469898284:role/jeff-sagemaker\n",
      "sagemaker bucket: sagemaker-us-east-2-618469898284\n",
      "sagemaker session region: us-east-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to `sagemaker_session_bucket`\n",
    "\n",
    "After we processed the `datasets` we are going to use the new `FileSystem` [integration](https://huggingface.co/docs/datasets/filesystems.html) to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.4',\n",
    "                            pytorch_version='1.6',\n",
    "                            py_version='py36',\n",
    "                            role=role,\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "```\n",
    "\n",
    "The `hyperparameters` you define in the `HuggingFace` estimator are passed in as named arguments. \n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local_gpu'` for gpu usage. _Note: this does not working within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset banking77 (/home/ec2-user/.cache/huggingface/datasets/banking77/default/1.1.0/17ffc2ed47c2ed928bee64127ff1dbc97204cb974c2f980becae7c864007aed9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': 11, 'text': 'I am still waiting on my card?'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('banking77')\n",
    "\n",
    "dataset['train'][0]\n",
    "\n",
    "# dataset['train'].features['label'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ClassLabel' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bb339cea316c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'ClassLabel' object does not support indexing"
     ]
    }
   ],
   "source": [
    "dataset[\"train\"].features[\"label\"][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['activate_my_card',\n",
       " 'age_limit',\n",
       " 'apple_pay_or_google_pay',\n",
       " 'atm_support',\n",
       " 'automatic_top_up']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = dataset[\"train\"].features[\"label\"].names[:5]\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['activate_my_card',\n",
       " 'age_limit',\n",
       " 'apple_pay_or_google_pay',\n",
       " 'atm_support',\n",
       " 'automatic_top_up']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list.sort()  # Let's sort it for determinism\n",
    "# num_labels = len(label_list)\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = raw_datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb22ef3e84c249658bc131c2ddb6a08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'label', 'text'],\n",
       "    num_rows: 3080\n",
       "})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset = dataset['test']\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "eval_dataset = eval_dataset.map(lambda x: tokenizer(x['text'], truncation=True), batched=True)\n",
    "\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['card_arrival',\n",
       " 'card_arrival',\n",
       " 'card_arrival',\n",
       " 'card_arrival',\n",
       " 'card_arrival',\n",
       " 'card_arrival',\n",
       " 'card_arrival',\n",
       " 'card_arrival',\n",
       " 'card_arrival',\n",
       " 'card_arrival']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_targets = eval_dataset['label']\n",
    "# convert labels to their respective class\n",
    "validation_targets = [eval_dataset.features['label'].int2str(x) for x in validation_targets]\n",
    "validation_targets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('text',\n",
       "               ['How do I locate my card?',\n",
       "                'I still have not received my new card, I ordered over a week ago.',\n",
       "                'I ordered a card but it has not arrived. Help please!',\n",
       "                'Is there a way to know when my card will arrive?',\n",
       "                'My card has not arrived yet.',\n",
       "                'When will I get my card?',\n",
       "                'Do you know if there is a tracking number for the new card you sent me?',\n",
       "                'i have not received my card',\n",
       "                'still waiting on that card',\n",
       "                'Is it normal to have to wait over a week for my new card?'])]),\n",
       " ['card_arrival',\n",
       "  'card_arrival',\n",
       "  'card_arrival',\n",
       "  'card_arrival',\n",
       "  'card_arrival',\n",
       "  'card_arrival',\n",
       "  'card_arrival',\n",
       "  'card_arrival',\n",
       "  'card_arrival',\n",
       "  'card_arrival'])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_inputs = eval_dataset.remove_columns(['label', 'attention_mask', 'input_ids'])\n",
    "\n",
    "validation_targets = eval_dataset['label']\n",
    "# convert labels to their respective class\n",
    "validation_targets = [eval_dataset.features['label'].int2str(x) for x in validation_targets]\n",
    "\n",
    "from wandb.sdk.integration_utils.data_logging import ValidationDataLogger\n",
    "\n",
    "validation_logger = ValidationDataLogger(\n",
    "    inputs = validation_inputs[:],\n",
    "    targets = validation_targets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# from transformers import EvalPrediction\n",
    "# from typing import Callable\n",
    "\n",
    "# class ComputeMetrics:\n",
    "#     def __init__(self, train_len, eval_steps):\n",
    "#         self.train_len = train_len\n",
    "#         self.eval_steps = eval_steps\n",
    "#         self.eval_step_count = eval_steps\n",
    "        \n",
    "#     def __call__(self, p: EvalPrediction):\n",
    "#             preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "#             preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "\n",
    "#     #         # ✍️ log predictions to W&B Validation Logger ✍️ \n",
    "#     #         preds_labels = [model.config.id2label[x.item()] for x in preds] # convert id to class, (0, 1, 2…) to label (Health, Science…)\n",
    "#     #         validation_logger.log_predictions(preds_labels)\n",
    "\n",
    "#             # Create W&B Table\n",
    "#             validation_table = wandb.Table(columns=['id', 'step', 'pred_label_id'])\n",
    "#             for i,p in enumerate(preds):\n",
    "#                 idx = i + len(train_dataset)\n",
    "#                 row = [idx, self.eval_step_count, p]                    \n",
    "#                 validation_table.add_data(*row)\n",
    "\n",
    "#             # Log the table to Weights & Biases\n",
    "#             wandb.log(\n",
    "#                 {f'Eval Predictions/{data_args.dataset_name}_{self.eval_step_count}' : validation_table}, \n",
    "#                 commit=False\n",
    "#             )\n",
    "            \n",
    "#             self.eval_step_count+=self.eval_steps\n",
    "            \n",
    "#             return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
    "        \n",
    "# compute_metrics = ComputeMetrics(100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "wandb.sagemaker_auth(path=\"scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'output_dir': 'tmp',\n",
    "    'model_name_or_path': 'albert-large-v2' #'distilbert-base-uncased',  # microsoft/deberta-base , roberta-base\n",
    "    'dataset_name': 'banking77',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'per_device_train_batch_size': 64,\n",
    "    'per_device_eval_batch_size': 64,\n",
    "    'learning_rate': 1e-4,\n",
    "    'warmup_steps': 100,\n",
    "    'fp16': True,\n",
    "    'logging_steps': 10,\n",
    "    'max_steps': 1000,\n",
    "    'eval_steps': 100,\n",
    "    'save_steps': 200,\n",
    "    'evaluation_strategy' : 'steps',\n",
    "    'load_best_model_at_end': True,\n",
    "    'metric_for_best_model': 'accuracy',\n",
    "    'report_to': 'wandb',    # ✍️\n",
    "    'run_name': 'distilbert2'    # ✍️\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='run_text_classification.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type= 'ml.p3.2xlarge', #'ml.p3.8xlarge', #'g4dn.12xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.6',\n",
    "                            pytorch_version='1.7',\n",
    "                            py_version='py36',\n",
    "                            hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-11 20:55:42 Starting - Starting the training job...\n",
      "2021-08-11 20:56:05 Starting - Launching requested ML instancesProfilerReport-1628715342: InProgress\n",
      "......"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-11 22:51:43 Starting - Starting the training job...\n",
      "2021-08-11 22:52:05 Starting - Launching requested ML instancesProfilerReport-1628722302: InProgress\n",
      "......\n",
      "2021-08-11 22:53:06 Starting - Preparing the instances for training......\n",
      "2021-08-11 22:54:06 Downloading - Downloading input data...\n",
      "2021-08-11 22:54:26 Training - Downloading the training image...............\n",
      "2021-08-11 22:57:06 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-08-11 22:57:00,706 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-08-11 22:57:00,730 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-08-11 22:57:06,961 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-08-11 22:57:07,335 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/transformers.git (from -r requirements.txt (line 1))\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-stuzypym\u001b[0m\n",
      "\u001b[34m  Installing build dependencies: started\u001b[0m\n",
      "\u001b[34m  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34m  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting datasets>=1.11.0\u001b[0m\n",
      "\u001b[34m  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\u001b[0m\n",
      "\u001b[34mCollecting wandb>=0.11.2\n",
      "  Downloading wandb-0.12.0-py2.py3-none-any.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0->-r requirements.txt (line 1)) (0.0.45)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0->-r requirements.txt (line 1)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.0.12\n",
      "  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0->-r requirements.txt (line 1)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0->-r requirements.txt (line 1)) (2021.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0.dev0->-r requirements.txt (line 1)) (0.10.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.11.0->-r requirements.txt (line 2)) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow!=4.0.0,>=1.0.0\n",
      "  Downloading pyarrow-5.0.0-cp36-cp36m-manylinux2014_x86_64.whl (23.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.11.0->-r requirements.txt (line 2)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.11.0->-r requirements.txt (line 2)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.11.0->-r requirements.txt (line 2)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.11.0->-r requirements.txt (line 2)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mCollecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34mCollecting subprocess32>=3.5.3\n",
      "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.6/site-packages (from wandb>=0.11.2->-r requirements.txt (line 3)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.6/site-packages (from wandb>=0.11.2->-r requirements.txt (line 3)) (3.17.1)\u001b[0m\n",
      "\u001b[34mCollecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from wandb>=0.11.2->-r requirements.txt (line 3)) (2.8.1)\u001b[0m\n",
      "\u001b[34mCollecting configparser>=3.8.1\n",
      "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.6/site-packages (from wandb>=0.11.2->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb>=0.11.2->-r requirements.txt (line 3)) (5.8.0)\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.0 in /opt/conda/lib/python3.6/site-packages (from GitPython>=1.0.0->wandb>=0.11.2->-r requirements.txt (line 3)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34mCollecting smmap<5,>=3.0.1\n",
      "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.10.0.dev0->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0.dev0->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0.dev0->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0.dev0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0.dev0->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.10.0.dev0->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.11.0->-r requirements.txt (line 2)) (2021.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0.dev0->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: transformers, promise, subprocess32, pathtools\n",
      "  Building wheel for transformers (PEP 517): started\u001b[0m\n",
      "\u001b[34m  Building wheel for transformers (PEP 517): finished with status 'done'\n",
      "  Created wheel for transformers: filename=transformers-4.10.0.dev0-py3-none-any.whl size=2633849 sha256=8f5f17ce11eb667efe1f4de7e2589e4b3579e85e5b42fee0e7b8f3e3aebdb254\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-77rbcods/wheels/83/0d/25/e0bd4216d0278109b70f8a6ada5ea35eebbad3de134e416ba3\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=d2074e4ea845c77b26ac29f0147d24e9c18465e9ddc7d4443054806778d9e4d0\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "  Building wheel for subprocess32 (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for subprocess32 (setup.py): finished with status 'done'\n",
      "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6487 sha256=29be3a96d08c9b117ce9912da723d1c2722128ac4ac96f3ebf2d1122cf41b03d\n",
      "  Stored in directory: /root/.cache/pip/wheels/44/3a/ab/102386d84fe551b6cedb628ed1e74c5f5be76af8b909aeda09\n",
      "  Building wheel for pathtools (setup.py): started\n",
      "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8785 sha256=bc6dc4c81eccaafd6a04ee014bbaa5b23041f82c7ddf3fc89a4e5e2df655b94c\n",
      "  Stored in directory: /root/.cache/pip/wheels/42/ea/90/e37d463fb3b03848bf715080595de62545266f53dd546b2497\u001b[0m\n",
      "\u001b[34mSuccessfully built transformers promise subprocess32 pathtools\u001b[0m\n",
      "\u001b[34mInstalling collected packages: smmap, gitdb, subprocess32, shortuuid, sentry-sdk, pyarrow, promise, pathtools, huggingface-hub, GitPython, docker-pycreds, configparser, wandb, transformers, datasets\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 4.0.0\n",
      "    Uninstalling pyarrow-4.0.0:\n",
      "      Successfully uninstalled pyarrow-4.0.0\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.8\n",
      "    Uninstalling huggingface-hub-0.0.8:\n",
      "      Successfully uninstalled huggingface-hub-0.0.8\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.6.1\u001b[0m\n",
      "\u001b[34m    Uninstalling transformers-4.6.1:\n",
      "      Successfully uninstalled transformers-4.6.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 1.6.2\n",
      "    Uninstalling datasets-1.6.2:\n",
      "      Successfully uninstalled datasets-1.6.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed GitPython-3.1.18 configparser-5.0.2 datasets-1.11.0 docker-pycreds-0.4.0 gitdb-4.0.7 huggingface-hub-0.0.15 pathtools-0.1.2 promise-2.3 pyarrow-5.0.0 sentry-sdk-1.3.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 transformers-4.10.0.dev0 wandb-0.12.0\u001b[0m\n",
      "\u001b[34m  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-stuzypym\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "\u001b[34m2021-08-11 22:57:30,773 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"evaluation_strategy\": \"steps\",\n",
      "        \"per_device_eval_batch_size\": 8,\n",
      "        \"load_best_model_at_end\": true,\n",
      "        \"max_steps\": 10,\n",
      "        \"metric_for_best_model\": \"accuracy\",\n",
      "        \"do_train\": true,\n",
      "        \"dataset_name\": \"banking77\",\n",
      "        \"gradient_accumulation_steps\": 8,\n",
      "        \"warmup_steps\": 100,\n",
      "        \"report_to\": \"wandb\",\n",
      "        \"save_steps\": 300,\n",
      "        \"output_dir\": \"tmp\",\n",
      "        \"eval_steps\": 100,\n",
      "        \"per_device_train_batch_size\": 8,\n",
      "        \"run_name\": \"roberta-large\",\n",
      "        \"learning_rate\": 0.0001,\n",
      "        \"logging_steps\": 10,\n",
      "        \"model_name_or_path\": \"roberta-large\",\n",
      "        \"fp16\": true\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2021-08-11-22-51-42-838\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-618469898284/huggingface-pytorch-training-2021-08-11-22-51-42-838/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_text_classification\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_text_classification.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dataset_name\":\"banking77\",\"do_train\":true,\"eval_steps\":100,\"evaluation_strategy\":\"steps\",\"fp16\":true,\"gradient_accumulation_steps\":8,\"learning_rate\":0.0001,\"load_best_model_at_end\":true,\"logging_steps\":10,\"max_steps\":10,\"metric_for_best_model\":\"accuracy\",\"model_name_or_path\":\"roberta-large\",\"output_dir\":\"tmp\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":8,\"report_to\":\"wandb\",\"run_name\":\"roberta-large\",\"save_steps\":300,\"warmup_steps\":100}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_text_classification.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_text_classification\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-618469898284/huggingface-pytorch-training-2021-08-11-22-51-42-838/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset_name\":\"banking77\",\"do_train\":true,\"eval_steps\":100,\"evaluation_strategy\":\"steps\",\"fp16\":true,\"gradient_accumulation_steps\":8,\"learning_rate\":0.0001,\"load_best_model_at_end\":true,\"logging_steps\":10,\"max_steps\":10,\"metric_for_best_model\":\"accuracy\",\"model_name_or_path\":\"roberta-large\",\"output_dir\":\"tmp\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":8,\"report_to\":\"wandb\",\"run_name\":\"roberta-large\",\"save_steps\":300,\"warmup_steps\":100},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2021-08-11-22-51-42-838\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-618469898284/huggingface-pytorch-training-2021-08-11-22-51-42-838/source/sourcedir.tar.gz\",\"module_name\":\"run_text_classification\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_text_classification.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dataset_name\",\"banking77\",\"--do_train\",\"True\",\"--eval_steps\",\"100\",\"--evaluation_strategy\",\"steps\",\"--fp16\",\"True\",\"--gradient_accumulation_steps\",\"8\",\"--learning_rate\",\"0.0001\",\"--load_best_model_at_end\",\"True\",\"--logging_steps\",\"10\",\"--max_steps\",\"10\",\"--metric_for_best_model\",\"accuracy\",\"--model_name_or_path\",\"roberta-large\",\"--output_dir\",\"tmp\",\"--per_device_eval_batch_size\",\"8\",\"--per_device_train_batch_size\",\"8\",\"--report_to\",\"wandb\",\"--run_name\",\"roberta-large\",\"--save_steps\",\"300\",\"--warmup_steps\",\"100\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_LOAD_BEST_MODEL_AT_END=true\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_METRIC_FOR_BEST_MODEL=accuracy\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_NAME=banking77\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=8\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_STEPS=100\u001b[0m\n",
      "\u001b[34mSM_HP_REPORT_TO=wandb\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STEPS=300\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=tmp\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_STEPS=100\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_RUN_NAME=roberta-large\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=roberta-large\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 run_text_classification.py --dataset_name banking77 --do_train True --eval_steps 100 --evaluation_strategy steps --fp16 True --gradient_accumulation_steps 8 --learning_rate 0.0001 --load_best_model_at_end True --logging_steps 10 --max_steps 10 --metric_for_best_model accuracy --model_name_or_path roberta-large --output_dir tmp --per_device_eval_batch_size 8 --per_device_train_batch_size 8 --report_to wandb --run_name roberta-large --save_steps 300 --warmup_steps 100\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:37 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:37 - INFO - __main__ - Training/evaluation parameters TrainingArguments(#015\u001b[0m\n",
      "\u001b[34m_n_gpu=1,#015\u001b[0m\n",
      "\u001b[34madafactor=False,#015\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,#015\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,#015\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,#015\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,#015\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,#015\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,#015\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,#015\u001b[0m\n",
      "\u001b[34mdebug=[],#015\u001b[0m\n",
      "\u001b[34mdeepspeed=None,#015\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,#015\u001b[0m\n",
      "\u001b[34mdo_eval=True,#015\u001b[0m\n",
      "\u001b[34mdo_predict=False,#015\u001b[0m\n",
      "\u001b[34mdo_train=True,#015\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,#015\u001b[0m\n",
      "\u001b[34meval_steps=100,#015\u001b[0m\n",
      "\u001b[34mevaluation_strategy=IntervalStrategy.STEPS,#015\u001b[0m\n",
      "\u001b[34mfp16=True,#015\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,#015\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,#015\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,#015\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=8,#015\u001b[0m\n",
      "\u001b[34mgreater_is_better=True,#015\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,#015\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,#015\u001b[0m\n",
      "\u001b[34mlabel_names=None,#015\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,#015\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0001,#015\u001b[0m\n",
      "\u001b[34mlength_column_name=length,#015\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=True,#015\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,#015\u001b[0m\n",
      "\u001b[34mlog_level=-1,#015\u001b[0m\n",
      "\u001b[34mlog_level_replica=-1,#015\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,#015\u001b[0m\n",
      "\u001b[34mlogging_dir=tmp/runs/Aug11_22-57-35_algo-1,#015\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,#015\u001b[0m\n",
      "\u001b[34mlogging_steps=10,#015\u001b[0m\n",
      "\u001b[34mlogging_strategy=IntervalStrategy.STEPS,#015\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=SchedulerType.LINEAR,#015\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,#015\u001b[0m\n",
      "\u001b[34mmax_steps=10,#015\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=accuracy,#015\u001b[0m\n",
      "\u001b[34mmp_parameters=,#015\u001b[0m\n",
      "\u001b[34mno_cuda=False,#015\u001b[0m\n",
      "\u001b[34mnum_train_epochs=3.0,#015\u001b[0m\n",
      "\u001b[34moutput_dir=tmp,#015\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,#015\u001b[0m\n",
      "\u001b[34mpast_index=-1,#015\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,#015\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=8,#015\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,#015\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,#015\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=tmp,#015\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,#015\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=None,#015\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,#015\u001b[0m\n",
      "\u001b[34mreport_to=['wandb'],#015\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,#015\u001b[0m\n",
      "\u001b[34mrun_name=roberta-large,#015\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,#015\u001b[0m\n",
      "\u001b[34msave_steps=300,#015\u001b[0m\n",
      "\u001b[34msave_strategy=IntervalStrategy.STEPS,#015\u001b[0m\n",
      "\u001b[34msave_total_limit=None,#015\u001b[0m\n",
      "\u001b[34mseed=42,#015\u001b[0m\n",
      "\u001b[34msharded_ddp=[],#015\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,#015\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,#015\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,#015\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,#015\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,#015\u001b[0m\n",
      "\u001b[34mwarmup_steps=100,#015\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,#015\u001b[0m\n",
      "\u001b[34m)#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/banking77/banking77.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpjmlymkqo#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/banking77/banking77.py in cache at /root/.cache/huggingface/datasets/downloads/ffc5f603d9b0dd57ecd43bf1bd5a6c1137e5b97a9f4cb0208b889c6eb880e518.6188529f4dffa93c0487be7e9b8d88a648e8eff6512e10cf8ae3a7ec01a82b9a.py#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/ffc5f603d9b0dd57ecd43bf1bd5a6c1137e5b97a9f4cb0208b889c6eb880e518.6188529f4dffa93c0487be7e9b8d88a648e8eff6512e10cf8ae3a7ec01a82b9a.py#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/banking77/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpo50d7s5j#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/banking77/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/fa37c8b1b9fdf0554c535caf1e7480906b9110646e6d369182f198d03e7c34db.0bdae177fb52017f87dbdaf681759f2369b91b1e00d0392616cabf4a5f6658ca#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/fa37c8b1b9fdf0554c535caf1e7480906b9110646e6d369182f198d03e7c34db.0bdae177fb52017f87dbdaf681759f2369b91b1e00d0392616cabf4a5f6658ca#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.load - Creating main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/banking77/banking77.py at /root/.cache/huggingface/modules/datasets_modules/datasets/banking77#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.load - Creating specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/banking77/banking77.py at /root/.cache/huggingface/modules/datasets_modules/datasets/banking77/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/banking77/banking77.py to /root/.cache/huggingface/modules/datasets_modules/datasets/banking77/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b/banking77.py#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.load - Copying dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/banking77/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/banking77/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b/dataset_infos.json#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.load - Creating metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.11.0/datasets/banking77/banking77.py at /root/.cache/huggingface/modules/datasets_modules/datasets/banking77/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b/banking77.json#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - WARNING - datasets.builder - Using custom data configuration default#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/banking77/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.builder - Generating dataset banking77 (/root/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b)#015\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset banking77/default (download: 1.03 MiB, generated: 897.51 KiB, post-processed: Unknown size, total: 1.91 MiB) to /root/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b...#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/PolyAI-LDN/task-specific-datasets/master/banking_data/train.csv not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpyy6lgzxp#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/PolyAI-LDN/task-specific-datasets/master/banking_data/train.csv in cache at /root/.cache/huggingface/datasets/downloads/bc5a3d80c133c109ecaffda500a00cd8dc8af9bf734036bff41520f5ffdbc23b#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/bc5a3d80c133c109ecaffda500a00cd8dc8af9bf734036bff41520f5ffdbc23b#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.utils.download_manager - Downloading took 0.0 min#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/PolyAI-LDN/task-specific-datasets/master/banking_data/test.csv not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmphnlbsoas#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/PolyAI-LDN/task-specific-datasets/master/banking_data/test.csv in cache at /root/.cache/huggingface/datasets/downloads/4d2945a4325688f0222a55e1f815b923080633734cc4b1606318611338ff5043#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/4d2945a4325688f0222a55e1f815b923080633734cc4b1606318611338ff5043#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.utils.download_manager - Downloading took 0.0 min#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:38 - INFO - datasets.builder - Generating split train#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:39 - INFO - datasets.builder - Generating split test#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:57:39 - INFO - datasets.utils.info_utils - All the splits matched successfully.#015\u001b[0m\n",
      "\u001b[34mDataset banking77 downloaded and prepared to /root/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b. Subsequent calls will reuse this data.#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-11 22:57:39,593 >> https://huggingface.co/roberta-large/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi4jk69qy#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-11 22:57:39,654 >> storing https://huggingface.co/roberta-large/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-11 22:57:39,654 >> creating metadata file for /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:545] 2021-08-11 22:57:39,655 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:581] 2021-08-11 22:57:39,657 >> Model config RobertaConfig {#015\n",
      "  \"architectures\": [#015\n",
      "    \"RobertaForMaskedLM\"#015\n",
      "  ],#015\n",
      "  \"attention_probs_dropout_prob\": 0.1,#015\n",
      "  \"bos_token_id\": 0,#015\n",
      "  \"classifier_dropout\": null,#015\n",
      "  \"eos_token_id\": 2,#015\n",
      "  \"gradient_checkpointing\": false,#015\n",
      "  \"hidden_act\": \"gelu\",#015\n",
      "  \"hidden_dropout_prob\": 0.1,#015\n",
      "  \"hidden_size\": 1024,#015\n",
      "  \"id2label\": {#015\n",
      "    \"0\": \"LABEL_0\",#015\n",
      "    \"1\": \"LABEL_1\",#015\n",
      "    \"2\": \"LABEL_2\",#015\n",
      "    \"3\": \"LABEL_3\",#015\n",
      "    \"4\": \"LABEL_4\",#015\n",
      "    \"5\": \"LABEL_5\",#015\n",
      "    \"6\": \"LABEL_6\",#015\n",
      "    \"7\": \"LABEL_7\",#015\n",
      "    \"8\": \"LABEL_8\",#015\n",
      "    \"9\": \"LABEL_9\",#015\n",
      "    \"10\": \"LABEL_10\",#015\n",
      "    \"11\": \"LABEL_11\",#015\n",
      "    \"12\": \"LABEL_12\",#015\n",
      "    \"13\": \"LABEL_13\",#015\n",
      "    \"14\": \"LABEL_14\",#015\n",
      "    \"15\": \"LABEL_15\",#015\n",
      "    \"16\": \"LABEL_16\",#015\n",
      "    \"17\": \"LABEL_17\",#015\n",
      "    \"18\": \"LABEL_18\",#015\n",
      "    \"19\": \"LABEL_19\",#015\n",
      "    \"20\": \"LABEL_20\",#015\n",
      "    \"21\": \"LABEL_21\",#015\n",
      "    \"22\": \"LABEL_22\",#015\n",
      "    \"23\": \"LABEL_23\",#015\n",
      "    \"24\": \"LABEL_24\",#015\n",
      "    \"25\": \"LABEL_25\",#015\n",
      "    \"26\": \"LABEL_26\",#015\n",
      "    \"27\": \"LABEL_27\",#015\n",
      "    \"28\": \"LABEL_28\",#015\n",
      "    \"29\": \"LABEL_29\",#015\n",
      "    \"30\": \"LABEL_30\",#015\n",
      "    \"31\": \"LABEL_31\",#015\n",
      "    \"32\": \"LABEL_32\",#015\n",
      "    \"33\": \"LABEL_33\",#015\n",
      "    \"34\": \"LABEL_34\",#015\n",
      "    \"35\": \"LABEL_35\",#015\n",
      "    \"36\": \"LABEL_36\",#015\n",
      "    \"37\": \"LABEL_37\",#015\n",
      "    \"38\": \"LABEL_38\",#015\n",
      "    \"39\": \"LABEL_39\",#015\n",
      "    \"40\": \"LABEL_40\",#015\n",
      "    \"41\": \"LABEL_41\",#015\n",
      "    \"42\": \"LABEL_42\",#015\n",
      "    \"43\": \"LABEL_43\",#015\n",
      "    \"44\": \"LABEL_44\",#015\n",
      "    \"45\": \"LABEL_45\",#015\n",
      "    \"46\": \"LABEL_46\",#015\n",
      "    \"47\": \"LABEL_47\",#015\n",
      "    \"48\": \"LABEL_48\",#015\n",
      "    \"49\": \"LABEL_49\",#015\n",
      "    \"50\": \"LABEL_50\",#015\n",
      "    \"51\": \"LABEL_51\",#015\n",
      "    \"52\": \"LABEL_52\",#015\n",
      "    \"53\": \"LABEL_53\",#015\n",
      "    \"54\": \"LABEL_54\",#015\n",
      "    \"55\": \"LABEL_55\",#015\n",
      "    \"56\": \"LABEL_56\",#015\n",
      "    \"57\": \"LABEL_57\",#015\n",
      "    \"58\": \"LABEL_58\",#015\n",
      "    \"59\": \"LABEL_59\",#015\n",
      "    \"60\": \"LABEL_60\",#015\n",
      "    \"61\": \"LABEL_61\",#015\n",
      "    \"62\": \"LABEL_62\",#015\n",
      "    \"63\": \"LABEL_63\",#015\n",
      "    \"64\": \"LABEL_64\",#015\n",
      "    \"65\": \"LABEL_65\",#015\n",
      "    \"66\": \"LABEL_66\",#015\n",
      "    \"67\": \"LABEL_67\",#015\n",
      "    \"68\": \"LABEL_68\",#015\n",
      "    \"69\": \"LABEL_69\",#015\n",
      "    \"70\": \"LABEL_70\",#015\n",
      "    \"71\": \"LABEL_71\",#015\n",
      "    \"72\": \"LABEL_72\",#015\n",
      "    \"73\": \"LABEL_73\",#015\n",
      "    \"74\": \"LABEL_74\",#015\n",
      "    \"75\": \"LABEL_75\",#015\n",
      "    \"76\": \"LABEL_76\"#015\n",
      "  },#015\n",
      "  \"initializer_range\": 0.02,#015\n",
      "  \"intermediate_size\": 4096,#015\n",
      "  \"label2id\": {#015\n",
      "    \"LABEL_0\": 0,#015\n",
      "    \"LABEL_1\": 1,#015\n",
      "    \"LABEL_10\": 10,#015\n",
      "    \"LABEL_11\": 11,#015\n",
      "    \"LABEL_12\": 12,#015\n",
      "    \"LABEL_13\": 13,#015\n",
      "    \"LABEL_14\": 14,#015\n",
      "    \"LABEL_15\": 15,#015\n",
      "    \"LABEL_16\": 16,#015\n",
      "    \"LABEL_17\": 17,#015\n",
      "    \"LABEL_18\": 18,#015\n",
      "    \"LABEL_19\": 19,#015\n",
      "    \"LABEL_2\": 2,#015\n",
      "    \"LABEL_20\": 20,#015\n",
      "    \"LABEL_21\": 21,#015\n",
      "    \"LABEL_22\": 22,#015\n",
      "    \"LABEL_23\": 23,#015\n",
      "    \"LABEL_24\": 24,#015\n",
      "    \"LABEL_25\": 25,#015\n",
      "    \"LABEL_26\": 26,#015\n",
      "    \"LABEL_27\": 27,#015\n",
      "    \"LABEL_28\": 28,#015\n",
      "    \"LABEL_29\": 29,#015\n",
      "    \"LABEL_3\": 3,#015\n",
      "    \"LABEL_30\": 30,#015\n",
      "    \"LABEL_31\": 31,#015\n",
      "    \"LABEL_32\": 32,#015\n",
      "    \"LABEL_33\": 33,#015\n",
      "    \"LABEL_34\": 34,#015\n",
      "    \"LABEL_35\": 35,#015\n",
      "    \"LABEL_36\": 36,#015\n",
      "    \"LABEL_37\": 37,#015\n",
      "    \"LABEL_38\": 38,#015\n",
      "    \"LABEL_39\": 39,#015\n",
      "    \"LABEL_4\": 4,#015\n",
      "    \"LABEL_40\": 40,#015\n",
      "    \"LABEL_41\": 41,#015\n",
      "    \"LABEL_42\": 42,#015\n",
      "    \"LABEL_43\": 43,#015\n",
      "    \"LABEL_44\": 44,#015\n",
      "    \"LABEL_45\": 45,#015\n",
      "    \"LABEL_46\": 46,#015\n",
      "    \"LABEL_47\": 47,#015\n",
      "    \"LABEL_48\": 48,#015\n",
      "    \"LABEL_49\": 49,#015\n",
      "    \"LABEL_5\": 5,#015\n",
      "    \"LABEL_50\": 50,#015\n",
      "    \"LABEL_51\": 51,#015\n",
      "    \"LABEL_52\": 52,#015\n",
      "    \"LABEL_53\": 53,#015\n",
      "    \"LABEL_54\": 54,#015\n",
      "    \"LABEL_55\": 55,#015\n",
      "    \"LABEL_56\": 56,#015\n",
      "    \"LABEL_57\": 57,#015\n",
      "    \"LABEL_58\": 58,#015\n",
      "    \"LABEL_59\": 59,#015\n",
      "    \"LABEL_6\": 6,#015\n",
      "    \"LABEL_60\": 60,#015\n",
      "    \"LABEL_61\": 61,#015\n",
      "    \"LABEL_62\": 62,#015\n",
      "    \"LABEL_63\": 63,#015\n",
      "    \"LABEL_64\": 64,#015\n",
      "    \"LABEL_65\": 65,#015\n",
      "    \"LABEL_66\": 66,#015\n",
      "    \"LABEL_67\": 67,#015\n",
      "    \"LABEL_68\": 68,#015\n",
      "    \"LABEL_69\": 69,#015\n",
      "    \"LABEL_7\": 7,#015\n",
      "    \"LABEL_70\": 70,#015\n",
      "    \"LABEL_71\": 71,#015\n",
      "    \"LABEL_72\": 72,#015\n",
      "    \"LABEL_73\": 73,#015\n",
      "    \"LABEL_74\": 74,#015\n",
      "    \"LABEL_75\": 75,#015\n",
      "    \"LABEL_76\": 76,#015\n",
      "    \"LABEL_8\": 8,#015\n",
      "    \"LABEL_9\": 9#015\n",
      "  },#015\n",
      "  \"layer_norm_eps\": 1e-05,#015\n",
      "  \"max_position_embeddings\": 514,#015\n",
      "  \"model_type\": \"roberta\",#015\n",
      "  \"num_attention_heads\": 16,#015\n",
      "  \"num_hidden_layers\": 24,#015\n",
      "  \"pad_token_id\": 1,#015\n",
      "  \"position_embedding_type\": \"absolute\",#015\n",
      "  \"transformers_version\": \"4.10.0.dev0\",#015\n",
      "  \"type_vocab_size\": 1,#015\n",
      "  \"use_cache\": true,#015\n",
      "  \"vocab_size\": 50265#015\u001b[0m\n",
      "\u001b[34m}#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_auto.py:303] 2021-08-11 22:57:39,720 >> Could not locate the tokenizer configuration file, will try to use the model config instead.#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:545] 2021-08-11 22:57:39,788 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:581] 2021-08-11 22:57:39,788 >> Model config RobertaConfig {#015\n",
      "  \"architectures\": [#015\n",
      "    \"RobertaForMaskedLM\"#015\n",
      "  ],#015\n",
      "  \"attention_probs_dropout_prob\": 0.1,#015\n",
      "  \"bos_token_id\": 0,#015\n",
      "  \"classifier_dropout\": null,#015\n",
      "  \"eos_token_id\": 2,#015\n",
      "  \"gradient_checkpointing\": false,#015\n",
      "  \"hidden_act\": \"gelu\",#015\n",
      "  \"hidden_dropout_prob\": 0.1,#015\n",
      "  \"hidden_size\": 1024,#015\n",
      "  \"initializer_range\": 0.02,#015\n",
      "  \"intermediate_size\": 4096,#015\n",
      "  \"layer_norm_eps\": 1e-05,#015\n",
      "  \"max_position_embeddings\": 514,#015\n",
      "  \"model_type\": \"roberta\",#015\n",
      "  \"num_attention_heads\": 16,#015\n",
      "  \"num_hidden_layers\": 24,#015\n",
      "  \"pad_token_id\": 1,#015\n",
      "  \"position_embedding_type\": \"absolute\",#015\n",
      "  \"transformers_version\": \"4.10.0.dev0\",#015\n",
      "  \"type_vocab_size\": 1,#015\n",
      "  \"use_cache\": true,#015\n",
      "  \"vocab_size\": 50265#015\u001b[0m\n",
      "\u001b[34m}#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-11 22:57:39,970 >> https://huggingface.co/roberta-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi4xes0c0#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-11 22:57:40,084 >> storing https://huggingface.co/roberta-large/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-11 22:57:40,084 >> creating metadata file for /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-11 22:57:40,147 >> https://huggingface.co/roberta-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp80iyi2gm#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-11 22:57:40,247 >> storing https://huggingface.co/roberta-large/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-11 22:57:40,248 >> creating metadata file for /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-11 22:57:40,309 >> https://huggingface.co/roberta-large/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0qw81zm0#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-11 22:57:40,437 >> storing https://huggingface.co/roberta-large/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-11 22:57:40,437 >> creating metadata file for /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-11 22:57:40,632 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-11 22:57:40,633 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-11 22:57:40,633 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-11 22:57:40,633 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-11 22:57:40,633 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-11 22:57:40,633 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:545] 2021-08-11 22:57:40,696 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:581] 2021-08-11 22:57:40,697 >> Model config RobertaConfig {#015\n",
      "  \"architectures\": [#015\n",
      "    \"RobertaForMaskedLM\"#015\n",
      "  ],#015\n",
      "  \"attention_probs_dropout_prob\": 0.1,#015\n",
      "  \"bos_token_id\": 0,#015\n",
      "  \"classifier_dropout\": null,#015\n",
      "  \"eos_token_id\": 2,#015\n",
      "  \"gradient_checkpointing\": false,#015\n",
      "  \"hidden_act\": \"gelu\",#015\n",
      "  \"hidden_dropout_prob\": 0.1,#015\n",
      "  \"hidden_size\": 1024,#015\n",
      "  \"initializer_range\": 0.02,#015\n",
      "  \"intermediate_size\": 4096,#015\n",
      "  \"layer_norm_eps\": 1e-05,#015\n",
      "  \"max_position_embeddings\": 514,#015\n",
      "  \"model_type\": \"roberta\",#015\n",
      "  \"num_attention_heads\": 16,#015\n",
      "  \"num_hidden_layers\": 24,#015\n",
      "  \"pad_token_id\": 1,#015\n",
      "  \"position_embedding_type\": \"absolute\",#015\n",
      "  \"transformers_version\": \"4.10.0.dev0\",#015\n",
      "  \"type_vocab_size\": 1,#015\n",
      "  \"use_cache\": true,#015\n",
      "  \"vocab_size\": 50265#015\u001b[0m\n",
      "\u001b[34m}#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-11 22:57:40,883 >> https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi1zwag5g#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-11 22:58:43,684 >> storing https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-11 22:58:43,684 >> creating metadata file for /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352#015\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1275] 2021-08-11 22:58:43,684 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352#015\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1512] 2021-08-11 22:58:52,085 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']#015\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).#015\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).#015\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1523] 2021-08-11 22:58:52,085 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']#015\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:58:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b/cache-21269e06ebce8879.arrow#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:58:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b/cache-5db99d259a5be841.arrow#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:58:59 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpqudf8a_6#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:59:00 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py in cache at /root/.cache/huggingface/datasets/downloads/591d385c0492ec80bad3323a081d7b1126c681a389acd9759f0a3098d2034ceb.1d0bd855d8e3c9eb14daa00edb910a511da1d653aeca8e7bc5dffb23a84d7b54.py#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:59:00 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/591d385c0492ec80bad3323a081d7b1126c681a389acd9759f0a3098d2034ceb.1d0bd855d8e3c9eb14daa00edb910a511da1d653aeca8e7bc5dffb23a84d7b54.py#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:59:00 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:59:00 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:59:00 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py to /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b/accuracy.py#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:59:00 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/dataset_infos.json#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:59:00 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b/accuracy.json#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:404] 2021-08-11 22:59:04,174 >> max_steps is given, it will override any value given in num_train_epochs#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:421] 2021-08-11 22:59:04,175 >> Using amp fp16 backend#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-11 22:59:04,176 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1171] 2021-08-11 22:59:04,204 >> ***** Running training *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1172] 2021-08-11 22:59:04,204 >>   Num examples = 10003#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1173] 2021-08-11 22:59:04,205 >>   Num Epochs = 1#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1174] 2021-08-11 22:59:04,205 >>   Instantaneous batch size per device = 8#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1175] 2021-08-11 22:59:04,205 >>   Total train batch size (w. parallel, distributed & accumulation) = 64#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1176] 2021-08-11 22:59:04,205 >>   Gradient Accumulation steps = 8#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1177] 2021-08-11 22:59:04,205 >>   Total optimization steps = 10#015\u001b[0m\n",
      "\u001b[34m[INFO|integrations.py:447] 2021-08-11 22:59:04,211 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.439 algo-1:75 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.502 algo-1:75 INFO profiler_config_parser.py:102] User has disabled profiler.#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.502 algo-1:75 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.503 algo-1:75 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.504 algo-1:75 INFO hook.py:255] Saving to /opt/ml/output/tensors#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.504 algo-1:75 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.676 algo-1:75 INFO hook.py:591] name:roberta.embeddings.word_embeddings.weight count_params:51471360#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.676 algo-1:75 INFO hook.py:591] name:roberta.embeddings.position_embeddings.weight count_params:526336#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.677 algo-1:75 INFO hook.py:591] name:roberta.embeddings.token_type_embeddings.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.677 algo-1:75 INFO hook.py:591] name:roberta.embeddings.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.677 algo-1:75 INFO hook.py:591] name:roberta.embeddings.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.677 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.0.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.677 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.0.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.677 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.0.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.677 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.0.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.677 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.0.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.677 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.0.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.677 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.0.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.677 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.0.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.678 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.678 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.678 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.0.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.678 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.0.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.678 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.0.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.678 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.0.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.678 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.0.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.678 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.0.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.678 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.1.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.678 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.1.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.678 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.1.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.678 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.1.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.678 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.1.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.679 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.1.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.679 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.1.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.679 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.1.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.679 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.679 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.679 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.1.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.679 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.1.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.679 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.1.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.684 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.1.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.695 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.1.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.695 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.1.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.695 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.2.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.695 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.2.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.711 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.2.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.711 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.2.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.713 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.2.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.713 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.2.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.713 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.2.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.713 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.2.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.713 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.713 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.714 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.2.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.714 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.2.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.714 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.2.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.714 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.2.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.714 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.2.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.714 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.2.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.714 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.3.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.714 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.3.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.714 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.3.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.714 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.3.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.714 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.3.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.714 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.3.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.715 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.3.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.715 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.3.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.715 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.715 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.715 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.3.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.715 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.3.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.715 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.3.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.716 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.3.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.716 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.3.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.716 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.3.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.716 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.4.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.716 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.4.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.716 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.4.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.716 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.4.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.716 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.4.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.716 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.4.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.716 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.4.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.716 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.4.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.716 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.716 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.717 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.4.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.717 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.4.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.717 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.4.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.717 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.4.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.717 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.4.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.717 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.4.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.717 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.5.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.717 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.5.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.717 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.5.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.717 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.5.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.717 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.5.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.717 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.5.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.717 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.5.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.718 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.5.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.718 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.718 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.718 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.5.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.718 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.5.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.718 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.5.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.718 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.5.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.718 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.5.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.718 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.5.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.718 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.6.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.718 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.6.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.718 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.6.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.718 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.6.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.719 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.6.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.719 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.6.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.719 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.6.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.719 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.6.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.719 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.719 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.719 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.6.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.719 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.6.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.719 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.6.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.719 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.6.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.719 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.6.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.719 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.6.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.719 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.7.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.720 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.7.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.720 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.7.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.720 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.7.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.720 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.7.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.720 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.7.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.720 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.7.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.720 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.7.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.720 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.720 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.720 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.7.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.720 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.7.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.720 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.7.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.720 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.7.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.721 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.7.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.721 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.7.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.721 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.8.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.721 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.8.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.721 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.8.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.721 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.8.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.721 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.8.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.721 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.8.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.721 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.8.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.721 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.8.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.721 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.721 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.721 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.8.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.722 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.8.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.722 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.8.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.722 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.8.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.722 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.8.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.722 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.8.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.722 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.9.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.722 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.9.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.722 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.9.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.722 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.9.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.723 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.9.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.723 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.9.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.723 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.9.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.723 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.9.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.723 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.724 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.724 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.9.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.724 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.9.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.724 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.9.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.724 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.9.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.724 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.9.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.724 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.9.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.725 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.10.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.725 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.10.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.725 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.10.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.725 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.10.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.725 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.10.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.725 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.10.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.725 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.10.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.725 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.10.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.725 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.725 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.725 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.10.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.725 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.10.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.725 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.10.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.726 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.10.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.726 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.10.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.726 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.10.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.726 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.11.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.726 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.11.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.726 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.11.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.726 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.11.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.726 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.11.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.726 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.11.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.726 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.11.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.726 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.11.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.726 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.726 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.727 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.11.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.727 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.11.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.727 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.11.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.727 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.11.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.727 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.11.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.727 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.11.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.727 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.12.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.727 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.12.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.727 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.12.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.727 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.12.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.727 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.12.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.727 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.12.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.727 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.12.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.728 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.12.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.728 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.728 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.728 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.12.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.728 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.12.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.728 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.12.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.728 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.12.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.728 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.12.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.728 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.12.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.728 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.13.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.728 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.13.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.728 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.13.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.728 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.13.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.729 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.13.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.729 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.13.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.729 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.13.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.729 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.13.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.729 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.729 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.729 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.13.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.729 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.13.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.729 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.13.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.729 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.13.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.729 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.13.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.730 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.13.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.730 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.14.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.730 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.14.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.730 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.14.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.730 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.14.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.730 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.14.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.730 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.14.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.730 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.14.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.730 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.14.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.730 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.730 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.731 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.14.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.731 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.14.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.731 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.14.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.731 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.14.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.731 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.14.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.731 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.14.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.731 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.15.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.731 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.15.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.731 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.15.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.731 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.15.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.732 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.15.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.732 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.15.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.732 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.15.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.732 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.15.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.732 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.732 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.732 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.15.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.732 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.15.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.732 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.15.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.732 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.15.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.732 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.15.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.732 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.15.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.732 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.16.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.733 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.16.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.733 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.16.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.733 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.16.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.733 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.16.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.733 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.16.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.733 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.16.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.733 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.16.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.733 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.733 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.733 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.16.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.733 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.16.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.733 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.16.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.733 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.16.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.734 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.16.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.734 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.16.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.734 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.17.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.734 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.17.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.734 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.17.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.734 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.17.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.734 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.17.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.734 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.17.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.734 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.17.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.734 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.17.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.734 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.734 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.735 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.17.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.735 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.17.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.735 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.17.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.735 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.17.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.735 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.17.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.736 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.17.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.736 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.18.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.736 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.18.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.736 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.18.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.736 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.18.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.736 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.18.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.737 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.18.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.737 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.18.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.737 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.18.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.737 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.737 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.737 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.18.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.737 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.18.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.737 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.18.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.737 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.18.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.737 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.18.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.737 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.18.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.737 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.19.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.738 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.19.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.738 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.19.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.738 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.19.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.738 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.19.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.738 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.19.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.738 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.19.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.738 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.19.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.738 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.738 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.738 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.19.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.738 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.19.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.738 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.19.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.739 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.19.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.739 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.19.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.739 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.19.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.739 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.20.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.739 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.20.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.739 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.20.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.739 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.20.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.739 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.20.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.739 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.20.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.739 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.20.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.739 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.20.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.739 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.739 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.740 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.20.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.740 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.20.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.740 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.20.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.740 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.20.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.740 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.20.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.740 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.20.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.740 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.21.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.740 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.21.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.740 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.21.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.740 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.21.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.740 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.21.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.740 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.21.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.740 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.21.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.741 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.21.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.741 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.741 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.741 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.21.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.741 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.21.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.741 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.21.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.741 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.21.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.741 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.21.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.741 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.21.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.741 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.22.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.741 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.22.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.741 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.22.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.741 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.22.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.742 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.22.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.742 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.22.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.742 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.22.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.742 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.22.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.742 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.742 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.742 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.22.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.742 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.22.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.742 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.22.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.742 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.22.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.742 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.22.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.742 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.22.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.743 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.23.attention.self.query.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.743 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.23.attention.self.query.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.743 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.23.attention.self.key.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.743 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.23.attention.self.key.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.743 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.23.attention.self.value.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.744 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.23.attention.self.value.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.744 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.23.attention.output.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.744 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.23.attention.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.744 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.744 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.744 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.23.intermediate.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.744 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.23.intermediate.dense.bias count_params:4096#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.744 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.23.output.dense.weight count_params:4194304#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.745 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.23.output.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.745 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.23.output.LayerNorm.weight count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.745 algo-1:75 INFO hook.py:591] name:roberta.encoder.layer.23.output.LayerNorm.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.745 algo-1:75 INFO hook.py:591] name:classifier.dense.weight count_params:1048576#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.745 algo-1:75 INFO hook.py:591] name:classifier.dense.bias count_params:1024#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.745 algo-1:75 INFO hook.py:591] name:classifier.out_proj.weight count_params:78848#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.745 algo-1:75 INFO hook.py:591] name:classifier.out_proj.bias count_params:77#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.745 algo-1:75 INFO hook.py:593] Total Trainable Params: 355438669#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.745 algo-1:75 INFO hook.py:425] Monitoring the collections: losses#015\u001b[0m\n",
      "\u001b[34m[2021-08-11 22:59:04.747 algo-1:75 INFO hook.py:488] Hook is writing from the hook with pid: 75#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m{'loss': 4.3919, 'learning_rate': 9e-06, 'epoch': 0.06}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1369] 2021-08-11 22:59:20,113 >> #015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m{'train_runtime': 15.9084, 'train_samples_per_second': 40.23, 'train_steps_per_second': 0.629, 'train_loss': 4.391919326782227, 'epoch': 0.06}#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:404] 2021-08-11 22:59:20,133 >> max_steps is given, it will override any value given in num_train_epochs#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:421] 2021-08-11 22:59:20,133 >> Using amp fp16 backend#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1928] 2021-08-11 22:59:20,134 >> Saving model checkpoint to /tmp/tmpgy8s4hj3#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:379] 2021-08-11 22:59:20,136 >> Configuration saved in /tmp/tmpgy8s4hj3/config.json#015\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1001] 2021-08-11 22:59:24,622 >> Model weights saved in /tmp/tmpgy8s4hj3/pytorch_model.bin#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2003] 2021-08-11 22:59:24,623 >> tokenizer config file saved in /tmp/tmpgy8s4hj3/tokenizer_config.json#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2009] 2021-08-11 22:59:24,624 >> Special tokens file saved in /tmp/tmpgy8s4hj3/special_tokens_map.json#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1928] 2021-08-11 22:59:51,528 >> Saving model checkpoint to tmp#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:379] 2021-08-11 22:59:51,530 >> Configuration saved in tmp/config.json#015\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1001] 2021-08-11 22:59:55,974 >> Model weights saved in tmp/pytorch_model.bin#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2003] 2021-08-11 22:59:55,975 >> tokenizer config file saved in tmp/tokenizer_config.json#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2009] 2021-08-11 22:59:55,975 >> Special tokens file saved in tmp/special_tokens_map.json#015\u001b[0m\n",
      "\u001b[34m***** train metrics *****#015\n",
      "  epoch                    =       0.06#015\n",
      "  train_loss               =     4.3919#015\n",
      "  train_runtime            = 0:00:15.90#015\n",
      "  train_samples            =      10003#015\n",
      "  train_samples_per_second =      40.23#015\n",
      "  train_steps_per_second   =      0.629#015\u001b[0m\n",
      "\u001b[34m08/11/2021 22:59:56 - INFO - __main__ - *** Evaluate ***#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-11 22:59:56,095 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-11 22:59:56,099 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-11 22:59:56,099 >>   Num examples = 3080#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-11 22:59:56,099 >>   Batch size = 8#015\u001b[0m\n",
      "\u001b[34m***** eval metrics *****#015\n",
      "  epoch                   =       0.06#015\n",
      "  eval_accuracy           =      0.013#015\n",
      "  eval_loss               =     4.3866#015\n",
      "  eval_runtime            = 0:00:26.99#015\n",
      "  eval_samples            =       3080#015\n",
      "  eval_samples_per_second =    114.109#015\n",
      "  eval_steps_per_second   =     14.264#015\u001b[0m\n",
      "\n",
      "2021-08-11 23:00:47 Uploading - Uploading generated training model\n",
      "2021-08-11 23:00:47 Completed - Training job completed\n",
      "\u001b[34mwandb: Currently logged in as: morgan (use `wandb login --relogin` to force relogin)\n",
      "\u001b[0m\n",
      "\u001b[34mCondaEnvException: Unable to determine environment\n",
      "\u001b[0m\n",
      "\u001b[34mPlease re-run this command with one of the following options:\n",
      "\u001b[0m\n",
      "\u001b[34m* Provide an environment name via --name or -n\u001b[0m\n",
      "\u001b[34m* Re-run this command inside an activated conda environment.\n",
      "\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.12.0\u001b[0m\n",
      "\u001b[34mwandb: Syncing run roberta-large\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/morgan/hf-sagemaker\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/morgan/hf-sagemaker/runs/huggingface-pytorch-training-2021-08-11-22-51-42-838-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20210811_225735-huggingface-pytorch-training-2021-08-11-22-51-42-838-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34m#015Downloading:   0% 0.00/2.34k [00:00<?, ?B/s]#015Downloading: 7.30kB [00:00, 5.64MB/s]       #015\u001b[0m\n",
      "\u001b[34m#015Downloading:   0% 0.00/1.75k [00:00<?, ?B/s]#015Downloading: 5.89kB [00:00, 4.87MB/s]       #015\u001b[0m\n",
      "\u001b[34m#015Downloading:   0% 0.00/158k [00:00<?, ?B/s]#015Downloading: 839kB [00:00, 32.9MB/s]       #015\u001b[0m\n",
      "\u001b[34m#015Downloading:   0% 0.00/51.1k [00:00<?, ?B/s]#015Downloading: 240kB [00:00, 19.4MB/s]        #015\u001b[0m\n",
      "\u001b[34m#0150 examples [00:00, ? examples/s]#0154039 examples [00:00, 40384.07 examples/s]#0158175 examples [00:00, 40671.47 examples/s]#015                                          #015#0150 examples [00:00, ? examples/s]#015                                #015#015  0% 0/2 [00:00<?, ?it/s]#015100% 2/2 [00:00<00:00, 348.26it/s]#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-11 22:57:39,593 >> https://huggingface.co/roberta-large/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi4jk69qy#015\u001b[0m\n",
      "\u001b[34m#015Downloading:   0% 0.00/482 [00:00<?, ?B/s]#015Downloading: 100% 482/482 [00:00<00:00, 579kB/s]#015\u001b[0m\n",
      "\u001b[34m2021-08-11 23:00:39,156 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-11 22:57:39,654 >> storing https://huggingface.co/roberta-large/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-11 22:57:39,654 >> creating metadata file for /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:545] 2021-08-11 22:57:39,655 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:581] 2021-08-11 22:57:39,657 >> Model config RobertaConfig {#015\n",
      "  \"architectures\": [#015\n",
      "    \"RobertaForMaskedLM\"#015\n",
      "  ],#015\n",
      "  \"attention_probs_dropout_prob\": 0.1,#015\n",
      "  \"bos_token_id\": 0,#015\n",
      "  \"classifier_dropout\": null,#015\n",
      "  \"eos_token_id\": 2,#015\n",
      "  \"gradient_checkpointing\": false,#015\n",
      "  \"hidden_act\": \"gelu\",#015\n",
      "  \"hidden_dropout_prob\": 0.1,#015\n",
      "  \"hidden_size\": 1024,#015\n",
      "  \"id2label\": {#015\n",
      "    \"0\": \"LABEL_0\",#015\n",
      "    \"1\": \"LABEL_1\",#015\n",
      "    \"2\": \"LABEL_2\",#015\n",
      "    \"3\": \"LABEL_3\",#015\n",
      "    \"4\": \"LABEL_4\",#015\n",
      "    \"5\": \"LABEL_5\",#015\n",
      "    \"6\": \"LABEL_6\",#015\n",
      "    \"7\": \"LABEL_7\",#015\n",
      "    \"8\": \"LABEL_8\",#015\n",
      "    \"9\": \"LABEL_9\",#015\n",
      "    \"10\": \"LABEL_10\",#015\n",
      "    \"11\": \"LABEL_11\",#015\n",
      "    \"12\": \"LABEL_12\",#015\n",
      "    \"13\": \"LABEL_13\",#015\n",
      "    \"14\": \"LABEL_14\",#015\n",
      "    \"15\": \"LABEL_15\",#015\n",
      "    \"16\": \"LABEL_16\",#015\n",
      "    \"17\": \"LABEL_17\",#015\n",
      "    \"18\": \"LABEL_18\",#015\n",
      "    \"19\": \"LABEL_19\",#015\n",
      "    \"20\": \"LABEL_20\",#015\n",
      "    \"21\": \"LABEL_21\",#015\n",
      "    \"22\": \"LABEL_22\",#015\n",
      "    \"23\": \"LABEL_23\",#015\n",
      "    \"24\": \"LABEL_24\",#015\n",
      "    \"25\": \"LABEL_25\",#015\n",
      "    \"26\": \"LABEL_26\",#015\n",
      "    \"27\": \"LABEL_27\",#015\n",
      "    \"28\": \"LABEL_28\",#015\n",
      "    \"29\": \"LABEL_29\",#015\n",
      "    \"30\": \"LABEL_30\",#015\n",
      "    \"31\": \"LABEL_31\",#015\n",
      "    \"32\": \"LABEL_32\",#015\n",
      "    \"33\": \"LABEL_33\",#015\n",
      "    \"34\": \"LABEL_34\",#015\n",
      "    \"35\": \"LABEL_35\",#015\n",
      "    \"36\": \"LABEL_36\",#015\n",
      "    \"37\": \"LABEL_37\",#015\n",
      "    \"38\": \"LABEL_38\",#015\n",
      "    \"39\": \"LABEL_39\",#015\n",
      "    \"40\": \"LABEL_40\",#015\n",
      "    \"41\": \"LABEL_41\",#015\n",
      "    \"42\": \"LABEL_42\",#015\n",
      "    \"43\": \"LABEL_43\",#015\n",
      "    \"44\": \"LABEL_44\",#015\n",
      "    \"45\": \"LABEL_45\",#015\n",
      "    \"46\": \"LABEL_46\",#015\n",
      "    \"47\": \"LABEL_47\",#015\n",
      "    \"48\": \"LABEL_48\",#015\n",
      "    \"49\": \"LABEL_49\",#015\n",
      "    \"50\": \"LABEL_50\",#015\n",
      "    \"51\": \"LABEL_51\",#015\n",
      "    \"52\": \"LABEL_52\",#015\n",
      "    \"53\": \"LABEL_53\",#015\n",
      "    \"54\": \"LABEL_54\",#015\n",
      "    \"55\": \"LABEL_55\",#015\n",
      "    \"56\": \"LABEL_56\",#015\n",
      "    \"57\": \"LABEL_57\",#015\n",
      "    \"58\": \"LABEL_58\",#015\n",
      "    \"59\": \"LABEL_59\",#015\n",
      "    \"60\": \"LABEL_60\",#015\n",
      "    \"61\": \"LABEL_61\",#015\n",
      "    \"62\": \"LABEL_62\",#015\n",
      "    \"63\": \"LABEL_63\",#015\n",
      "    \"64\": \"LABEL_64\",#015\n",
      "    \"65\": \"LABEL_65\",#015\n",
      "    \"66\": \"LABEL_66\",#015\n",
      "    \"67\": \"LABEL_67\",#015\n",
      "    \"68\": \"LABEL_68\",#015\n",
      "    \"69\": \"LABEL_69\",#015\n",
      "    \"70\": \"LABEL_70\",#015\n",
      "    \"71\": \"LABEL_71\",#015\n",
      "    \"72\": \"LABEL_72\",#015\n",
      "    \"73\": \"LABEL_73\",#015\n",
      "    \"74\": \"LABEL_74\",#015\n",
      "    \"75\": \"LABEL_75\",#015\n",
      "    \"76\": \"LABEL_76\"#015\n",
      "  },#015\n",
      "  \"initializer_range\": 0.02,#015\n",
      "  \"intermediate_size\": 4096,#015\n",
      "  \"label2id\": {#015\n",
      "    \"LABEL_0\": 0,#015\n",
      "    \"LABEL_1\": 1,#015\n",
      "    \"LABEL_10\": 10,#015\n",
      "    \"LABEL_11\": 11,#015\n",
      "    \"LABEL_12\": 12,#015\n",
      "    \"LABEL_13\": 13,#015\n",
      "    \"LABEL_14\": 14,#015\n",
      "    \"LABEL_15\": 15,#015\n",
      "    \"LABEL_16\": 16,#015\n",
      "    \"LABEL_17\": 17,#015\n",
      "    \"LABEL_18\": 18,#015\n",
      "    \"LABEL_19\": 19,#015\n",
      "    \"LABEL_2\": 2,#015\n",
      "    \"LABEL_20\": 20,#015\n",
      "    \"LABEL_21\": 21,#015\n",
      "    \"LABEL_22\": 22,#015\n",
      "    \"LABEL_23\": 23,#015\n",
      "    \"LABEL_24\": 24,#015\n",
      "    \"LABEL_25\": 25,#015\n",
      "    \"LABEL_26\": 26,#015\n",
      "    \"LABEL_27\": 27,#015\n",
      "    \"LABEL_28\": 28,#015\n",
      "    \"LABEL_29\": 29,#015\n",
      "    \"LABEL_3\": 3,#015\n",
      "    \"LABEL_30\": 30,#015\n",
      "    \"LABEL_31\": 31,#015\n",
      "    \"LABEL_32\": 32,#015\n",
      "    \"LABEL_33\": 33,#015\n",
      "    \"LABEL_34\": 34,#015\n",
      "    \"LABEL_35\": 35,#015\n",
      "    \"LABEL_36\": 36,#015\n",
      "    \"LABEL_37\": 37,#015\n",
      "    \"LABEL_38\": 38,#015\n",
      "    \"LABEL_39\": 39,#015\n",
      "    \"LABEL_4\": 4,#015\n",
      "    \"LABEL_40\": 40,#015\n",
      "    \"LABEL_41\": 41,#015\n",
      "    \"LABEL_42\": 42,#015\n",
      "    \"LABEL_43\": 43,#015\n",
      "    \"LABEL_44\": 44,#015\n",
      "    \"LABEL_45\": 45,#015\n",
      "    \"LABEL_46\": 46,#015\n",
      "    \"LABEL_47\": 47,#015\n",
      "    \"LABEL_48\": 48,#015\n",
      "    \"LABEL_49\": 49,#015\n",
      "    \"LABEL_5\": 5,#015\n",
      "    \"LABEL_50\": 50,#015\n",
      "    \"LABEL_51\": 51,#015\n",
      "    \"LABEL_52\": 52,#015\n",
      "    \"LABEL_53\": 53,#015\n",
      "    \"LABEL_54\": 54,#015\n",
      "    \"LABEL_55\": 55,#015\n",
      "    \"LABEL_56\": 56,#015\n",
      "    \"LABEL_57\": 57,#015\n",
      "    \"LABEL_58\": 58,#015\n",
      "    \"LABEL_59\": 59,#015\n",
      "    \"LABEL_6\": 6,#015\n",
      "    \"LABEL_60\": 60,#015\n",
      "    \"LABEL_61\": 61,#015\n",
      "    \"LABEL_62\": 62,#015\n",
      "    \"LABEL_63\": 63,#015\n",
      "    \"LABEL_64\": 64,#015\n",
      "    \"LABEL_65\": 65,#015\n",
      "    \"LABEL_66\": 66,#015\n",
      "    \"LABEL_67\": 67,#015\n",
      "    \"LABEL_68\": 68,#015\n",
      "    \"LABEL_69\": 69,#015\n",
      "    \"LABEL_7\": 7,#015\n",
      "    \"LABEL_70\": 70,#015\n",
      "    \"LABEL_71\": 71,#015\n",
      "    \"LABEL_72\": 72,#015\n",
      "    \"LABEL_73\": 73,#015\n",
      "    \"LABEL_74\": 74,#015\n",
      "    \"LABEL_75\": 75,#015\n",
      "    \"LABEL_76\": 76,#015\n",
      "    \"LABEL_8\": 8,#015\n",
      "    \"LABEL_9\": 9#015\n",
      "  },#015\n",
      "  \"layer_norm_eps\": 1e-05,#015\n",
      "  \"max_position_embeddings\": 514,#015\n",
      "  \"model_type\": \"roberta\",#015\n",
      "  \"num_attention_heads\": 16,#015\n",
      "  \"num_hidden_layers\": 24,#015\n",
      "  \"pad_token_id\": 1,#015\n",
      "  \"position_embedding_type\": \"absolute\",#015\n",
      "  \"transformers_version\": \"4.10.0.dev0\",#015\n",
      "  \"type_vocab_size\": 1,#015\n",
      "  \"use_cache\": true,#015\n",
      "  \"vocab_size\": 50265#015\u001b[0m\n",
      "\u001b[34m}#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_auto.py:303] 2021-08-11 22:57:39,720 >> Could not locate the tokenizer configuration file, will try to use the model config instead.#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:545] 2021-08-11 22:57:39,788 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:581] 2021-08-11 22:57:39,788 >> Model config RobertaConfig {#015\n",
      "  \"architectures\": [#015\n",
      "    \"RobertaForMaskedLM\"#015\n",
      "  ],#015\n",
      "  \"attention_probs_dropout_prob\": 0.1,#015\n",
      "  \"bos_token_id\": 0,#015\n",
      "  \"classifier_dropout\": null,#015\n",
      "  \"eos_token_id\": 2,#015\n",
      "  \"gradient_checkpointing\": false,#015\n",
      "  \"hidden_act\": \"gelu\",#015\n",
      "  \"hidden_dropout_prob\": 0.1,#015\n",
      "  \"hidden_size\": 1024,#015\n",
      "  \"initializer_range\": 0.02,#015\n",
      "  \"intermediate_size\": 4096,#015\n",
      "  \"layer_norm_eps\": 1e-05,#015\n",
      "  \"max_position_embeddings\": 514,#015\n",
      "  \"model_type\": \"roberta\",#015\n",
      "  \"num_attention_heads\": 16,#015\n",
      "  \"num_hidden_layers\": 24,#015\n",
      "  \"pad_token_id\": 1,#015\n",
      "  \"position_embedding_type\": \"absolute\",#015\n",
      "  \"transformers_version\": \"4.10.0.dev0\",#015\n",
      "  \"type_vocab_size\": 1,#015\n",
      "  \"use_cache\": true,#015\n",
      "  \"vocab_size\": 50265#015\u001b[0m\n",
      "\u001b[34m}#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-11 22:57:39,970 >> https://huggingface.co/roberta-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi4xes0c0#015\u001b[0m\n",
      "\u001b[34m#015Downloading:   0% 0.00/899k [00:00<?, ?B/s]#015Downloading: 100% 899k/899k [00:00<00:00, 19.9MB/s]#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-11 22:57:40,084 >> storing https://huggingface.co/roberta-large/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-11 22:57:40,084 >> creating metadata file for /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-11 22:57:40,147 >> https://huggingface.co/roberta-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp80iyi2gm#015\u001b[0m\n",
      "\u001b[34m#015Downloading:   0% 0.00/456k [00:00<?, ?B/s]#015Downloading: 100% 456k/456k [00:00<00:00, 12.6MB/s]#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-11 22:57:40,247 >> storing https://huggingface.co/roberta-large/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-11 22:57:40,248 >> creating metadata file for /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-11 22:57:40,309 >> https://huggingface.co/roberta-large/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0qw81zm0#015\u001b[0m\n",
      "\u001b[34m#015Downloading:   0% 0.00/1.36M [00:00<?, ?B/s]#015Downloading: 100% 1.36M/1.36M [00:00<00:00, 24.0MB/s]#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-11 22:57:40,437 >> storing https://huggingface.co/roberta-large/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-11 22:57:40,437 >> creating metadata file for /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-11 22:57:40,632 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-11 22:57:40,633 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-11 22:57:40,633 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-11 22:57:40,633 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-11 22:57:40,633 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1730] 2021-08-11 22:57:40,633 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:545] 2021-08-11 22:57:40,696 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:581] 2021-08-11 22:57:40,697 >> Model config RobertaConfig {#015\n",
      "  \"architectures\": [#015\n",
      "    \"RobertaForMaskedLM\"#015\n",
      "  ],#015\n",
      "  \"attention_probs_dropout_prob\": 0.1,#015\n",
      "  \"bos_token_id\": 0,#015\n",
      "  \"classifier_dropout\": null,#015\n",
      "  \"eos_token_id\": 2,#015\n",
      "  \"gradient_checkpointing\": false,#015\n",
      "  \"hidden_act\": \"gelu\",#015\n",
      "  \"hidden_dropout_prob\": 0.1,#015\n",
      "  \"hidden_size\": 1024,#015\n",
      "  \"initializer_range\": 0.02,#015\n",
      "  \"intermediate_size\": 4096,#015\n",
      "  \"layer_norm_eps\": 1e-05,#015\n",
      "  \"max_position_embeddings\": 514,#015\n",
      "  \"model_type\": \"roberta\",#015\n",
      "  \"num_attention_heads\": 16,#015\n",
      "  \"num_hidden_layers\": 24,#015\n",
      "  \"pad_token_id\": 1,#015\n",
      "  \"position_embedding_type\": \"absolute\",#015\n",
      "  \"transformers_version\": \"4.10.0.dev0\",#015\n",
      "  \"type_vocab_size\": 1,#015\n",
      "  \"use_cache\": true,#015\n",
      "  \"vocab_size\": 50265#015\u001b[0m\n",
      "\u001b[34m}#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1632] 2021-08-11 22:57:40,883 >> https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi1zwag5g#015\u001b[0m\n",
      "\u001b[34m#015Downloading:   0% 0.00/1.43G [00:00<?, ?B/s]#015Downloading:   0% 976k/1.43G [00:00<02:26, 9.76MB/s]#015Downloading:   0% 4.98M/1.43G [00:00<01:52, 12.6MB/s]#015Downloading:   0% 6.81M/1.43G [00:00<01:42, 13.9MB/s]#015Downloading:   1% 9.51M/1.43G [00:00<01:27, 16.3MB/s]#015Downloading:   1% 12.1M/1.43G [00:00<01:17, 18.3MB/s]#015Downloading:   1% 14.2M/1.43G [00:00<01:34, 15.0MB/s]#015Downloading:   1% 18.6M/1.43G [00:00<01:15, 18.7MB/s]#015Downloading:   1% 21.2M/1.43G [00:00<01:16, 18.2MB/s]#015Downloading:   2% 23.5M/1.43G [00:01<01:26, 16.2MB/s]#015Downloading:   2% 25.5M/1.43G [00:01<01:28, 15.8MB/s]#015Downloading:   2% 27.4M/1.43G [00:01<01:29, 15.7MB/s]#015Downloading:   2% 30.1M/1.43G [00:01<01:17, 18.0MB/s]#015Downloading:   2% 32.8M/1.43G [00:01<01:09, 19.9MB/s]#015Downloading:   2% 35.1M/1.43G [00:01<01:09, 19.9MB/s]#015Downloading:   3% 37.7M/1.43G [00:01<01:04, 21.5MB/s]#015Downloading:   3% 40.2M/1.43G [00:01<01:02, 22.3MB/s]#015Downloading:   3% 42.7M/1.43G [00:02<00:59, 23.2MB/s]#015Downloading:   3% 45.3M/1.43G [00:02<00:57, 23.8MB/s]#015Downloading:   3% 47.7M/1.43G [00:02<01:04, 21.4MB/s]#015Downloading:   4% 50.5M/1.43G [00:02<00:59, 23.0MB/s]#015Downloading:   4% 53.2M/1.43G [00:02<00:57, 24.0MB/s]#015Downloading:   4% 55.8M/1.43G [00:02<00:55, 24.6MB/s]#015Downloading:   4% 58.4M/1.43G [00:02<00:55, 24.8MB/s]#015Downloading:   4% 61.2M/1.43G [00:02<00:53, 25.7MB/s]#015Downloading:   4% 63.8M/1.43G [00:02<00:52, 26.0MB/s]#015Downloading:   5% 66.6M/1.43G [00:02<00:51, 26.4MB/s]#015Downloading:   5% 69.2M/1.43G [00:03<00:51, 26.4MB/s]#015Downloading:   5% 71.9M/1.43G [00:03<00:52, 26.0MB/s]#015Downloading:   5% 74.6M/1.43G [00:03<00:51, 26.2MB/s]#015Downloading:   5% 77.2M/1.43G [00:03<00:51, 26.3MB/s]#015Downloading:   6% 80.0M/1.43G [00:03<00:50, 26.8MB/s]#015Downloading:   6% 82.8M/1.43G [00:03<00:49, 26.9MB/s]#015Downloading:   6% 85.5M/1.43G [00:03<00:50, 26.7MB/s]#015Downloading:   6% 88.2M/1.43G [00:03<00:49, 26.9MB/s]#015Downloading:   6% 90.9M/1.43G [00:03<00:50, 26.4MB/s]#015Downloading:   7% 93.6M/1.43G [00:03<00:51, 25.9MB/s]#015Downloading:   7% 96.3M/1.43G [00:04<00:50, 26.3MB/s]#015Downloading:   7% 99.0M/1.43G [00:04<00:49, 26.6MB/s]#015Downloading:   7% 102M/1.43G [00:04<00:49, 27.0MB/s] #015Downloading:   7% 105M/1.43G [00:04<00:49, 26.5MB/s]#015Downloading:   8% 107M/1.43G [00:04<00:49, 26.7MB/s]#015Downloading:   8% 110M/1.43G [00:04<00:48, 27.0MB/s]#015Downloading:   8% 113M/1.43G [00:04<00:49, 26.4MB/s]#015Downloading:   8% 115M/1.43G [00:04<00:49, 26.4MB/s]#015Downloading:   8% 118M/1.43G [00:04<00:50, 26.1MB/s]#015Downloading:   8% 121M/1.43G [00:04<00:50, 25.7MB/s]#015Downloading:   9% 123M/1.43G [00:05<00:50, 25.8MB/s]#015Downloading:   9% 126M/1.43G [00:05<00:50, 25.9MB/s]#015Downloading:   9% 129M/1.43G [00:05<00:49, 26.2MB/s]#015Downloading:   9% 131M/1.43G [00:05<00:49, 26.3MB/s]#015Downloading:   9% 134M/1.43G [00:05<00:49, 26.2MB/s]#015Downloading:  10% 137M/1.43G [00:05<00:48, 26.5MB/s]#015Downloading:  10% 139M/1.43G [00:05<00:48, 26.6MB/s]#015Downloading:  10% 142M/1.43G [00:05<00:53, 24.2MB/s]#015Downloading:  10% 145M/1.43G [00:05<00:51, 25.1MB/s]#015Downloading:  10% 147M/1.43G [00:06<00:50, 25.6MB/s]#015Downloading:  11% 150M/1.43G [00:06<00:48, 26.1MB/s]#015Downloading:  11% 153M/1.43G [00:06<00:47, 26.6MB/s]#015Downloading:  11% 156M/1.43G [00:06<00:47, 26.9MB/s]#015Downloading:  11% 158M/1.43G [00:06<00:47, 26.5MB/s]#015Downloading:  11% 161M/1.43G [00:06<00:47, 26.7MB/s]#015Downloading:  11% 164M/1.43G [00:06<00:47, 26.8MB/s]#015Downloading:  12% 167M/1.43G [00:06<00:48, 25.8MB/s]#015Downloading:  12% 169M/1.43G [00:06<00:55, 22.8MB/s]#015Downloading:  12% 172M/1.43G [00:06<00:52, 23.8MB/s]#015Downloading:  12% 174M/1.43G [00:07<00:50, 24.6MB/s]#015Downloading:  12% 177M/1.43G [00:07<00:52, 24.0MB/s]#015Downloading:  13% 180M/1.43G [00:07<00:51, 24.4MB/s]#015Downloading:  13% 183M/1.43G [00:07<00:49, 25.3MB/s]#015Downloading:  13% 185M/1.43G [00:07<00:47, 25.9MB/s]#015Downloading:  13% 188M/1.43G [00:07<00:46, 26.5MB/s]#015Downloading:  13% 191M/1.43G [00:07<00:47, 26.0MB/s]#015Downloading:  14% 193M/1.43G [00:07<00:48, 25.3MB/s]#015Downloading:  14% 196M/1.43G [00:07<00:47, 26.0MB/s]#015Downloading:  14% 199M/1.43G [00:08<00:48, 25.5MB/s]#015Downloading:  14% 201M/1.43G [00:08<00:47, 25.7MB/s]#015Downloading:  14% 204M/1.43G [00:08<00:47, 25.8MB/s]#015Downloading:  14% 207M/1.43G [00:08<00:47, 25.8MB/s]#015Downloading:  15% 209M/1.43G [00:08<00:46, 26.2MB/s]#015Downloading:  15% 212M/1.43G [00:08<00:46, 26.3MB/s]#015Downloading:  15% 215M/1.43G [00:08<00:46, 26.1MB/s]#015Downloading:  15% 217M/1.43G [00:08<00:46, 25.9MB/s]#015Downloading:  15% 220M/1.43G [00:08<00:47, 25.5MB/s]#015Downloading:  16% 223M/1.43G [00:08<00:46, 26.0MB/s]#015Downloading:  16% 225M/1.43G [00:09<00:45, 26.4MB/s]#015Downloading:  16% 228M/1.43G [00:09<00:48, 24.9MB/s]#015Downloading:  16% 231M/1.43G [00:09<00:46, 25.5MB/s]#015Downloading:  16% 233M/1.43G [00:09<00:46, 25.9MB/s]#015Downloading:  17% 236M/1.43G [00:09<00:45, 25.9MB/s]#015Downloading:  17% 239M/1.43G [00:09<00:45, 25.8MB/s]#015Downloading:  17% 241M/1.43G [00:09<00:45, 26.2MB/s]#015Downloading:  17% 244M/1.43G [00:09<00:46, 25.7MB/s]#015Downloading:  17% 247M/1.43G [00:09<00:45, 26.0MB/s]#015Downloading:  17% 249M/1.43G [00:09<00:45, 26.0MB/s]#015Downloading:  18% 252M/1.43G [00:10<00:44, 26.4MB/s]#015Downloading:  18% 255M/1.43G [00:10<00:43, 26.9MB/s]#015Downloading:  18% 258M/1.43G [00:10<00:44, 26.3MB/s]#015Downloading:  18% 260M/1.43G [00:10<00:45, 25.8MB/s]#015Downloading:  18% 263M/1.43G [00:10<00:44, 26.0MB/s]#015Downloading:  19% 266M/1.43G [00:10<00:43, 26.4MB/s]#015Downloading:  19% 268M/1.43G [00:10<00:43, 26.3MB/s]#015Downloading:  19% 271M/1.43G [00:10<00:44, 26.0MB/s]#015Downloading:  19% 273M/1.43G [00:10<00:44, 25.7MB/s]#015Downloading:  19% 276M/1.43G [00:11<00:44, 25.7MB/s]#015Downloading:  20% 279M/1.43G [00:11<00:47, 24.0MB/s]#015Downloading:  20% 281M/1.43G [00:11<00:56, 20.3MB/s]#015Downloading:  20% 284M/1.43G [00:11<00:52, 21.7MB/s]#015Downloading:  20% 286M/1.43G [00:11<00:49, 23.0MB/s]#015Downloading:  20% 289M/1.43G [00:11<00:51, 22.3MB/s]#015Downloading:  20% 291M/1.43G [00:11<00:51, 22.1MB/s]#015Downloading:  21% 293M/1.43G [00:11<00:56, 19.9MB/s]#015Downloading:  21% 295M/1.43G [00:11<00:57, 19.7MB/s]#015Downloading:  21% 298M/1.43G [00:12<00:53, 21.3MB/s]#015Downloading:  21% 301M/1.43G [00:12<00:49, 22.7MB/s]#015Downloading:  21% 303M/1.43G [00:12<00:47, 23.6MB/s]#015Downloading:  21% 306M/1.43G [00:12<00:46, 24.3MB/s]#015Downloading:  22% 309M/1.43G [00:12<00:45, 24.8MB/s]#015Downloading:  22% 311M/1.43G [00:12<00:49, 22.5MB/s]#015Downloading:  22% 313M/1.43G [00:12<00:53, 20.7MB/s]#015Downloading:  22% 315M/1.43G [00:12<01:00, 18.4MB/s]#015Downloading:  22% 317M/1.43G [00:13<01:02, 17.7MB/s]#015Downloading:  22% 320M/1.43G [00:13<00:56, 19.6MB/s]#015Downloading:  23% 323M/1.43G [00:13<00:51, 21.4MB/s]#015Downloading:  23% 325M/1.43G [00:13<01:09, 15.8MB/s]#015Downloading:  23% 327M/1.43G [00:13<01:04, 16.9MB/s]#015Downloading:  23% 330M/1.43G [00:13<00:57, 19.0MB/s]#015Downloading:  23% 332M/1.43G [00:13<00:58, 18.7MB/s]#015Downloading:  23% 334M/1.43G [00:13<00:54, 20.2MB/s]#015Downloading:  24% 337M/1.43G [00:13<00:49, 22.0MB/s]#015Downloading:  24% 340M/1.43G [00:14<00:47, 23.0MB/s]#015Downloading:  24% 342M/1.43G [00:14<00:46, 23.3MB/s]#015Downloading:  24% 345M/1.43G [00:14<00:43, 24.7MB/s]#015Downloading:  24% 348M/1.43G [00:14<00:42, 25.6MB/s]#015Downloading:  25% 351M/1.43G [00:14<00:46, 23.2MB/s]#015Downloading:  25% 353M/1.43G [00:14<00:51, 20.8MB/s]#015Downloading:  25% 355M/1.43G [00:14<01:08, 15.6MB/s]#015Downloading:  25% 357M/1.43G [00:14<01:02, 17.1MB/s]#015Downloading:  25% 359M/1.43G [00:15<00:58, 18.1MB/s]#015Downloading:  25% 362M/1.43G [00:15<00:55, 19.1MB/s]#015Downloading:  26% 364M/1.43G [00:15<00:52, 20.0MB/s]#015Downloading:  26% 367M/1.43G [00:15<00:48, 21.7MB/s]#015Downloading:  26% 369M/1.43G [00:15<00:46, 22.7MB/s]#015Downloading:  26% 372M/1.43G [00:15<00:44, 23.9MB/s]#015Downloading:  26% 375M/1.43G [00:15<00:47, 22.0MB/s]#015Downloading:  26% 377M/1.43G [00:15<00:57, 18.2MB/s]#015Downloading:  27% 379M/1.43G [00:15<00:55, 18.8MB/s]#015Downloading:  27% 381M/1.43G [00:16<00:51, 20.1MB/s]#015Downloading:  27% 384M/1.43G [00:16<00:54, 19.1MB/s]#015Downloading:  27% 386M/1.43G [00:16<00:50, 20.8MB/s]#015Downloading:  27% 389M/1.43G [00:16<00:46, 22.5MB/s]#015Downloading:  27% 391M/1.43G [00:16<00:50, 20.4MB/s]#015Downloading:  28% 394M/1.43G [00:16<00:50, 20.2MB/s]#015Downloading:  28% 396M/1.43G [00:16<00:52, 19.7MB/s]#015Downloading:  28% 398M/1.43G [00:16<00:55, 18.7MB/s]#015Downloading:  28% 400M/1.43G [00:17<00:50, 20.5MB/s]#015Downloading:  28% 403M/1.43G [00:17<00:48, 21.0MB/s]#015Downloading:  28% 405M/1.43G [00:17<00:47, 21.7MB/s]#015Downloading:  29% 407M/1.43G [00:17<00:48, 21.0MB/s]#015Downloading:  29% 410M/1.43G [00:17<00:45, 22.3MB/s]#015Downloading:  29% 412M/1.43G [00:17<00:44, 22.8MB/s]#015Downloading:  29% 415M/1.43G [00:17<00:42, 23.7MB/s]#015Downloading:  29% 418M/1.43G [00:17<00:44, 22.6MB/s]#015Downloading:  29% 420M/1.43G [00:17<00:43, 23.3MB/s]#015Downloading:  30% 423M/1.43G [00:17<00:41, 24.3MB/s]#015Downloading:  30% 426M/1.43G [00:18<00:39, 25.4MB/s]#015Downloading:  30% 428M/1.43G [00:18<00:38, 26.0MB/s]#015Downloading:  30% 431M/1.43G [00:18<00:38, 26.2MB/s]#015Downloading:  30% 434M/1.43G [00:18<00:37, 26.5MB/s]#015Downloading:  31% 437M/1.43G [00:18<00:36, 27.1MB/s]#015Downloading:  31% 439M/1.43G [00:18<00:36, 27.3MB/s]#015Downloading:  31% 442M/1.43G [00:18<00:36, 26.9MB/s]#015Downloading:  31% 445M/1.43G [00:18<00:37, 26.5MB/s]#015Downloading:  31% 448M/1.43G [00:18<00:38, 25.7MB/s]#015Downloading:  32% 450M/1.43G [00:18<00:38, 25.4MB/s]#015Downloading:  32% 453M/1.43G [00:19<00:38, 25.4MB/s]#015Downloading:  32% 455M/1.43G [00:19<00:37, 25.6MB/s]#015Downloading:  32% 458M/1.43G [00:19<00:37, 25.8MB/s]#015Downloading:  32% 461M/1.43G [00:19<00:36, 26.8MB/s]#015Downloading:  33% 464M/1.43G [00:19<00:35, 27.0MB/s]#015Downloading:  33% 466M/1.43G [00:19<00:35, 27.2MB/s]#015Downloading:  33% 469M/1.43G [00:19<00:38, 25.1MB/s]#015Downloading:  33% 472M/1.43G [00:19<00:40, 23.8MB/s]#015Downloading:  33% 474M/1.43G [00:19<00:38, 24.5MB/s]#015Downloading:  33% 477M/1.43G [00:20<00:37, 25.5MB/s]#015Downloading:  34% 480M/1.43G [00:20<00:36, 25.9MB/s]#015Downloading:  34% 483M/1.43G [00:20<00:37, 25.2MB/s]#015Downloading:  34% 485M/1.43G [00:20<00:37, 25.1MB/s]#015Downloading:  34% 488M/1.43G [00:20<00:41, 22.5MB/s]#015Downloading:  34% 490M/1.43G [00:20<00:40, 23.1MB/s]#015Downloading:  35% 493M/1.43G [00:20<00:38, 24.1MB/s]#015Downloading:  35% 495M/1.43G [00:20<00:38, 24.5MB/s]#015Downloading:  35% 498M/1.43G [00:20<00:36, 25.4MB/s]#015Downloading:  35% 501M/1.43G [00:21<00:38, 24.1MB/s]#015Downloading:  35% 503M/1.43G [00:21<00:37, 24.8MB/s]#015Downloading:  35% 506M/1.43G [00:21<00:35, 25.6MB/s]#015Downloading:  36% 509M/1.43G [00:21<00:35, 25.9MB/s]#015Downloading:  36% 511M/1.43G [00:21<00:38, 23.8MB/s]#015Downloading:  36% 514M/1.43G [00:21<00:38, 23.8MB/s]#015Downloading:  36% 516M/1.43G [00:21<00:57, 15.7MB/s]#015Downloading:  37% 521M/1.43G [00:21<00:45, 19.8MB/s]#015Downloading:  37% 524M/1.43G [00:22<00:40, 22.3MB/s]#015Downloading:  37% 527M/1.43G [00:22<00:38, 23.5MB/s]#015Downloading:  37% 530M/1.43G [00:22<00:36, 24.6MB/s]#015Downloading:  37% 533M/1.43G [00:22<00:35, 25.2MB/s]#015Downloading:  38% 536M/1.43G [00:22<00:34, 25.7MB/s]#015Downloading:  38% 539M/1.43G [00:22<00:34, 25.9MB/s]#015Downloading:  38% 541M/1.43G [00:22<00:34, 25.8MB/s]#015Downloading:  38% 544M/1.43G [00:22<00:34, 25.5MB/s]#015Downloading:  38% 547M/1.43G [00:22<00:37, 23.6MB/s]#015Downloading:  39% 549M/1.43G [00:22<00:35, 24.7MB/s]#015Downloading:  39% 552M/1.43G [00:23<00:35, 24.6MB/s]#015Downloading:  39% 555M/1.43G [00:23<00:34, 25.2MB/s]#015Downloading:  39% 557M/1.43G [00:23<00:34, 25.4MB/s]#015Downloading:  39% 560M/1.43G [00:23<00:33, 26.2MB/s]#015Downloading:  39% 563M/1.43G [00:23<00:33, 26.0MB/s]#015Downloading:  40% 565M/1.43G [00:23<00:32, 26.5MB/s]#015Downloading:  40% 568M/1.43G [00:23<00:31, 26.9MB/s]#015Downloading:  40% 571M/1.43G [00:23<00:35, 23.9MB/s]#015Downloading:  40% 573M/1.43G [00:23<00:36, 23.5MB/s]#015Downloading:  40% 576M/1.43G [00:24<00:37, 22.8MB/s]#015Downloading:  41% 578M/1.43G [00:24<00:40, 20.8MB/s]#015Downloading:  41% 581M/1.43G [00:24<00:38, 22.0MB/s]#015Downloading:  41% 583M/1.43G [00:24<00:37, 22.5MB/s]#015Downloading:  41% 586M/1.43G [00:24<00:37, 22.3MB/s]#015Downloading:  41% 588M/1.43G [00:24<00:43, 19.4MB/s]#015Downloading:  41% 590M/1.43G [00:24<00:40, 20.7MB/s]#015Downloading:  42% 593M/1.43G [00:24<00:37, 22.4MB/s]#015Downloading:  42% 595M/1.43G [00:25<00:40, 20.6MB/s]#015Downloading:  42% 598M/1.43G [00:25<00:37, 22.1MB/s]#015Downloading:  42% 601M/1.43G [00:25<00:37, 21.8MB/s]#015Downloading:  42% 603M/1.43G [00:25<00:36, 22.7MB/s]#015Downloading:  42% 606M/1.43G [00:25<00:34, 24.1MB/s]#015Downloading:  43% 608M/1.43G [00:25<00:38, 21.2MB/s]#015Downloading:  43% 611M/1.43G [00:25<00:35, 22.7MB/s]#015Downloading:  43% 613M/1.43G [00:25<00:39, 20.4MB/s]#015Downloading:  43% 616M/1.43G [00:25<00:37, 21.8MB/s]#015Downloading:  43% 619M/1.43G [00:26<00:35, 23.0MB/s]#015Downloading:  44% 621M/1.43G [00:26<00:34, 23.4MB/s]#015Downloading:  44% 624M/1.43G [00:26<00:33, 23.7MB/s]#015Downloading:  44% 626M/1.43G [00:26<00:33, 24.0MB/s]#015Downloading:  44% 629M/1.43G [00:26<00:33, 23.9MB/s]#015Downloading:  44% 631M/1.43G [00:26<00:31, 25.0MB/s]#015Downloading:  44% 634M/1.43G [00:26<00:36, 21.7MB/s]#015Downloading:  45% 637M/1.43G [00:26<00:34, 23.2MB/s]#015Downloading:  45% 640M/1.43G [00:26<00:32, 24.4MB/s]#015Downloading:  45% 642M/1.43G [00:27<00:36, 21.8MB/s]#015Downloading:  45% 645M/1.43G [00:27<00:34, 22.5MB/s]#015Downloading:  45% 647M/1.43G [00:27<00:33, 23.2MB/s]#015Downloading:  46% 649M/1.43G [00:27<00:39, 19.7MB/s]#015Downloading:  46% 652M/1.43G [00:27<00:36, 21.4MB/s]#015Downloading:  46% 655M/1.43G [00:27<00:33, 22.9MB/s]#015Downloading:  46% 657M/1.43G [00:27<00:33, 23.2MB/s]#015Downloading:  46% 660M/1.43G [00:27<00:31, 24.5MB/s]#015Downloading:  46% 663M/1.43G [00:27<00:34, 21.8MB/s]#015Downloading:  47% 665M/1.43G [00:28<00:32, 23.3MB/s]#015Downloading:  47% 668M/1.43G [00:28<00:31, 23.7MB/s]#015Downloading:  47% 670M/1.43G [00:28<00:31, 23.6MB/s]#015Downloading:  47% 673M/1.43G [00:28<00:39, 19.3MB/s]#015Downloading:  47% 675M/1.43G [00:28<00:36, 20.8MB/s]#015Downloading:  48% 678M/1.43G [00:28<00:33, 22.4MB/s]#015Downloading:  48% 681M/1.43G [00:28<00:31, 23.8MB/s]#015Downloading:  48% 683M/1.43G [00:28<00:30, 24.7MB/s]#015Downloading:  48% 686M/1.43G [00:28<00:33, 22.1MB/s]#015Downloading:  48% 688M/1.43G [00:29<00:32, 22.5MB/s]#015Downloading:  48% 691M/1.43G [00:29<00:30, 24.0MB/s]#015Downloading:  49% 694M/1.43G [00:29<00:29, 24.7MB/s]#015Downloading:  49% 696M/1.43G [00:29<00:29, 24.7MB/s]#015Downloading:  49% 699M/1.43G [00:29<00:28, 25.6MB/s]#015Downloading:  49% 702M/1.43G [00:29<00:29, 24.5MB/s]#015Downloading:  49% 705M/1.43G [00:29<00:28, 25.4MB/s]#015Downloading:  50% 707M/1.43G [00:29<00:28, 25.6MB/s]#015Downloading:  50% 710M/1.43G [00:29<00:31, 22.8MB/s]#015Downloading:  50% 712M/1.43G [00:30<00:33, 21.2MB/s]#015Downloading:  50% 715M/1.43G [00:30<00:32, 21.9MB/s]#015Downloading:  50% 717M/1.43G [00:30<00:30, 23.2MB/s]#015Downloading:  50% 720M/1.43G [00:30<00:33, 21.0MB/s]#015Downloading:  51% 722M/1.43G [00:30<00:31, 22.6MB/s]#015Downloading:  51% 725M/1.43G [00:30<00:29, 23.4MB/s]#015Downloading:  51% 728M/1.43G [00:30<00:33, 20.7MB/s]#015Downloading:  51% 730M/1.43G [00:30<00:31, 22.0MB/s]#015Downloading:  51% 733M/1.43G [00:30<00:29, 23.2MB/s]#015Downloading:  52% 735M/1.43G [00:31<00:28, 24.0MB/s]#015Downloading:  52% 738M/1.43G [00:31<00:28, 23.7MB/s]#015Downloading:  52% 740M/1.43G [00:31<00:28, 24.3MB/s]#015Downloading:  52% 743M/1.43G [00:31<00:29, 23.4MB/s]#015Downloading:  52% 745M/1.43G [00:31<00:28, 23.8MB/s]#015Downloading:  52% 748M/1.43G [00:31<00:27, 24.7MB/s]#015Downloading:  53% 751M/1.43G [00:31<00:29, 22.7MB/s]#015Downloading:  53% 753M/1.43G [00:31<00:28, 23.6MB/s]#015Downloading:  53% 756M/1.43G [00:31<00:27, 24.0MB/s]#015Downloading:  53% 758M/1.43G [00:31<00:27, 24.7MB/s]#015Downloading:  53% 761M/1.43G [00:32<00:31, 21.1MB/s]#015Downloading:  54% 763M/1.43G [00:32<00:29, 22.3MB/s]#015Downloading:  54% 766M/1.43G [00:32<00:28, 23.0MB/s]#015Downloading:  54% 768M/1.43G [00:32<00:29, 22.6MB/s]#015Downloading:  54% 771M/1.43G [00:32<00:42, 15.3MB/s]#015Downloading:  54% 773M/1.43G [00:32<00:43, 15.1MB/s]#015Downloading:  54% 774M/1.43G [00:32<00:41, 15.5MB/s]#015Downloading:  54% 777M/1.43G [00:33<00:36, 17.8MB/s]#015Downloading:  55% 779M/1.43G [00:33<00:34, 18.7MB/s]#015Downloading:  55% 782M/1.43G [00:33<00:31, 20.7MB/s]#015Downloading:  55% 785M/1.43G [00:33<00:28, 22.1MB/s]#015Downloading:  55% 787M/1.43G [00:33<00:32, 19.7MB/s]#015Downloading:  55% 790M/1.43G [00:33<00:30, 21.1MB/s]#015Downloading:  56% 792M/1.43G [00:33<00:34, 18.4MB/s]#015Downloading:  56% 794M/1.43G [00:33<00:31, 20.0MB/s]#015Downloading:  56% 796M/1.43G [00:34<00:33, 18.7MB/s]#015Downloading:  56% 799M/1.43G \u001b[0m\n",
      "\u001b[34m[00:34<00:30, 20.4MB/s]#015Downloading:  56% 802M/1.43G [00:34<00:28, 21.8MB/s]#015Downloading:  56% 804M/1.43G [00:34<00:27, 23.0MB/s]#015Downloading:  57% 807M/1.43G [00:34<00:25, 24.3MB/s]#015Downloading:  57% 810M/1.43G [00:34<00:25, 23.7MB/s]#015Downloading:  57% 812M/1.43G [00:34<00:24, 24.9MB/s]#015Downloading:  57% 815M/1.43G [00:34<00:23, 25.7MB/s]#015Downloading:  57% 818M/1.43G [00:34<00:23, 25.6MB/s]#015Downloading:  58% 820M/1.43G [00:34<00:23, 25.5MB/s]#015Downloading:  58% 823M/1.43G [00:35<00:23, 25.8MB/s]#015Downloading:  58% 826M/1.43G [00:35<00:23, 25.8MB/s]#015Downloading:  58% 828M/1.43G [00:35<00:29, 20.0MB/s]#015Downloading:  58% 831M/1.43G [00:35<00:43, 13.6MB/s]#015Downloading:  59% 835M/1.43G [00:35<00:40, 14.7MB/s]#015Downloading:  59% 839M/1.43G [00:35<00:32, 18.3MB/s]#015Downloading:  59% 843M/1.43G [00:36<00:26, 22.1MB/s]#015Downloading:  59% 846M/1.43G [00:36<00:28, 20.5MB/s]#015Downloading:  60% 849M/1.43G [00:36<00:28, 20.1MB/s]#015Downloading:  60% 852M/1.43G [00:36<00:26, 21.8MB/s]#015Downloading:  60% 854M/1.43G [00:36<00:25, 22.8MB/s]#015Downloading:  60% 857M/1.43G [00:36<00:24, 23.1MB/s]#015Downloading:  60% 859M/1.43G [00:36<00:23, 23.7MB/s]#015Downloading:  60% 862M/1.43G [00:36<00:22, 24.9MB/s]#015Downloading:  61% 865M/1.43G [00:36<00:21, 25.7MB/s]#015Downloading:  61% 868M/1.43G [00:37<00:21, 25.5MB/s]#015Downloading:  61% 870M/1.43G [00:37<00:21, 25.5MB/s]#015Downloading:  61% 873M/1.43G [00:37<00:21, 25.7MB/s]#015Downloading:  61% 875M/1.43G [00:37<00:22, 25.0MB/s]#015Downloading:  62% 878M/1.43G [00:37<00:22, 24.7MB/s]#015Downloading:  62% 881M/1.43G [00:37<00:21, 25.3MB/s]#015Downloading:  62% 883M/1.43G [00:37<00:21, 25.3MB/s]#015Downloading:  62% 886M/1.43G [00:37<00:20, 25.7MB/s]#015Downloading:  62% 889M/1.43G [00:37<00:20, 26.2MB/s]#015Downloading:  63% 891M/1.43G [00:37<00:20, 26.3MB/s]#015Downloading:  63% 894M/1.43G [00:38<00:20, 26.2MB/s]#015Downloading:  63% 897M/1.43G [00:38<00:20, 26.2MB/s]#015Downloading:  63% 899M/1.43G [00:38<00:20, 26.1MB/s]#015Downloading:  63% 902M/1.43G [00:38<00:19, 26.5MB/s]#015Downloading:  63% 905M/1.43G [00:38<00:20, 25.1MB/s]#015Downloading:  64% 907M/1.43G [00:38<00:20, 25.1MB/s]#015Downloading:  64% 910M/1.43G [00:38<00:20, 25.2MB/s]#015Downloading:  64% 912M/1.43G [00:38<00:19, 25.9MB/s]#015Downloading:  64% 915M/1.43G [00:38<00:19, 26.1MB/s]#015Downloading:  64% 918M/1.43G [00:39<00:19, 25.8MB/s]#015Downloading:  65% 920M/1.43G [00:39<00:19, 26.1MB/s]#015Downloading:  65% 923M/1.43G [00:39<00:18, 26.6MB/s]#015Downloading:  65% 926M/1.43G [00:39<00:18, 27.1MB/s]#015Downloading:  65% 929M/1.43G [00:39<00:18, 26.9MB/s]#015Downloading:  65% 931M/1.43G [00:39<00:19, 25.1MB/s]#015Downloading:  66% 934M/1.43G [00:39<00:19, 25.5MB/s]#015Downloading:  66% 937M/1.43G [00:39<00:19, 25.5MB/s]#015Downloading:  66% 939M/1.43G [00:39<00:19, 25.5MB/s]#015Downloading:  66% 942M/1.43G [00:39<00:19, 25.3MB/s]#015Downloading:  66% 944M/1.43G [00:40<00:19, 25.3MB/s]#015Downloading:  66% 947M/1.43G [00:40<00:18, 26.1MB/s]#015Downloading:  67% 950M/1.43G [00:40<00:18, 26.3MB/s]#015Downloading:  67% 953M/1.43G [00:40<00:18, 26.3MB/s]#015Downloading:  67% 955M/1.43G [00:40<00:19, 23.9MB/s]#015Downloading:  67% 958M/1.43G [00:40<00:21, 21.4MB/s]#015Downloading:  67% 960M/1.43G [00:40<00:20, 23.0MB/s]#015Downloading:  68% 963M/1.43G [00:40<00:20, 23.0MB/s]#015Downloading:  68% 965M/1.43G [00:40<00:20, 22.7MB/s]#015Downloading:  68% 968M/1.43G [00:41<00:22, 20.8MB/s]#015Downloading:  68% 971M/1.43G [00:41<00:20, 22.5MB/s]#015Downloading:  68% 973M/1.43G [00:41<00:19, 23.5MB/s]#015Downloading:  68% 976M/1.43G [00:41<00:18, 24.1MB/s]#015Downloading:  69% 978M/1.43G [00:41<00:22, 19.6MB/s]#015Downloading:  69% 981M/1.43G [00:41<00:21, 20.5MB/s]#015Downloading:  69% 983M/1.43G [00:41<00:25, 17.1MB/s]#015Downloading:  69% 986M/1.43G [00:41<00:22, 19.5MB/s]#015Downloading:  69% 988M/1.43G [00:42<00:20, 20.9MB/s]#015Downloading:  69% 991M/1.43G [00:42<00:19, 22.3MB/s]#015Downloading:  70% 993M/1.43G [00:42<00:20, 20.7MB/s]#015Downloading:  70% 996M/1.43G [00:42<00:20, 21.5MB/s]#015Downloading:  70% 998M/1.43G [00:42<00:25, 16.9MB/s]#015Downloading:  70% 1.00G/1.43G [00:42<00:24, 17.1MB/s]#015Downloading:  70% 1.00G/1.43G [00:42<00:22, 19.2MB/s]#015Downloading:  70% 1.00G/1.43G [00:42<00:20, 20.7MB/s]#015Downloading:  71% 1.01G/1.43G [00:42<00:18, 22.4MB/s]#015Downloading:  71% 1.01G/1.43G [00:43<00:17, 23.1MB/s]#015Downloading:  71% 1.01G/1.43G [00:43<00:18, 22.8MB/s]#015Downloading:  71% 1.02G/1.43G [00:43<00:17, 23.7MB/s]#015Downloading:  71% 1.02G/1.43G [00:43<00:16, 24.8MB/s]#015Downloading:  72% 1.02G/1.43G [00:43<00:15, 25.4MB/s]#015Downloading:  72% 1.02G/1.43G [00:43<00:15, 26.2MB/s]#015Downloading:  72% 1.03G/1.43G [00:43<00:15, 25.6MB/s]#015Downloading:  72% 1.03G/1.43G [00:43<00:15, 26.0MB/s]#015Downloading:  72% 1.03G/1.43G [00:43<00:15, 25.2MB/s]#015Downloading:  73% 1.03G/1.43G [00:44<00:16, 24.4MB/s]#015Downloading:  73% 1.04G/1.43G [00:44<00:16, 23.0MB/s]#015Downloading:  73% 1.04G/1.43G [00:44<00:16, 24.0MB/s]#015Downloading:  73% 1.04G/1.43G [00:44<00:18, 21.2MB/s]#015Downloading:  73% 1.04G/1.43G [00:44<00:16, 23.1MB/s]#015Downloading:  73% 1.05G/1.43G [00:44<00:17, 22.2MB/s]#015Downloading:  74% 1.05G/1.43G [00:44<00:18, 20.1MB/s]#015Downloading:  74% 1.05G/1.43G [00:44<00:17, 21.5MB/s]#015Downloading:  74% 1.05G/1.43G [00:44<00:16, 23.2MB/s]#015Downloading:  74% 1.06G/1.43G [00:45<00:15, 24.2MB/s]#015Downloading:  74% 1.06G/1.43G [00:45<00:21, 17.1MB/s]#015Downloading:  74% 1.06G/1.43G [00:45<00:21, 17.0MB/s]#015Downloading:  75% 1.06G/1.43G [00:45<00:20, 17.5MB/s]#015Downloading:  75% 1.07G/1.43G [00:45<00:18, 19.7MB/s]#015Downloading:  75% 1.07G/1.43G [00:45<00:22, 16.2MB/s]#015Downloading:  75% 1.07G/1.43G [00:45<00:22, 15.9MB/s]#015Downloading:  75% 1.07G/1.43G [00:46<00:22, 15.4MB/s]#015Downloading:  75% 1.08G/1.43G [00:46<00:20, 17.3MB/s]#015Downloading:  76% 1.08G/1.43G [00:46<00:18, 19.0MB/s]#015Downloading:  76% 1.08G/1.43G [00:46<00:16, 21.0MB/s]#015Downloading:  76% 1.08G/1.43G [00:46<00:16, 20.5MB/s]#015Downloading:  76% 1.09G/1.43G [00:46<00:19, 17.2MB/s]#015Downloading:  76% 1.09G/1.43G [00:46<00:20, 16.4MB/s]#015Downloading:  76% 1.09G/1.43G [00:46<00:18, 17.9MB/s]#015Downloading:  77% 1.09G/1.43G [00:47<00:16, 19.9MB/s]#015Downloading:  77% 1.09G/1.43G [00:47<00:15, 21.4MB/s]#015Downloading:  77% 1.10G/1.43G [00:47<00:14, 22.6MB/s]#015Downloading:  77% 1.10G/1.43G [00:47<00:13, 23.5MB/s]#015Downloading:  77% 1.10G/1.43G [00:47<00:13, 24.3MB/s]#015Downloading:  78% 1.11G/1.43G [00:47<00:12, 24.8MB/s]#015Downloading:  78% 1.11G/1.43G [00:47<00:13, 22.9MB/s]#015Downloading:  78% 1.11G/1.43G [00:47<00:13, 23.9MB/s]#015Downloading:  78% 1.11G/1.43G [00:47<00:12, 24.3MB/s]#015Downloading:  78% 1.12G/1.43G [00:47<00:12, 23.9MB/s]#015Downloading:  78% 1.12G/1.43G [00:48<00:12, 24.8MB/s]#015Downloading:  79% 1.12G/1.43G [00:48<00:11, 25.5MB/s]#015Downloading:  79% 1.12G/1.43G [00:48<00:11, 25.9MB/s]#015Downloading:  79% 1.13G/1.43G [00:48<00:13, 21.9MB/s]#015Downloading:  79% 1.13G/1.43G [00:48<00:12, 23.5MB/s]#015Downloading:  79% 1.13G/1.43G [00:48<00:12, 24.5MB/s]#015Downloading:  80% 1.13G/1.43G [00:48<00:11, 25.0MB/s]#015Downloading:  80% 1.14G/1.43G [00:48<00:11, 24.6MB/s]#015Downloading:  80% 1.14G/1.43G [00:48<00:13, 21.6MB/s]#015Downloading:  80% 1.14G/1.43G [00:49<00:13, 20.5MB/s]#015Downloading:  80% 1.14G/1.43G [00:49<00:12, 21.8MB/s]#015Downloading:  80% 1.15G/1.43G [00:49<00:11, 23.5MB/s]#015Downloading:  81% 1.15G/1.43G [00:49<00:11, 24.2MB/s]#015Downloading:  81% 1.15G/1.43G [00:49<00:13, 19.5MB/s]#015Downloading:  81% 1.16G/1.43G [00:49<00:12, 21.2MB/s]#015Downloading:  81% 1.16G/1.43G [00:49<00:11, 22.8MB/s]#015Downloading:  81% 1.16G/1.43G [00:49<00:11, 22.6MB/s]#015Downloading:  82% 1.16G/1.43G [00:50<00:16, 16.3MB/s]#015Downloading:  82% 1.17G/1.43G [00:50<00:14, 18.2MB/s]#015Downloading:  82% 1.17G/1.43G [00:50<00:12, 20.0MB/s]#015Downloading:  82% 1.17G/1.43G [00:50<00:11, 21.4MB/s]#015Downloading:  82% 1.17G/1.43G [00:50<00:12, 20.7MB/s]#015Downloading:  82% 1.18G/1.43G [00:50<00:11, 22.3MB/s]#015Downloading:  83% 1.18G/1.43G [00:50<00:11, 21.4MB/s]#015Downloading:  83% 1.18G/1.43G [00:50<00:12, 19.7MB/s]#015Downloading:  83% 1.18G/1.43G [00:51<00:11, 21.2MB/s]#015Downloading:  83% 1.19G/1.43G [00:51<00:10, 22.9MB/s]#015Downloading:  83% 1.19G/1.43G [00:51<00:10, 22.7MB/s]#015Downloading:  84% 1.19G/1.43G [00:51<00:09, 24.0MB/s]#015Downloading:  84% 1.19G/1.43G [00:51<00:09, 24.4MB/s]#015Downloading:  84% 1.20G/1.43G [00:51<00:09, 23.5MB/s]#015Downloading:  84% 1.20G/1.43G [00:51<00:09, 23.9MB/s]#015Downloading:  84% 1.20G/1.43G [00:51<00:09, 24.2MB/s]#015Downloading:  84% 1.20G/1.43G [00:52<00:15, 14.2MB/s]#015Downloading:  85% 1.21G/1.43G [00:52<00:12, 17.9MB/s]#015Downloading:  85% 1.21G/1.43G [00:52<00:09, 22.2MB/s]#015Downloading:  85% 1.22G/1.43G [00:52<00:07, 26.4MB/s]#015Downloading:  86% 1.22G/1.43G [00:52<00:08, 25.2MB/s]#015Downloading:  86% 1.23G/1.43G [00:52<00:07, 25.3MB/s]#015Downloading:  86% 1.23G/1.43G [00:52<00:07, 25.7MB/s]#015Downloading:  86% 1.23G/1.43G [00:52<00:07, 26.3MB/s]#015Downloading:  87% 1.23G/1.43G [00:53<00:07, 25.7MB/s]#015Downloading:  87% 1.24G/1.43G [00:53<00:07, 25.7MB/s]#015Downloading:  87% 1.24G/1.43G [00:53<00:07, 25.7MB/s]#015Downloading:  87% 1.24G/1.43G [00:53<00:07, 26.0MB/s]#015Downloading:  87% 1.25G/1.43G [00:53<00:06, 26.2MB/s]#015Downloading:  88% 1.25G/1.43G [00:53<00:06, 26.6MB/s]#015Downloading:  88% 1.25G/1.43G [00:53<00:06, 26.6MB/s]#015Downloading:  88% 1.25G/1.43G [00:53<00:07, 24.1MB/s]#015Downloading:  88% 1.26G/1.43G [00:53<00:06, 25.2MB/s]#015Downloading:  88% 1.26G/1.43G [00:54<00:06, 25.8MB/s]#015Downloading:  88% 1.26G/1.43G [00:54<00:06, 25.9MB/s]#015Downloading:  89% 1.26G/1.43G [00:54<00:06, 26.2MB/s]#015Downloading:  89% 1.27G/1.43G [00:54<00:06, 24.3MB/s]#015Downloading:  89% 1.27G/1.43G [00:54<00:06, 24.5MB/s]#015Downloading:  89% 1.27G/1.43G [00:54<00:06, 25.1MB/s]#015Downloading:  89% 1.27G/1.43G [00:54<00:05, 25.5MB/s]#015Downloading:  90% 1.28G/1.43G [00:54<00:07, 21.2MB/s]#015Downloading:  90% 1.28G/1.43G [00:54<00:06, 22.9MB/s]#015Downloading:  90% 1.28G/1.43G [00:55<00:06, 23.7MB/s]#015Downloading:  90% 1.29G/1.43G [00:55<00:05, 23.7MB/s]#015Downloading:  90% 1.29G/1.43G [00:55<00:05, 24.8MB/s]#015Downloading:  91% 1.29G/1.43G [00:55<00:07, 17.1MB/s]#015Downloading:  91% 1.29G/1.43G [00:55<00:06, 19.1MB/s]#015Downloading:  91% 1.30G/1.43G [00:55<00:06, 20.9MB/s]#015Downloading:  91% 1.30G/1.43G [00:55<00:07, 17.4MB/s]#015Downloading:  91% 1.30G/1.43G [00:55<00:06, 18.2MB/s]#015Downloading:  91% 1.30G/1.43G [00:56<00:07, 17.3MB/s]#015Downloading:  92% 1.31G/1.43G [00:56<00:06, 19.3MB/s]#015Downloading:  92% 1.31G/1.43G [00:56<00:06, 17.9MB/s]#015Downloading:  92% 1.31G/1.43G [00:56<00:05, 19.8MB/s]#015Downloading:  92% 1.31G/1.43G [00:56<00:05, 20.1MB/s]#015Downloading:  92% 1.31G/1.43G [00:56<00:06, 18.6MB/s]#015Downloading:  92% 1.32G/1.43G [00:56<00:05, 20.4MB/s]#015Downloading:  93% 1.32G/1.43G [00:56<00:04, 22.0MB/s]#015Downloading:  93% 1.32G/1.43G [00:57<00:05, 20.1MB/s]#015Downloading:  93% 1.32G/1.43G [00:57<00:05, 18.4MB/s]#015Downloading:  93% 1.33G/1.43G [00:57<00:05, 19.9MB/s]#015Downloading:  93% 1.33G/1.43G [00:57<00:04, 21.5MB/s]#015Downloading:  93% 1.33G/1.43G [00:57<00:05, 18.9MB/s]#015Downloading:  94% 1.33G/1.43G [00:57<00:04, 20.7MB/s]#015Downloading:  94% 1.34G/1.43G [00:57<00:04, 19.4MB/s]#015Downloading:  94% 1.34G/1.43G [00:57<00:04, 21.4MB/s]#015Downloading:  94% 1.34G/1.43G [00:57<00:03, 22.8MB/s]#015Downloading:  94% 1.34G/1.43G [00:58<00:04, 20.0MB/s]#015Downloading:  94% 1.35G/1.43G [00:58<00:03, 21.5MB/s]#015Downloading:  95% 1.35G/1.43G [00:58<00:04, 18.9MB/s]#015Downloading:  95% 1.35G/1.43G [00:58<00:03, 20.6MB/s]#015Downloading:  95% 1.35G/1.43G [00:58<00:03, 21.3MB/s]#015Downloading:  95% 1.36G/1.43G [00:58<00:03, 21.3MB/s]#015Downloading:  95% 1.36G/1.43G [00:58<00:02, 22.6MB/s]#015Downloading:  95% 1.36G/1.43G [00:58<00:03, 19.2MB/s]#015Downloading:  96% 1.36G/1.43G [00:59<00:03, 20.7MB/s]#015Downloading:  96% 1.37G/1.43G [00:59<00:02, 21.7MB/s]#015Downloading:  96% 1.37G/1.43G [00:59<00:02, 21.0MB/s]#015Downloading:  96% 1.37G/1.43G [00:59<00:03, 15.9MB/s]#015Downloading:  96% 1.37G/1.43G [00:59<00:03, 16.6MB/s]#015Downloading:  96% 1.38G/1.43G [00:59<00:02, 18.3MB/s]#015Downloading:  97% 1.38G/1.43G [00:59<00:02, 18.2MB/s]#015Downloading:  97% 1.38G/1.43G [00:59<00:02, 20.0MB/s]#015Downloading:  97% 1.38G/1.43G [00:59<00:02, 21.4MB/s]#015Downloading:  97% 1.38G/1.43G [01:00<00:01, 22.5MB/s]#015Downloading:  97% 1.39G/1.43G [01:00<00:01, 21.7MB/s]#015Downloading:  97% 1.39G/1.43G [01:00<00:02, 17.2MB/s]#015Downloading:  98% 1.39G/1.43G [01:00<00:01, 17.4MB/s]#015Downloading:  98% 1.39G/1.43G [01:00<00:01, 19.0MB/s]#015Downloading:  98% 1.40G/1.43G [01:00<00:01, 17.3MB/s]#015Downloading:  98% 1.40G/1.43G [01:00<00:01, 19.6MB/s]#015Downloading:  98% 1.40G/1.43G [01:00<00:01, 19.9MB/s]#015Downloading:  98% 1.40G/1.43G [01:01<00:01, 20.3MB/s]#015Downloading:  99% 1.40G/1.43G [01:01<00:01, 18.1MB/s]#015Downloading:  99% 1.41G/1.43G [01:01<00:00, 19.0MB/s]#015Downloading:  99% 1.41G/1.43G [01:01<00:00, 17.3MB/s]#015Downloading:  99% 1.41G/1.43G [01:01<00:00, 19.2MB/s]#015Downloading:  99% 1.41G/1.43G [01:01<00:00, 16.2MB/s]#015Downloading:  99% 1.42G/1.43G [01:01<00:00, 15.3MB/s]#015Downloading:  99% 1.42G/1.43G [01:01<00:00, 16.3MB/s]#015Downloading: 100% 1.42G/1.43G [01:02<00:00, 12.2MB/s]#015Downloading: 100% 1.42G/1.43G [01:02<00:00, 14.4MB/s]#015Downloading: 100% 1.42G/1.43G [01:02<00:00, 15.6MB/s]#015Downloading: 100% 1.43G/1.43G [01:02<00:00, 22.8MB/s]#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1636] 2021-08-11 22:58:43,684 >> storing https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352#015\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1644] 2021-08-11 22:58:43,684 >> creating metadata file for /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352#015\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1275] 2021-08-11 22:58:43,684 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352#015\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1512] 2021-08-11 22:58:52,085 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']#015\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).#015\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).#015\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1523] 2021-08-11 22:58:52,085 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']#015\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.#015\u001b[0m\n",
      "\u001b[34m#015Running tokenizer on dataset:   0% 0/11 [00:00<?, ?ba/s]#015Running tokenizer on dataset:   9% 1/11 [00:00<00:01,  6.41ba/s]#015Running tokenizer on dataset:  36% 4/11 [00:00<00:00,  8.25ba/s]#015Running tokenizer on dataset:  64% 7/11 [00:00<00:00, 10.30ba/s]#015Running tokenizer on dataset:  91% 10/11 [00:00<00:00, 12.35ba/s]#015Running tokenizer on dataset: 100% 11/11 [00:00<00:00, 20.68ba/s]#015\u001b[0m\n",
      "\u001b[34m#015Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]#015Running tokenizer on dataset:  50% 2/4 [00:00<00:00,  9.54ba/s]#015Running tokenizer on dataset: 100% 4/4 [00:00<00:00, 15.81ba/s]#015\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mwandb#033[0m: #033[33mWARNING#033[0m Calling wandb.login() after wandb.init() has no effect.#015\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mwandb#033[0m: #033[33mWARNING#033[0m Calling wandb.login() after wandb.init() has no effect.#015\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mwandb#033[0m: #033[33mWARNING#033[0m Calling wandb.login() after wandb.init() has no effect.#015\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mwandb#033[0m: #033[33mWARNING#033[0m Calling wandb.login() after wandb.init() has no effect.#015\u001b[0m\n",
      "\u001b[34m#015Downloading:   0% 0.00/1.42k [00:00<?, ?B/s]#015Downloading: 3.21kB [00:00, 2.70MB/s]       #015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:404] 2021-08-11 22:59:04,174 >> max_steps is given, it will override any value given in num_train_epochs#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:421] 2021-08-11 22:59:04,175 >> Using amp fp16 backend#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-11 22:59:04,176 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1171] 2021-08-11 22:59:04,204 >> ***** Running training *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1172] 2021-08-11 22:59:04,204 >>   Num examples = 10003#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1173] 2021-08-11 22:59:04,205 >>   Num Epochs = 1#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1174] 2021-08-11 22:59:04,205 >>   Instantaneous batch size per device = 8#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1175] 2021-08-11 22:59:04,205 >>   Total train batch size (w. parallel, distributed & accumulation) = 64#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1176] 2021-08-11 22:59:04,205 >>   Gradient Accumulation steps = 8#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1177] 2021-08-11 22:59:04,205 >>   Total optimization steps = 10#015\u001b[0m\n",
      "\u001b[34m[INFO|integrations.py:447] 2021-08-11 22:59:04,211 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/10 [00:00<?, ?it/s]#015 10% 1/10 [00:03<00:28,  3.17s/it]#015 20% 2/10 [00:04<00:21,  2.66s/it]#015 30% 3/10 [00:05<00:15,  2.26s/it]#015 40% 4/10 [00:07<00:12,  2.03s/it]#015 50% 5/10 [00:08<00:09,  1.85s/it]#015 60% 6/10 [00:10<00:06,  1.72s/it]#015 70% 7/10 [00:11<00:04,  1.62s/it]#015 80% 8/10 [00:13<00:03,  1.55s/it]#015 90% 9/10 [00:14<00:01,  1.51s/it]#015100% 10/10 [00:15<00:00,  1.47s/it]#015                                   #015#015100% 10/10 [00:15<00:00,  1.47s/it][INFO|trainer.py:1369] 2021-08-11 22:59:20,113 >> #015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34m#015                                   #015#015100% 10/10 [00:15<00:00,  1.47s/it][INFO|trainer.py:404] 2021-08-11 22:59:20,133 >> max_steps is given, it will override any value given in num_train_epochs#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:421] 2021-08-11 22:59:20,133 >> Using amp fp16 backend#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1928] 2021-08-11 22:59:20,134 >> Saving model checkpoint to /tmp/tmpgy8s4hj3#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:379] 2021-08-11 22:59:20,136 >> Configuration saved in /tmp/tmpgy8s4hj3/config.json#015\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1001] 2021-08-11 22:59:24,622 >> Model weights saved in /tmp/tmpgy8s4hj3/pytorch_model.bin#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2003] 2021-08-11 22:59:24,623 >> tokenizer config file saved in /tmp/tmpgy8s4hj3/tokenizer_config.json#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2009] 2021-08-11 22:59:24,624 >> Special tokens file saved in /tmp/tmpgy8s4hj3/special_tokens_map.json#015\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mwandb#033[0m: #033[33mWARNING#033[0m Calling wandb.login() after wandb.init() has no effect.#015\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mwandb#033[0m: #033[33mWARNING#033[0m Calling wandb.login() after wandb.init() has no effect.#015\u001b[0m\n",
      "\u001b[34m#015100% 10/10 [00:47<00:00,  4.73s/it]#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1928] 2021-08-11 22:59:51,528 >> Saving model checkpoint to tmp#015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:379] 2021-08-11 22:59:51,530 >> Configuration saved in tmp/config.json#015\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1001] 2021-08-11 22:59:55,974 >> Model weights saved in tmp/pytorch_model.bin#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2003] 2021-08-11 22:59:55,975 >> tokenizer config file saved in tmp/tokenizer_config.json#015\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2009] 2021-08-11 22:59:55,975 >> Special tokens file saved in tmp/special_tokens_map.json#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:528] 2021-08-11 22:59:56,095 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2174] 2021-08-11 22:59:56,099 >> ***** Running Evaluation *****#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2176] 2021-08-11 22:59:56,099 >>   Num examples = 3080#015\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2179] 2021-08-11 22:59:56,099 >>   Batch size = 8#015\u001b[0m\n",
      "\u001b[34m#015  0% 0/385 [00:00<?, ?it/s]#015  1% 3/385 [00:00<00:21, 17.81it/s]#015  1% 5/385 [00:00<00:24, 15.74it/s]#015  2% 7/385 [00:00<00:25, 14.56it/s]#015  2% 9/385 [00:00<00:27, 13.91it/s]#015  3% 11/385 [00:00<00:28, 13.32it/s]#015  3% 13/385 [00:00<00:28, 13.15it/s]#015  4% 15/385 [00:01<00:28, 12.81it/s]#015  4% 17/385 [00:01<00:28, 12.83it/s]#015  5% 19/385 [00:01<00:28, 12.87it/s]#015  5% 21/385 [00:01<00:29, 12.43it/s]#015  6% 23/385 [00:01<00:29, 12.40it/s]#015  6% 25/385 [00:01<00:28, 12.76it/s]#015  7% 27/385 [00:02<00:26, 13.30it/s]#015  8% 29/385 [00:02<00:25, 13.74it/s]#015  8% 31/385 [00:02<00:25, 14.00it/s]#015  9% 33/385 [00:02<00:24, 14.22it/s]#015  9% 35/385 [00:02<00:24, 14.40it/s]#015 10% 37/385 [00:02<00:23, 14.53it/s]#015 10% 39/385 [00:02<00:23, 14.51it/s]#015 11% 41/385 [00:03<00:23, 14.55it/s]#015 11% 43/385 [00:03<00:23, 14.63it/s]#015 12% 45/385 [00:03<00:23, 14.68it/s]#015 12% 47/385 [00:03<00:22, 14.73it/s]#015 13% 49/385 [00:03<00:22, 14.73it/s]#015 13% 51/385 [00:03<00:22, 14.72it/s]#015 14% 53/385 [00:03<00:22, 14.73it/s]#015 14% 55/385 [00:03<00:22, 14.78it/s]#015 15% 57/385 [00:04<00:22, 14.83it/s]#015 15% 59/385 [00:04<00:22, 14.80it/s]#015 16% 61/385 [00:04<00:21, 14.82it/s]#015 16% 63/385 [00:04<00:21, 14.85it/s]#015 17% 65/385 [00:04<00:21, 14.85it/s]#015 17% 67/385 [00:04<00:21, 14.82it/s]#015 18% 69/385 [00:04<00:21, 14.81it/s]#015 18% 71/385 [00:05<00:21, 14.82it/s]#015 19% 73/385 [00:05<00:21, 14.77it/s]#015 19% 75/385 [00:05<00:21, 14.75it/s]#015 20% 77/385 [00:05<00:20, 14.74it/s]#015 21% 79/385 [00:05<00:21, 14.26it/s]#015 21% 81/385 [00:05<00:22, 13.50it/s]#015 22% 83/385 [00:05<00:21, 13.81it/s]#015 22% 85/385 [00:06<00:21, 14.02it/s]#015 23% 87/385 [00:06<00:20, 14.23it/s]#015 23% 89/385 [00:06<00:20, 14.32it/s]#015 24% 91/385 [00:06<00:20, 14.42it/s]#015 24% 93/385 [00:06<00:20, 14.31it/s]#015 25% 95/385 [00:06<00:20, 14.49it/s]#015 25% 97/385 [00:06<00:19, 14.50it/s]#015 26% 99/385 [00:07<00:19, 14.46it/s]#015 26% 101/385 [00:07<00:19, 14.48it/s]#015 27% 103/385 [00:07<00:19, 14.44it/s]#015 27% 105/385 [00:07<00:19, 14.14it/s]#015 28% 107/385 [00:07<00:19, 13.99it/s]#015 28% 109/385 [00:07<00:20, 13.27it/s]#015 29% 111/385 [00:07<00:20, 13.21it/s]#015 29% 113/385 [00:08<00:21, 12.38it/s]#015 30% 115/385 [00:08<00:21, 12.28it/s]#015 30% 117/385 [00:08<00:20, 12.93it/s]#015 31% 119/385 [00:08<00:19, 13.47it/s]#015 31% 121/385 [00:08<00:19, 13.81it/s]#015 32% 123/385 [00:08<00:18, 14.07it/s]#015 32% 125/385 [00:08<00:18, 14.20it/s]#015 33% 127/385 [00:09<00:18, 14.33it/s]#015 34% 129/385 [00:09<00:17, 14.42it/s]#015 34% 131/385 [00:09<00:17, 14.33it/s]#015 35% 133/385 [00:09<00:17, 14.37it/s]#015 35% 135/385 [00:09<00:17, 14.40it/s]#015 36% 137/385 [00:09<00:17, 14.34it/s]#015 36% 139/385 [00:09<00:17, 14.41it/s]#015 37% 141/385 [00:10<00:16, 14.46it/s]#015 37% 143/385 [00:10<00:16, 14.53it/s]#015 38% 145/385 [00:10<00:16, 14.34it/s]#015 38% 147/385 [00:10<00:16, 14.49it/s]#015 39% 149/385 [00:10<00:16, 14.59it/s]#015 39% 151/385 [00:10<00:15, 14.68it/s]#015 40% 153/385 [00:10<00:15, 14.71it/s]#015 40% 155/385 [00:10<00:15, 14.74it/s]#015 41% 157/385 [00:11<00:15, 14.73it/s]#015 41% 159/385 [00:11<00:15, 14.78it/s]#015 42% 161/385 [00:11<00:15, 14.79it/s]#015 42% 163/385 [00:11<00:14, 14.81it/s]#015 43% 165/385 [00:11<00:14, 14.82it/s]#015 43% 167/385 [00:11<00:14, 14.81it/s]#015 44% 169/385 [00:11<00:14, 14.84it/s]#015 44% 171/385 [00:12<00:14, 14.84it/s]#015 45% 173/385 [00:12<00:14, 14.76it/s]#015 45% 175/385 [00:12<00:14, 14.76it/s]#015 46% 177/385 [00:12<00:14, 14.77it/s]#015 46% 179/385 [00:12<00:13, 14.77it/s]#015 47% 181/385 [00:12<00:13, 14.73it/s]#015 48% 183/385 [00:12<00:13, 14.74it/s]#015 48% 185/385 [00:13<00:13, 14.74it/s]#015 49% 187/385 [00:13<00:13, 14.71it/s]#015 49% 189/385 [00:13<00:13, 14.71it/s]#015 50% 191/385 [00:13<00:13, 14.73it/s]#015 50% 193/385 [00:13<00:13, 14.75it/s]#015 51% 195/385 [00:13<00:12, 14.72it/s]#015 51% 197/385 [00:13<00:12, 14.73it/s]#015 52% 199/385 [00:13<00:12, 14.73it/s]#015 52% 201/385 [00:14<00:12, 14.74it/s]#015 53% 203/385 [00:14<00:12, 14.74it/s]#015 53% 205/385 [00:14<00:12, 14.76it/s]#015 54% 207/385 [00:14<00:12, 14.75it/s]#015 54% 209/385 [00:14<00:11, 14.74it/s]#015 55% 211/385 [00:14<00:11, 14.73it/s]#015 55% 213/385 [00:14<00:11, 14.75it/s]#015 56% 215/385 [00:15<00:11, 14.77it/s]#015 56% 217/385 [00:15<00:11, 14.79it/s]#015 57% 219/385 [00:15<00:11, 14.79it/s]#015 57% 221/385 [00:15<00:11, 14.79it/s]#015 58% 223/385 [00:15<00:10, 14.78it/s]#015 58% 225/385 [00:15<00:10, 14.78it/s]#015 59% 227/385 [00:15<00:10, 14.78it/s]#015 59% 229/385 [00:16<00:10, 14.76it/s]#015 60% 231/385 [00:16<00:10, 14.74it/s]#015 61% 233/385 [00:16<00:10, 14.65it/s]#015 61% 235/385 [00:16<00:10, 14.69it/s]#015 62% 237/385 [00:16<00:10, 14.69it/s]#015 62% 239/385 [00:16<00:09, 14.71it/s]#015 63% 241/385 [00:16<00:09, 14.71it/s]#015 63% 243/385 [00:16<00:09, 14.73it/s]#015 64% 245/385 [00:17<00:09, 14.73it/s]#015 64% 247/385 [00:17<00:09, 14.73it/s]#015 65% 249/385 [00:17<00:09, 14.73it/s]#015 65% 251/385 [00:17<00:09, 14.76it/s]#015 66% 253/385 [00:17<00:08, 14.77it/s]#015 66% 255/385 [00:17<00:08, 14.77it/s]#015 67% 257/385 [00:17<00:08, 14.78it/s]#015 67% 259/385 [00:18<00:08, 14.79it/s]#015 68% 261/385 [00:18<00:08, 14.67it/s]#015 68% 263/385 [00:18<00:08, 14.23it/s]#015 69% 265/385 [00:18<00:08, 14.38it/s]#015 69% 267/385 [00:18<00:08, 14.50it/s]#015 70% 269/385 [00:18<00:07, 14.58it/s]#015 70% 271/385 [00:18<00:07, 14.57it/s]#015 71% 273/385 [00:19<00:07, 14.61it/s]#015 71% 275/385 [00:19<00:07, 14.65it/s]#015 72% 277/385 [00:19<00:07, 14.67it/s]#015 72% 279/385 [00:19<00:07, 14.70it/s]#015 73% 281/385 [00:19<00:07, 14.71it/s]#015 74% 283/385 [00:19<00:06, 14.71it/s]#015 74% 285/385 [00:19<00:06, 14.67it/s]#015 75% 287/385 [00:19<00:06, 14.71it/s]#015 75% 289/385 [00:20<00:06, 14.73it/s]#015 76% 291/385 [00:20<00:06, 14.76it/s]#015 76% 293/385 [00:20<00:06, 14.74it/s]#015 77% 295/385 [00:20<00:06, 14.73it/s]#015 77% 297/385 [00:20<00:05, 14.71it/s]#015 78% 299/385 [00:20<00:05, 14.74it/s]#015 78% 301/385 [00:20<00:05, 14.72it/s]#015 79% 303/385 [00:21<00:05, 14.74it/s]#015 79% 305/385 [00:21<00:05, 14.75it/s]#015 80% 307/385 [00:21<00:05, 14.75it/s]#015 80% 309/385 [00:21<00:05, 14.75it/s]#015 81% 311/385 [00:21<00:05, 14.75it/s]#015 81% 313/385 [00:21<00:04, 14.74it/s]#015 82% 315/385 [00:21<00:04, 14.69it/s]#015 82% 317/385 [00:21<00:04, 14.69it/s]#015 83% 319/385 [00:22<00:04, 14.71it/s]#015 83% 321/385 [00:22<00:04, 14.69it/s]#015 84% 323/385 [00:22<00:04, 14.69it/s]#015 84% 325/385 [00:22<00:04, 14.70it/s]#015 85% 327/385 [00:22<00:03, 14.72it/s]#015 85% 329/385 [00:22<00:03, 14.75it/s]#015 86% 331/385 [00:22<00:03, 14.68it/s]#015 86% 333/385 [00:23<00:03, 14.68it/s]#015 87% 335/385 [00:23<00:03, 14.68it/s]#015 88% 337/385 [00:23<00:03, 14.57it/s]#015 88% 339/385 [00:23<00:03, 14.66it/s]#015 89% 341/385 [00:23<00:03, 14.60it/s]#015 89% 343/385 [00:23<00:02, 14.64it/s]#015 90% 345/385 [00:23<00:02, 14.64it/s]#015 90% 347/385 [00:24<00:02, 14.66it/s]#015 91% 349/385 [00:24<00:02, 14.69it/s]#015 91% 351/385 [00:24<00:02, 14.69it/s]#015 92% 353/385 [00:24<00:02, 14.71it/s]#015 92% 355/385 [00:24<00:02, 14.72it/s]#015 93% 357/385 [00:24<00:01, 14.72it/s]#015 93% 359/385 [00:24<00:01, 14.70it/s]#015 94% 361/385 [00:24<00:01, 14.70it/s]#015 94% 363/385 [00:25<00:01, 14.73it/s]#015 95% 365/385 [00:25<00:01, 14.72it/s]#015 95% 367/385 [00:25<00:01, 14.69it/s]#015 96% 369/385 [00:25<00:01, 14.73it/s]#015 96% 371/385 [00:25<00:00, 14.74it/s]#015 97% 373/385 [00:25<00:00, 14.73it/s]#015 97% 375/385 [00:25<00:00, 14.72it/s]#015 98% 377/385 [00:26<00:00, 14.73it/s]#015 98% 379/385 [00:26<00:00, 14.72it/s]#015 99% 381/385 [00:26<00:00, 14.70it/s]#015 99% 383/385 [00:26<00:00, 14.71it/s]#015100% 385/385 [00:26<00:00, 14.66it/s]#033[34m#033[1mwandb#033[0m: #033[33mWARNING#033[0m Calling wandb.login() after wandb.init() has no effect.#015\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mwandb#033[0m: #033[33mWARNING#033[0m Calling wandb.login() after wandb.init() has no effect.#015\u001b[0m\n",
      "\u001b[34m#015100% 385/385 [00:36<00:00, 10.63it/s]#015\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish, PID 99\u001b[0m\n",
      "\u001b[34mwandb: Program ended successfully.\u001b[0m\n",
      "\u001b[34mwandb: - 1358.80MB of 1358.80MB uploaded (0.00MB deduped)#015wandb: \\ 1358.80MB of 1358.80MB uploaded (0.00MB deduped)#015wandb: | 1358.80MB of 1358.91MB uploaded (0.00MB deduped)#015wandb: / 1358.80MB of 1358.91MB uploaded (0.00MB deduped)#015wandb: - 1358.91MB of 1358.91MB uploaded (0.00MB deduped)#015wandb: \\ 1358.91MB of 1358.91MB uploaded (0.00MB deduped)#015wandb: | 1358.91MB of 1358.91MB uploaded (0.00MB deduped)#015wandb: / 1358.91MB of 1358.91MB uploaded (0.00MB deduped)#015wandb: - 1358.91MB of 1358.91MB uploaded (0.00MB deduped)#015wandb: \\ 1358.91MB of 1358.91MB uploaded (0.00MB deduped)#015wandb: | 1358.91MB of 1358.91MB uploaded (0.00MB deduped)#015wandb:                                                                                \u001b[0m\n",
      "\u001b[34mwandb: Find user logs for this run at: /opt/ml/code/wandb/run-20210811_225735-huggingface-pytorch-training-2021-08-11-22-51-42-838-algo-1/logs/debug.log\u001b[0m\n",
      "\u001b[34mwandb: Find internal logs for this run at: /opt/ml/code/wandb/run-20210811_225735-huggingface-pytorch-training-2021-08-11-22-51-42-838-algo-1/logs/debug-internal.log\u001b[0m\n",
      "\u001b[34mwandb: Run summary:\u001b[0m\n",
      "\u001b[34mwandb:                                        train/loss 4.3919\u001b[0m\n",
      "\u001b[34mwandb:                               train/learning_rate 1e-05\u001b[0m\n",
      "\u001b[34mwandb:                                       train/epoch 0.06\u001b[0m\n",
      "\u001b[34mwandb:                                 train/global_step 10\u001b[0m\n",
      "\u001b[34mwandb:                                          _runtime 168\u001b[0m\n",
      "\u001b[34mwandb:                                        _timestamp 1628722823\u001b[0m\n",
      "\u001b[34mwandb:                                             _step 2\u001b[0m\n",
      "\u001b[34mwandb:                               train/train_runtime 15.9084\u001b[0m\n",
      "\u001b[34mwandb:                    train/train_samples_per_second 40.23\u001b[0m\n",
      "\u001b[34mwandb:                      train/train_steps_per_second 0.629\u001b[0m\n",
      "\u001b[34mwandb:                                  train/total_flos 149146803732480.0\u001b[0m\n",
      "\u001b[34mwandb:                                  train/train_loss 4.39192\u001b[0m\n",
      "\u001b[34mwandb:                                         eval/loss 4.38664\u001b[0m\n",
      "\u001b[34mwandb:                                     eval/accuracy 0.01299\u001b[0m\n",
      "\u001b[34mwandb:                                      eval/runtime 26.9917\u001b[0m\n",
      "\u001b[34mwandb:                           eval/samples_per_second 114.109\u001b[0m\n",
      "\u001b[34mwandb:                             eval/steps_per_second 14.264\u001b[0m\n",
      "\u001b[34mwandb: Run history:\u001b[0m\n",
      "\u001b[34mwandb:                       train/loss ▁\u001b[0m\n",
      "\u001b[34mwandb:              train/learning_rate ▁\u001b[0m\n",
      "\u001b[34mwandb:                      train/epoch ▁▁▁\u001b[0m\n",
      "\u001b[34mwandb:                train/global_step ▁▁▁\u001b[0m\n",
      "\u001b[34mwandb:                         _runtime ▁▁█\u001b[0m\n",
      "\u001b[34mwandb:                       _timestamp ▁▁█\u001b[0m\n",
      "\u001b[34mwandb:                            _step ▁▅█\u001b[0m\n",
      "\u001b[34mwandb:              train/train_runtime ▁\u001b[0m\n",
      "\u001b[34mwandb:   train/train_samples_per_second ▁\u001b[0m\n",
      "\u001b[34mwandb:     train/train_steps_per_second ▁\u001b[0m\n",
      "\u001b[34mwandb:                 train/total_flos ▁\u001b[0m\n",
      "\u001b[34mwandb:                 train/train_loss ▁\u001b[0m\n",
      "\u001b[34mwandb:                        eval/loss ▁\u001b[0m\n",
      "\u001b[34mwandb:                    eval/accuracy ▁\u001b[0m\n",
      "\u001b[34mwandb:                     eval/runtime ▁\u001b[0m\n",
      "\u001b[34mwandb:          eval/samples_per_second ▁\u001b[0m\n",
      "\u001b[34mwandb:            eval/steps_per_second ▁\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Synced 5 W&B file(s), 1 media file(s), 9 artifact file(s) and 1 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Synced roberta-large: https://wandb.ai/morgan/hf-sagemaker/runs/huggingface-pytorch-training-2021-08-11-22-51-42-838-algo-1\n",
      "\u001b[0m\n",
      "Training seconds: 404\n",
      "Billable seconds: 404\n",
      "2021-08-11 23:00:59 Starting - Starting the training job...\n",
      "2021-08-11 23:01:01 Starting - Launching requested ML instancesProfilerReport-1628722859: InProgress\n",
      "......\n",
      "2021-08-11 23:02:25 Starting - Preparing the instances for training........"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-1121d64df77d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m                                 hyperparameters = hyperparameters)\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3618\u001b[0m             )\n\u001b[1;32m   3619\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLogState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3620\u001b[0;31m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3622\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "for i in range(12):\n",
    "    \n",
    "    # hyperparameters, which are passed into the training job\n",
    "    hyperparameters={\n",
    "        'output_dir': 'tmp',\n",
    "        'model_name_or_path': 'albert-large-v2', #'distilbert-base-uncased',  # microsoft/deberta-base , roberta-base\n",
    "        'dataset_name': 'banking77',\n",
    "        'do_train': True,\n",
    "        'per_device_train_batch_size': 16,\n",
    "        'per_device_eval_batch_size': 16,\n",
    "        'gradient_accumulation_steps': 4,\n",
    "        'learning_rate': 1e-4,\n",
    "        'warmup_steps': 100,\n",
    "        'fp16': True,\n",
    "        'logging_steps': 10,\n",
    "        'max_steps': 1200,\n",
    "        'eval_steps': 100,\n",
    "        'save_steps': 400,\n",
    "        'evaluation_strategy' : 'steps',\n",
    "        'load_best_model_at_end': True,\n",
    "        'metric_for_best_model': 'accuracy',\n",
    "        'report_to': 'wandb',    # ✍️\n",
    "    }\n",
    "    \n",
    "    if i == 1: \n",
    "        hyperparameters['model_name_or_path'] = \"roberta-base\"\n",
    "    if i == 2: \n",
    "        hyperparameters['model_name_or_path'] = \"roberta-large\"        \n",
    "    elif i == 3: \n",
    "        hyperparameters['learning_rate'] = 3e-4\n",
    "    elif i == 4: \n",
    "        hyperparameters['model_name_or_path'] = \"roberta-base\"\n",
    "        hyperparameters['learning_rate'] = 3e-4\n",
    "    elif i == 5: \n",
    "        hyperparameters['model_name_or_path'] = \"roberta-large\"\n",
    "        hyperparameters['learning_rate'] = 3e-4\n",
    "    elif i == 6: \n",
    "        hyperparameters['learning_rate'] = 3e-5\n",
    "    elif i == 7: \n",
    "        hyperparameters['model_name_or_path'] = \"roberta-base\"\n",
    "        hyperparameters['learning_rate'] = 3e-5\n",
    "    elif i == 8: \n",
    "        hyperparameters['model_name_or_path'] = \"roberta-large\"\n",
    "        hyperparameters['learning_rate'] = 3e-5\n",
    "    elif i == 9: \n",
    "        hyperparameters['warmup_steps'] = None\n",
    "    elif i == 10: \n",
    "        hyperparameters['warmup_steps'] = None\n",
    "        hyperparameters['model_name_or_path'] = \"roberta-base\"\n",
    "    elif i == 11: \n",
    "        hyperparameters['warmup_steps'] = None\n",
    "        hyperparameters['model_name_or_path'] = \"roberta-large\"\n",
    "\n",
    "    hyperparameters['run_name'] = hyperparameters['model_name_or_path']    # ✍️'\n",
    "        \n",
    "    huggingface_estimator = HuggingFace(entry_point='run_text_classification.py',\n",
    "                                source_dir='./scripts',\n",
    "                                instance_type= 'ml.p3.2xlarge', #'ml.p3.8xlarge', #'g4dn.12xlarge',\n",
    "                                instance_count=1,\n",
    "                                role=role,\n",
    "                                transformers_version='4.6',\n",
    "                                pytorch_version='1.7',\n",
    "                                py_version='py36',\n",
    "                                hyperparameters = hyperparameters)\n",
    "\n",
    "    huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W&B Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'output_dir': 'tmp',\n",
    "    'model_name_or_path': 'albert-large-v2', #'distilbert-base-uncased',  # microsoft/deberta-base , roberta-base\n",
    "    'dataset_name': 'banking77',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'per_device_train_batch_size': 16,\n",
    "    'per_device_eval_batch_size': 16,\n",
    "    'learning_rate': 1e-4,\n",
    "    'warmup_steps': 100,\n",
    "    'fp16': True,\n",
    "    'logging_steps': 10,\n",
    "    'max_steps': 1200,\n",
    "    'eval_steps': 100,\n",
    "    'save_steps': 400,\n",
    "    'evaluation_strategy' : 'steps',\n",
    "    'load_best_model_at_end': True,\n",
    "    'metric_for_best_model': 'accuracy',\n",
    "    'report_to': 'wandb',    # ✍️\n",
    "#     'run_name': 'distilbert2'    # ✍️\n",
    "}\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='run_text_cls_sweep.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type= 'ml.p3.2xlarge', #'ml.p3.8xlarge', #'g4dn.12xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.6',\n",
    "                            pytorch_version='1.7',\n",
    "                            py_version='py36',\n",
    "                            hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-11 23:05:20 Starting - Starting the training job...ProfilerReport-1628723120: InProgress\n",
      "...\n",
      "2021-08-11 23:06:06 Starting - Launching requested ML instances.........\n",
      "2021-08-11 23:07:48 Starting - Preparing the instances for training.........\n",
      "2021-08-11 23:09:14 Downloading - Downloading input data\n",
      "2021-08-11 23:09:14 Training - Downloading the training image..............."
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container image used for training job: \n",
      "558105141721.dkr.ecr.us-east-1.amazonaws.com/huggingface-training:pytorch1.6.0-transformers4.2.2-tokenizers0.9.4-datasets1.2.1-py36-gpu-cu110\n",
      "\n",
      "s3 uri where the trained model is located: \n",
      "s3://philipps-sagemaker-bucket-us-east-1/huggingface-training-2021-02-04-16-47-39-189/output/model.tar.gz\n",
      "\n",
      "latest training job name for this estimator: \n",
      "huggingface-training-2021-02-04-16-47-39-189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
